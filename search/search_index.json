{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>ChromatinHD analyzes single-cell ATAC+RNA data using the raw fragments as input, by automatically adapting the scale at which relevant chromatin changes on a per-position, per-cell, and per-gene basis. This enables identification of functional chromatin changes regardless of whether they occur in a narrow or broad region.</p> <p>As we show in our paper, ChromatinHD models are better able to capture functional chromatin changes that the typical approach, i.e. peak-calling + statistical analysis. This is because there are extensive functional accessibility changes both outside and within peaks.</p> <p>ChromatinHD models can capture long-range interactions by considering fragments co-occuring within the same cell, as we highlight in Figure 5 of our paper,</p> <p>ChromatinHD models can also capture changes in fragment size that are related to gene expression changes, likely driven by dense direct and indirect binding of transcription factors, as we highlight in Figure 6 of our paper.</p> <p>Currently, the following models are supported:</p> Pred <p> To learn where and how accessibility is predictive for gene expression </p> Diff <p>To understand the differences in accessibilty between cell types/states</p> Time <p>To learn where and how accessibility is predictive over (pseudo)time</p> Dime <p>To learn the differences in accessibility over (pseudo)time</p>"},{"location":"benchmark/","title":"Benchmark","text":"<p>Lightweight benchmarking of the various models in terms of function and scalability. More comprehensive benchmarking with the state-of-the-art was performed in our paper.</p>"},{"location":"benchmark/diff/models/","title":"Models","text":"In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nimport seaborn as sns\n\nsns.set_style(\"ticks\")\n%config InlineBackend.figure_format='retina'\n\nimport tqdm.auto as tqdm\n</pre> %load_ext autoreload %autoreload 2  import numpy as np import pandas as pd  import matplotlib.pyplot as plt import matplotlib as mpl  import seaborn as sns  sns.set_style(\"ticks\") %config InlineBackend.figure_format='retina'  import tqdm.auto as tqdm In\u00a0[2]: Copied! <pre>import chromatinhd as chd\n\nchd.set_default_device(\"cuda:1\")\nchd.get_default_device()\n</pre> import chromatinhd as chd  chd.set_default_device(\"cuda:1\") chd.get_default_device() Out[2]: <pre>'cuda:1'</pre> In\u00a0[3]: Copied! <pre>dataset_folder_original = chd.get_output() / \"datasets\" / \"pbmc10k\"\ntranscriptome_original = chd.data.Transcriptome(dataset_folder_original / \"transcriptome\")\nfragments_original = chd.data.Fragments(dataset_folder_original / \"fragments\" / \"10k10k\")\n</pre> dataset_folder_original = chd.get_output() / \"datasets\" / \"pbmc10k\" transcriptome_original = chd.data.Transcriptome(dataset_folder_original / \"transcriptome\") fragments_original = chd.data.Fragments(dataset_folder_original / \"fragments\" / \"10k10k\") In\u00a0[4]: Copied! <pre>genes_oi = transcriptome_original.var.sort_values(\"dispersions_norm\", ascending=False).head(50).index\nregions = fragments_original.regions.filter(genes_oi)\nfragments = fragments_original.filter_regions(regions)\nfragments.create_cellxgene_indptr()\ntranscriptome = transcriptome_original.filter_genes(regions.coordinates.index)\n</pre> genes_oi = transcriptome_original.var.sort_values(\"dispersions_norm\", ascending=False).head(50).index regions = fragments_original.regions.filter(genes_oi) fragments = fragments_original.filter_regions(regions) fragments.create_cellxgene_indptr() transcriptome = transcriptome_original.filter_genes(regions.coordinates.index) <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n/home/wsaelens/projects/chromatinhd/chromatinhd/docs/source/benchmark/diff/models.ipynb Cell 4 in 3\n      &lt;a href='vscode-notebook-cell://ssh-remote%2Bgpu/home/wsaelens/projects/chromatinhd/chromatinhd/docs/source/benchmark/diff/models.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'&gt;1&lt;/a&gt; genes_oi = transcriptome_original.var.sort_values(\"dispersions_norm\", ascending=False).head(50).index\n      &lt;a href='vscode-notebook-cell://ssh-remote%2Bgpu/home/wsaelens/projects/chromatinhd/chromatinhd/docs/source/benchmark/diff/models.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'&gt;2&lt;/a&gt; regions = fragments_original.regions.filter(genes_oi)\n----&gt; &lt;a href='vscode-notebook-cell://ssh-remote%2Bgpu/home/wsaelens/projects/chromatinhd/chromatinhd/docs/source/benchmark/diff/models.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'&gt;3&lt;/a&gt; fragments = fragments_original.filter_regions(regions)\n      &lt;a href='vscode-notebook-cell://ssh-remote%2Bgpu/home/wsaelens/projects/chromatinhd/chromatinhd/docs/source/benchmark/diff/models.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'&gt;4&lt;/a&gt; fragments.create_cellxgene_indptr()\n      &lt;a href='vscode-notebook-cell://ssh-remote%2Bgpu/home/wsaelens/projects/chromatinhd/chromatinhd/docs/source/benchmark/diff/models.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'&gt;5&lt;/a&gt; transcriptome = transcriptome_original.filter_genes(regions.coordinates.index)\n\nFile ~/projects/chromatinhd/chromatinhd/src/chromatinhd/data/fragments/fragments.py:246, in Fragments.filter_regions(self, regions, path)\n    244 self.regions.coordinates[\"ix\"] = np.arange(self.regions.coordinates.shape[0])\n    245 regions.coordinates[\"ix\"] = self.regions.coordinates[\"ix\"].loc[regions.coordinates.index]\n--&gt; 246 fragments_oi = np.isin(self.mapping[:, 1].numpy(), regions.coordinates[\"ix\"])\n    248 mapping = self.mapping[fragments_oi]\n    249 coordinates = self.coordinates[fragments_oi]\n\nFile ~/projects/chromatinhd/chromatinhd/src/chromatinhd/flow/tensorstore.py:150, in TensorstoreInstance.__getitem__(self, key)\n    149 def __getitem__(self, key):\n--&gt; 150     return self.open_reader()[key].read().result()\n\nFile ~/projects/chromatinhd/chromatinhd/src/chromatinhd/flow/tensorstore.py:131, in TensorstoreInstance.open_reader(self, spec)\n    130 def open_reader(self, spec=None):\n--&gt; 131     return ts.open(self.spec_read, read=True).result()\n\nValueError: NOT_FOUND: Error opening \"zarr\" driver: Metadata at local file \"/home/wsaelens/projects/chromatinhd/chromatinhd/output/datasets/pbmc10k/fragments/10k10k/mapping.zarr/.zarray\" does not exist [tensorstore_spec='{\\\"context\\\":{\\\"cache_pool\\\":{},\\\"data_copy_concurrency\\\":{},\\\"file_io_concurrency\\\":{},\\\"file_io_sync\\\":true},\\\"driver\\\":\\\"zarr\\\",\\\"kvstore\\\":{\\\"driver\\\":\\\"file\\\",\\\"path\\\":\\\"/home/wsaelens/projects/chromatinhd/chromatinhd/output/datasets/pbmc10k/fragments/10k10k/mapping.zarr/\\\"}}'] [source locations='tensorstore/driver/kvs_backed_chunk_driver.cc:1180\\ntensorstore/driver/driver.cc:114']</pre> In\u00a0[5]: Copied! <pre>folds = chd.data.folds.Folds()\nfolds.sample_cells(fragments, 5)\n</pre> folds = chd.data.folds.Folds() folds.sample_cells(fragments, 5) In\u00a0[6]: Copied! <pre>clustering = chd.data.Clustering.from_labels(transcriptome.obs[\"celltype\"])\n</pre> clustering = chd.data.Clustering.from_labels(transcriptome.obs[\"celltype\"]) In\u00a0[7]: Copied! <pre>fold = folds[0]\n</pre> fold = folds[0] In\u00a0[8]: Copied! <pre>models = {}\n</pre> models = {} In\u00a0[9]: Copied! <pre>import logging\n\nlogger = chd.models.diff.trainer.trainer.logger\nlogger.setLevel(logging.DEBUG)\nlogger.handlers = []\n# logger.handlers = [logging.StreamHandler()]\n</pre> import logging  logger = chd.models.diff.trainer.trainer.logger logger.setLevel(logging.DEBUG) logger.handlers = [] # logger.handlers = [logging.StreamHandler()] In\u00a0[10]: Copied! <pre>model = chd.models.diff.model.cutnf.Model(\n    fragments,\n    clustering,\n)\nmodel.train_model(fragments, clustering, fold, n_epochs=30)\nmodels[\"original\"] = model\n</pre> model = chd.models.diff.model.cutnf.Model(     fragments,     clustering, ) model.train_model(fragments, clustering, fold, n_epochs=30) models[\"original\"] = model <pre>  0%|          | 0/1230 [00:00&lt;?, ?it/s]</pre> In\u00a0[11]: Copied! <pre>model = chd.models.diff.model.cutnf.Model(\n    fragments,\n    clustering,\n    nbins = (32, 64, 128)\n)\nmodel.train_model(fragments, clustering, fold, n_epochs=30)\nmodels[\"original_rev\"] = model\n</pre> model = chd.models.diff.model.cutnf.Model(     fragments,     clustering,     nbins = (32, 64, 128) ) model.train_model(fragments, clustering, fold, n_epochs=30) models[\"original_rev\"] = model <pre>  0%|          | 0/1230 [00:00&lt;?, ?it/s]</pre> In\u00a0[12]: Copied! <pre>model = chd.models.diff.model.cutnf.Model(\n    fragments,\n    clustering,\n)\nmodel.train_model(fragments, clustering, fold, n_epochs=100)\nmodels[\"original_100epoch\"] = model\n</pre> model = chd.models.diff.model.cutnf.Model(     fragments,     clustering, ) model.train_model(fragments, clustering, fold, n_epochs=100) models[\"original_100epoch\"] = model <pre>  0%|          | 0/4100 [00:00&lt;?, ?it/s]</pre> In\u00a0[13]: Copied! <pre>model = chd.models.diff.model.cutnf.Model(\n    fragments,\n    clustering,\n    mixture_delta_p_scale=5.0,\n)\nmodel.train_model(fragments, clustering, fold, n_epochs=30)\nmodels[\"original_mixture-delta-p=5\"] = model\n</pre> model = chd.models.diff.model.cutnf.Model(     fragments,     clustering,     mixture_delta_p_scale=5.0, ) model.train_model(fragments, clustering, fold, n_epochs=30) models[\"original_mixture-delta-p=5\"] = model <pre>  0%|          | 0/1230 [00:00&lt;?, ?it/s]</pre> In\u00a0[14]: Copied! <pre>model = chd.models.diff.model.cutnf.Model(\n    fragments,\n    clustering,\n    mixture_delta_regularization=False,\n    rho_delta_regularization=False,\n)\nmodel.train_model(fragments, clustering, fold, n_epochs=30)\nmodels[\"original_noreg\"] = model\n</pre> model = chd.models.diff.model.cutnf.Model(     fragments,     clustering,     mixture_delta_regularization=False,     rho_delta_regularization=False, ) model.train_model(fragments, clustering, fold, n_epochs=30) models[\"original_noreg\"] = model <pre>  0%|          | 0/1230 [00:00&lt;?, ?it/s]</pre> In\u00a0[15]: Copied! <pre>model = chd.models.diff.model.cutnf.Model(\n    fragments,\n    clustering,\n    mixture_delta_p_scale=0.001,\n)\nmodel.train_model(fragments, clustering, fold, n_epochs=30)\nmodels[\"baseline_orig\"] = model\n</pre> model = chd.models.diff.model.cutnf.Model(     fragments,     clustering,     mixture_delta_p_scale=0.001, ) model.train_model(fragments, clustering, fold, n_epochs=30) models[\"baseline_orig\"] = model <pre>  0%|          | 0/1230 [00:00&lt;?, ?it/s]</pre> In\u00a0[16]: Copied! <pre>model = chd.models.diff.model.cutnf.Model(\n    fragments, clustering, mixture_delta_regularization=False, rho_delta_regularization=False, nbins=(128,)\n)\nmodel.train_model(fragments, clustering, fold, n_epochs=30)\nmodels[\"original_noreg_128\"] = model\n</pre> model = chd.models.diff.model.cutnf.Model(     fragments, clustering, mixture_delta_regularization=False, rho_delta_regularization=False, nbins=(128,) ) model.train_model(fragments, clustering, fold, n_epochs=30) models[\"original_noreg_128\"] = model <pre>  0%|          | 0/1230 [00:00&lt;?, ?it/s]</pre> In\u00a0[17]: Copied! <pre>model = chd.models.diff.model.cutnf.Model(\n    fragments,\n    clustering,\n    mixture_delta_regularization=False,\n    rho_delta_regularization=False,\n    nbins=(256,),\n)\nmodel.train_model(fragments, clustering, fold, n_epochs=30)\nmodels[\"original_noreg_256\"] = model\n</pre> model = chd.models.diff.model.cutnf.Model(     fragments,     clustering,     mixture_delta_regularization=False,     rho_delta_regularization=False,     nbins=(256,), ) model.train_model(fragments, clustering, fold, n_epochs=30) models[\"original_noreg_256\"] = model <pre>  0%|          | 0/1230 [00:00&lt;?, ?it/s]</pre> In\u00a0[18]: Copied! <pre>model = chd.models.diff.model.cutnf.Model(\n    fragments,\n    clustering,\n    mixture_delta_regularization=False,\n    rho_delta_regularization=False,\n    nbins=(256, 128, 64),\n)\nmodel.train_model(fragments, clustering, fold, n_epochs=30)\nmodels[\"original_noreg_256,128,64\"] = model\n</pre> model = chd.models.diff.model.cutnf.Model(     fragments,     clustering,     mixture_delta_regularization=False,     rho_delta_regularization=False,     nbins=(256, 128, 64), ) model.train_model(fragments, clustering, fold, n_epochs=30) models[\"original_noreg_256,128,64\"] = model <pre>  0%|          | 0/1230 [00:00&lt;?, ?it/s]</pre> In\u00a0[19]: Copied! <pre>model = chd.models.diff.model.cutnf.Model(\n    fragments,\n    clustering,\n    nbins=(256, 128, 64),\n)\nmodel.train_model(fragments, clustering, fold, n_epochs=30)\nmodels[\"original_256,128,64\"] = model\n</pre> model = chd.models.diff.model.cutnf.Model(     fragments,     clustering,     nbins=(256, 128, 64), ) model.train_model(fragments, clustering, fold, n_epochs=30) models[\"original_256,128,64\"] = model <pre>  0%|          | 0/1230 [00:00&lt;?, ?it/s]</pre> In\u00a0[20]: Copied! <pre>model = chd.models.diff.model.cutnf.Model(\n    fragments,\n    clustering,\n    nbins=(512, 256, 128, 64),\n)\nmodel.train_model(fragments, clustering, fold, n_epochs=30)\nmodels[\"original_512,256,128,64\"] = model\n</pre> model = chd.models.diff.model.cutnf.Model(     fragments,     clustering,     nbins=(512, 256, 128, 64), ) model.train_model(fragments, clustering, fold, n_epochs=30) models[\"original_512,256,128,64\"] = model <pre>  0%|          | 0/1230 [00:00&lt;?, ?it/s]</pre> In\u00a0[21]: Copied! <pre>model = chd.models.diff.model.cutnf.Model(fragments, clustering, mixture_delta_p_scale=0.001, nbins=(128,))\nmodel.train_model(fragments, clustering, fold, n_epochs=30)\nmodels[\"baseline_128\"] = model\n</pre> model = chd.models.diff.model.cutnf.Model(fragments, clustering, mixture_delta_p_scale=0.001, nbins=(128,)) model.train_model(fragments, clustering, fold, n_epochs=30) models[\"baseline_128\"] = model <pre>  0%|          | 0/1230 [00:00&lt;?, ?it/s]</pre> In\u00a0[22]: Copied! <pre>model = chd.models.diff.model.cutnf.Model(fragments, clustering, mixture_delta_p_scale=0.001, nbins=(256,))\nmodel.train_model(fragments, clustering, fold, n_epochs=30)\nmodels[\"baseline_256\"] = model\n</pre> model = chd.models.diff.model.cutnf.Model(fragments, clustering, mixture_delta_p_scale=0.001, nbins=(256,)) model.train_model(fragments, clustering, fold, n_epochs=30) models[\"baseline_256\"] = model <pre>  0%|          | 0/1230 [00:00&lt;?, ?it/s]</pre> In\u00a0[23]: Copied! <pre>model = chd.models.diff.model.cutnf.Model(fragments, clustering, mixture_delta_p_scale=0.001, nbins=(256, 128, 64))\nmodel.train_model(fragments, clustering, fold, n_epochs=30)\nmodels[\"baseline_256,128,64\"] = model\n</pre> model = chd.models.diff.model.cutnf.Model(fragments, clustering, mixture_delta_p_scale=0.001, nbins=(256, 128, 64)) model.train_model(fragments, clustering, fold, n_epochs=30) models[\"baseline_256,128,64\"] = model <pre>  0%|          | 0/1230 [00:00&lt;?, ?it/s]</pre> In\u00a0[29]: Copied! <pre>scores = []\ngenescores = []\nfor model_id, model in models.items():\n    prediction_test = model.get_prediction(fragments, clustering, cell_ixs=fold[\"cells_test\"])\n    prediction_validation = model.get_prediction(fragments, clustering, cell_ixs=fold[\"cells_validation\"])\n    prediction_train = model.get_prediction(fragments, clustering, cell_ixs=fold[\"cells_train\"])\n    scores.append(\n        {\n            \"model_id\": model_id,\n            \"lik_test\": (prediction_test[\"likelihood_mixture\"]).sum().item(),\n            \"n_test\": len(fold[\"cells_test\"]),\n            \"lik_validation\": (prediction_validation[\"likelihood_mixture\"]).sum().item(),\n            \"n_validation\": len(fold[\"cells_validation\"]),\n            \"lik_train\": (prediction_train[\"likelihood_mixture\"]).sum().item(),\n            \"n_train\": len(fold[\"cells_train\"]),\n        }\n    )\n    genescores.append(\n        pd.DataFrame(\n            {\n                \"model_id\": model_id,\n                \"lik_test\": (prediction_test[\"likelihood_mixture\"]).sum(\"cell\").to_pandas(),\n                \"n_test\": len(fold[\"cells_test\"]),\n                \"lik_validation\": (prediction_validation[\"likelihood_mixture\"]).sum(\"cell\").to_pandas(),\n                \"n_validation\": len(fold[\"cells_validation\"]),\n                \"lik_train\": (prediction_train[\"likelihood_mixture\"]).sum(\"cell\").to_pandas(),\n                \"n_train\": len(fold[\"cells_train\"]),\n            }\n        )\n    )\nscores = pd.DataFrame(scores).set_index(\"model_id\")\ngenescores = (\n    pd.concat([genescores[i] for i in range(len(genescores))], axis=0).reset_index().set_index([\"model_id\", \"gene\"])\n)\n</pre> scores = [] genescores = [] for model_id, model in models.items():     prediction_test = model.get_prediction(fragments, clustering, cell_ixs=fold[\"cells_test\"])     prediction_validation = model.get_prediction(fragments, clustering, cell_ixs=fold[\"cells_validation\"])     prediction_train = model.get_prediction(fragments, clustering, cell_ixs=fold[\"cells_train\"])     scores.append(         {             \"model_id\": model_id,             \"lik_test\": (prediction_test[\"likelihood_mixture\"]).sum().item(),             \"n_test\": len(fold[\"cells_test\"]),             \"lik_validation\": (prediction_validation[\"likelihood_mixture\"]).sum().item(),             \"n_validation\": len(fold[\"cells_validation\"]),             \"lik_train\": (prediction_train[\"likelihood_mixture\"]).sum().item(),             \"n_train\": len(fold[\"cells_train\"]),         }     )     genescores.append(         pd.DataFrame(             {                 \"model_id\": model_id,                 \"lik_test\": (prediction_test[\"likelihood_mixture\"]).sum(\"cell\").to_pandas(),                 \"n_test\": len(fold[\"cells_test\"]),                 \"lik_validation\": (prediction_validation[\"likelihood_mixture\"]).sum(\"cell\").to_pandas(),                 \"n_validation\": len(fold[\"cells_validation\"]),                 \"lik_train\": (prediction_train[\"likelihood_mixture\"]).sum(\"cell\").to_pandas(),                 \"n_train\": len(fold[\"cells_train\"]),             }         )     ) scores = pd.DataFrame(scores).set_index(\"model_id\") genescores = (     pd.concat([genescores[i] for i in range(len(genescores))], axis=0).reset_index().set_index([\"model_id\", \"gene\"]) ) In\u00a0[30]: Copied! <pre>baseline_id = \"baseline_orig\"\nscores[[\"lr_test\", \"lr_validation\", \"lr_train\"]] = (\n    scores[[\"lik_test\", \"lik_validation\", \"lik_train\"]]\n    - scores[[\"lik_test\", \"lik_validation\", \"lik_train\"]].loc[baseline_id]\n)\nscores[[\"nlr_test\", \"nlr_validation\", \"nlr_train\"]] = (\n    scores[[\"lr_test\", \"lr_validation\", \"lr_train\"]].values / scores[[\"n_test\", \"n_validation\", \"n_train\"]].values\n)\ngenescores[[\"lr_test\", \"lr_validation\", \"lr_train\"]] = (\n    genescores[[\"lik_test\", \"lik_validation\", \"lik_train\"]]\n    - genescores[[\"lik_test\", \"lik_validation\", \"lik_train\"]].loc[baseline_id]\n)\ngenescores[[\"nlr_test\", \"nlr_validation\", \"nlr_train\"]] = (\n    genescores[[\"lr_test\", \"lr_validation\", \"lr_train\"]].values\n    / genescores[[\"n_test\", \"n_validation\", \"n_train\"]].values\n)\n</pre> baseline_id = \"baseline_orig\" scores[[\"lr_test\", \"lr_validation\", \"lr_train\"]] = (     scores[[\"lik_test\", \"lik_validation\", \"lik_train\"]]     - scores[[\"lik_test\", \"lik_validation\", \"lik_train\"]].loc[baseline_id] ) scores[[\"nlr_test\", \"nlr_validation\", \"nlr_train\"]] = (     scores[[\"lr_test\", \"lr_validation\", \"lr_train\"]].values / scores[[\"n_test\", \"n_validation\", \"n_train\"]].values ) genescores[[\"lr_test\", \"lr_validation\", \"lr_train\"]] = (     genescores[[\"lik_test\", \"lik_validation\", \"lik_train\"]]     - genescores[[\"lik_test\", \"lik_validation\", \"lik_train\"]].loc[baseline_id] ) genescores[[\"nlr_test\", \"nlr_validation\", \"nlr_train\"]] = (     genescores[[\"lr_test\", \"lr_validation\", \"lr_train\"]].values     / genescores[[\"n_test\", \"n_validation\", \"n_train\"]].values ) In\u00a0[31]: Copied! <pre>model_info = pd.DataFrame({\"model\": models.keys()}).set_index(\"model\")\nmodel_info[\"model_type\"] = model_info.index.map(lambda x: \"_\".join(x.split(\"_\")[:-1]))\nmodel_info = model_info.sort_values([\"model_type\"])\nmodel_info[\"ix\"] = np.arange(model_info.shape[0])\n</pre> model_info = pd.DataFrame({\"model\": models.keys()}).set_index(\"model\") model_info[\"model_type\"] = model_info.index.map(lambda x: \"_\".join(x.split(\"_\")[:-1])) model_info = model_info.sort_values([\"model_type\"]) model_info[\"ix\"] = np.arange(model_info.shape[0]) In\u00a0[32]: Copied! <pre>fig = chd.grid.Figure(chd.grid.Wrap(padding_width=0.1))\nheight = len(scores) * 0.2\n\nplotdata = scores.copy().loc[model_info.index]\n\npanel, ax = fig.main.add(chd.grid.Ax((1, height)))\nax.barh(plotdata.index, plotdata[\"lr_test\"])\nax.axvline(0, color=\"black\", linestyle=\"--\", lw=1)\nax.set_title(\"Test\")\nax.set_xlabel(\"Log-likehood ratio\")\n\npanel, ax = fig.main.add(chd.grid.Ax((1, height)))\nax.set_yticks([])\nax.barh(plotdata.index, plotdata[\"lr_validation\"])\nax.axvline(0, color=\"black\", linestyle=\"--\", lw=1)\nax.set_title(\"Validation\")\n\npanel, ax = fig.main.add(chd.grid.Ax((1, height)))\nax.set_yticks([])\nax.barh(plotdata.index, plotdata[\"lr_train\"])\nax.axvline(0, color=\"black\", linestyle=\"--\", lw=1)\nax.set_title(\"Train\")\nfig.plot()\n</pre> fig = chd.grid.Figure(chd.grid.Wrap(padding_width=0.1)) height = len(scores) * 0.2  plotdata = scores.copy().loc[model_info.index]  panel, ax = fig.main.add(chd.grid.Ax((1, height))) ax.barh(plotdata.index, plotdata[\"lr_test\"]) ax.axvline(0, color=\"black\", linestyle=\"--\", lw=1) ax.set_title(\"Test\") ax.set_xlabel(\"Log-likehood ratio\")  panel, ax = fig.main.add(chd.grid.Ax((1, height))) ax.set_yticks([]) ax.barh(plotdata.index, plotdata[\"lr_validation\"]) ax.axvline(0, color=\"black\", linestyle=\"--\", lw=1) ax.set_title(\"Validation\")  panel, ax = fig.main.add(chd.grid.Ax((1, height))) ax.set_yticks([]) ax.barh(plotdata.index, plotdata[\"lr_train\"]) ax.axvline(0, color=\"black\", linestyle=\"--\", lw=1) ax.set_title(\"Train\") fig.plot() In\u00a0[33]: Copied! <pre>plotdata = genescores[\"lr_test\"].unstack()\nplotdata.columns = transcriptome.symbol(plotdata.columns)\nplotdata = plotdata.loc[model_info.index].T\n\nfig, ax = plt.subplots(figsize=(plotdata.shape[1] * 0.2, plotdata.shape[0] * 0.2))\nsns.heatmap(plotdata, vmax=100, vmin=-100, cmap=\"RdBu_r\", center=0, cbar_kws={\"shrink\": 0.5}, yticklabels=True)\n\n# add dot for highest\nfor i, gene in enumerate(plotdata.index):\n    j = plotdata.loc[gene].argmax()\n    plt.plot(j + 0.5, i + 0.5, \"o\", color=\"black\", markersize=4, markeredgewidth=1.0, markeredgecolor=\"white\")\n</pre> plotdata = genescores[\"lr_test\"].unstack() plotdata.columns = transcriptome.symbol(plotdata.columns) plotdata = plotdata.loc[model_info.index].T  fig, ax = plt.subplots(figsize=(plotdata.shape[1] * 0.2, plotdata.shape[0] * 0.2)) sns.heatmap(plotdata, vmax=100, vmin=-100, cmap=\"RdBu_r\", center=0, cbar_kws={\"shrink\": 0.5}, yticklabels=True)  # add dot for highest for i, gene in enumerate(plotdata.index):     j = plotdata.loc[gene].argmax()     plt.plot(j + 0.5, i + 0.5, \"o\", color=\"black\", markersize=4, markeredgewidth=1.0, markeredgecolor=\"white\") In\u00a0[34]: Copied! <pre>genepositional = chd.models.diff.interpret.genepositional.GenePositional(\n    path=chd.get_output() / \"interpret\" / \"genepositional\"\n)\n</pre> genepositional = chd.models.diff.interpret.genepositional.GenePositional(     path=chd.get_output() / \"interpret\" / \"genepositional\" ) In\u00a0[35]: Copied! <pre>symbol = \"EBF1\"\nmodel_id = \"original\"\n# model_id = \"original_512,256,128,64\"\n\ngenepositional.score(fragments, clustering, [models[model_id]], force=True, genes=transcriptome.gene_id([symbol]))\n</pre> symbol = \"EBF1\" model_id = \"original\" # model_id = \"original_512,256,128,64\"  genepositional.score(fragments, clustering, [models[model_id]], force=True, genes=transcriptome.gene_id([symbol])) <pre>  0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> In\u00a0[165]: Copied! <pre>fig = chd.grid.Figure(chd.grid.Grid(padding_height=0.05, padding_width=0.05))\nwidth = 10\n\nregion = fragments.regions.coordinates.loc[transcriptome.gene_id(symbol)]\npanel_genes = chd.plot.genome.genes.Genes.from_region(region, width=width)\nfig.main.add_under(panel_genes)\n\nplotdata, plotdata_mean = genepositional.get_plotdata(transcriptome.gene_id(symbol))\npanel_differential = chd.models.diff.plot.Differential(\n    plotdata, plotdata_mean, cluster_info=clustering.cluster_info, panel_height=0.5, width=width\n)\nfig.main.add_under(panel_differential)\n\npanel_expression = chd.models.diff.plot.DifferentialExpression.from_transcriptome(\n    transcriptome=transcriptome, clustering=clustering, gene=transcriptome.gene_id(symbol), panel_height=0.5\n)\nfig.main.add_right(panel_expression, row=panel_differential)\n\nfig.plot()\n</pre> fig = chd.grid.Figure(chd.grid.Grid(padding_height=0.05, padding_width=0.05)) width = 10  region = fragments.regions.coordinates.loc[transcriptome.gene_id(symbol)] panel_genes = chd.plot.genome.genes.Genes.from_region(region, width=width) fig.main.add_under(panel_genes)  plotdata, plotdata_mean = genepositional.get_plotdata(transcriptome.gene_id(symbol)) panel_differential = chd.models.diff.plot.Differential(     plotdata, plotdata_mean, cluster_info=clustering.cluster_info, panel_height=0.5, width=width ) fig.main.add_under(panel_differential)  panel_expression = chd.models.diff.plot.DifferentialExpression.from_transcriptome(     transcriptome=transcriptome, clustering=clustering, gene=transcriptome.gene_id(symbol), panel_height=0.5 ) fig.main.add_right(panel_expression, row=panel_differential)  fig.plot()"},{"location":"benchmark/diff/models/#train","title":"Train\u00b6","text":""},{"location":"benchmark/diff/models/#score","title":"Score\u00b6","text":""},{"location":"benchmark/diff/models/#interpret","title":"Interpret\u00b6","text":""},{"location":"benchmark/diff/simulated/","title":"Simulated","text":"In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nimport seaborn as sns\n\nsns.set_style(\"ticks\")\n%config InlineBackend.figure_format='retina'\n\nimport tqdm.auto as tqdm\n</pre> %load_ext autoreload %autoreload 2  import numpy as np import pandas as pd  import matplotlib.pyplot as plt import matplotlib as mpl  import seaborn as sns  sns.set_style(\"ticks\") %config InlineBackend.figure_format='retina'  import tqdm.auto as tqdm In\u00a0[3]: Copied! <pre>import chromatinhd as chd\n\nchd.set_default_device(\"cuda:1\")\nchd.get_default_device()\n</pre> import chromatinhd as chd  chd.set_default_device(\"cuda:1\") chd.get_default_device() Out[3]: <pre>'cuda:1'</pre> In\u00a0[4]: Copied! <pre>import chromatinhd.simulation.simulate\n</pre> import chromatinhd.simulation.simulate In\u00a0[5]: Copied! <pre>simulation = chd.simulation.simulate.Simulation(n_genes=10, window=[-10000, 10000])\nsimulation.create_regions()\nsimulation.create_obs()\n\nsimulation.create_fragments()\n\nsimulation.create_motifscan()\n</pre> simulation = chd.simulation.simulate.Simulation(n_genes=10, window=[-10000, 10000]) simulation.create_regions() simulation.create_obs()  simulation.create_fragments()  simulation.create_motifscan() <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> In\u00a0[6]: Copied! <pre>fragments = simulation.fragments\nfragments.create_cellxgene_indptr()\n</pre> fragments = simulation.fragments fragments.create_cellxgene_indptr() In\u00a0[7]: Copied! <pre>sns.histplot((fragments.coordinates[:, 1] - fragments.coordinates[:, 0]))\n</pre> sns.histplot((fragments.coordinates[:, 1] - fragments.coordinates[:, 0])) Out[7]: <pre>&lt;Axes: ylabel='Count'&gt;</pre> In\u00a0[8]: Copied! <pre>clustering = simulation.clustering\n</pre> clustering = simulation.clustering In\u00a0[9]: Copied! <pre>folds = chd.data.folds.Folds()\nfolds.sample_cells(fragments, 5)\nfold = folds[0]\n</pre> folds = chd.data.folds.Folds() folds.sample_cells(fragments, 5) fold = folds[0] In\u00a0[10]: Copied! <pre>minibatcher = chd.models.diff.loader.Minibatcher(np.arange(len(fragments.obs)), np.arange(len(fragments.var)), 100, 100)\n</pre> minibatcher = chd.models.diff.loader.Minibatcher(np.arange(len(fragments.obs)), np.arange(len(fragments.var)), 100, 100) In\u00a0[11]: Copied! <pre>import chromatinhd.models.diff.loader.clustering_fragments\n\nloader = chromatinhd.models.diff.loader.clustering_fragments.ClusteringFragments(\n    clustering, fragments, minibatcher.cellxregion_batch_size\n)\ndata = loader.load(next(iter(minibatcher)))\n</pre> import chromatinhd.models.diff.loader.clustering_fragments  loader = chromatinhd.models.diff.loader.clustering_fragments.ClusteringFragments(     clustering, fragments, minibatcher.cellxregion_batch_size ) data = loader.load(next(iter(minibatcher))) In\u00a0[12]: Copied! <pre>import chromatinhd.models.diff.model.playground\n\nmodel = chd.models.diff.model.playground.Model(fragments, clustering)\n</pre> import chromatinhd.models.diff.model.playground  model = chd.models.diff.model.playground.Model(fragments, clustering) In\u00a0[47]: Copied! <pre>transform = chromatinhd.models.diff.model.spline.DifferentialQuadraticSplineStack(nbins=(128,), n_genes=1)\n</pre> transform = chromatinhd.models.diff.model.spline.DifferentialQuadraticSplineStack(nbins=(128,), n_genes=1) In\u00a0[48]: Copied! <pre>import truncated_normal\n</pre> import truncated_normal In\u00a0[49]: Copied! <pre>import math\n\n\ndef log_prob_normal(value, loc, scale):\n    var = scale**2\n    log_scale = math.log(scale)\n    return -((value - loc) ** 2) / (2 * var) - log_scale - math.log(math.sqrt(2 * math.pi))\n\n\ndef log_prob_trun_normal(value, loc, scale, a=0, b=1):\n    return truncated_normal.TruncatedNormal(loc, scale, a, b).log_prob(value)\n\n\ndef apply_trunc_normal(value, loc, scale, a=0, b=1):\n    dist = truncated_normal.TruncatedNormal(loc, scale, a, b)\n    return dist.cdf(value), dist.log_prob(value)\n</pre> import math   def log_prob_normal(value, loc, scale):     var = scale**2     log_scale = math.log(scale)     return -((value - loc) ** 2) / (2 * var) - log_scale - math.log(math.sqrt(2 * math.pi))   def log_prob_trun_normal(value, loc, scale, a=0, b=1):     return truncated_normal.TruncatedNormal(loc, scale, a, b).log_prob(value)   def apply_trunc_normal(value, loc, scale, a=0, b=1):     dist = truncated_normal.TruncatedNormal(loc, scale, a, b)     return dist.cdf(value), dist.log_prob(value) In\u00a0[50]: Copied! <pre>import torch\n\nx = torch.linspace(0.001, 1, 100)\ngenes_oi = torch.tensor([0])\nlocal_gene_ix = torch.zeros(len(x), dtype=torch.int)\ndelta = torch.zeros((len(x), np.sum(transform.split_deltas)))\ndelta[:, :30] = 1\ndelta[:, 30:40] = -1\n\nlog_prob = torch.zeros_like(x)\n\noutput, logabsdet = transform.transform_forward(x, genes_oi, local_gene_ix, delta)\nlog_prob += logabsdet\n\nfig, (ax_pdf, ax_cdf) = plt.subplots(1, 2, figsize=(8, 4))\nax_pdf.plot(x, torch.exp(log_prob).detach().numpy())\nax_cdf.plot(x.numpy(), output.detach().numpy())\nassert np.isclose(np.trapz(torch.exp(log_prob).detach().numpy(), x), 1, atol=1e-2)\n\nloc = 0.9\nscale = 0.3\nloc2 = output[torch.argmin((x - loc).abs())]\n\noutput, logabsdet = apply_trunc_normal(output, loc2, scale)\nlog_prob += logabsdet\n\nax_pdf.plot(x, torch.exp(log_prob).detach().numpy())\nax_cdf.plot(x.numpy(), output.detach().numpy())\nassert np.isclose(np.trapz(torch.exp(log_prob).detach().numpy(), x), 1, atol=1e-2)\n\nloc = 0.1\nscale = 0.3\nloc2 = output[torch.argmin((x - loc).abs())]\n\noutput, logabsdet = apply_trunc_normal(output, loc2, scale)\nlog_prob += logabsdet\n\nax_pdf.plot(x, torch.exp(log_prob).detach().numpy())\nax_cdf.plot(x.numpy(), output.detach().numpy())\nassert np.isclose(np.trapz(torch.exp(log_prob).detach().numpy(), x), 1, atol=1e-2)\n</pre> import torch  x = torch.linspace(0.001, 1, 100) genes_oi = torch.tensor([0]) local_gene_ix = torch.zeros(len(x), dtype=torch.int) delta = torch.zeros((len(x), np.sum(transform.split_deltas))) delta[:, :30] = 1 delta[:, 30:40] = -1  log_prob = torch.zeros_like(x)  output, logabsdet = transform.transform_forward(x, genes_oi, local_gene_ix, delta) log_prob += logabsdet  fig, (ax_pdf, ax_cdf) = plt.subplots(1, 2, figsize=(8, 4)) ax_pdf.plot(x, torch.exp(log_prob).detach().numpy()) ax_cdf.plot(x.numpy(), output.detach().numpy()) assert np.isclose(np.trapz(torch.exp(log_prob).detach().numpy(), x), 1, atol=1e-2)  loc = 0.9 scale = 0.3 loc2 = output[torch.argmin((x - loc).abs())]  output, logabsdet = apply_trunc_normal(output, loc2, scale) log_prob += logabsdet  ax_pdf.plot(x, torch.exp(log_prob).detach().numpy()) ax_cdf.plot(x.numpy(), output.detach().numpy()) assert np.isclose(np.trapz(torch.exp(log_prob).detach().numpy(), x), 1, atol=1e-2)  loc = 0.1 scale = 0.3 loc2 = output[torch.argmin((x - loc).abs())]  output, logabsdet = apply_trunc_normal(output, loc2, scale) log_prob += logabsdet  ax_pdf.plot(x, torch.exp(log_prob).detach().numpy()) ax_cdf.plot(x.numpy(), output.detach().numpy()) assert np.isclose(np.trapz(torch.exp(log_prob).detach().numpy(), x), 1, atol=1e-2) In\u00a0[60]: Copied! <pre>models = {}\n</pre> models = {} In\u00a0[61]: Copied! <pre>model = chd.models.diff.model.playground.Model(fragments, clustering, cut_embedder=\"dummy\")\nmodel.train_model(fragments, clustering, fold, n_epochs=100)\nmodels[\"original_cutdummy\"] = model\n</pre> model = chd.models.diff.model.playground.Model(fragments, clustering, cut_embedder=\"dummy\") model.train_model(fragments, clustering, fold, n_epochs=100) models[\"original_cutdummy\"] = model <pre>  0%|          | 0/400 [00:00&lt;?, ?it/s]</pre> In\u00a0[62]: Copied! <pre>model = chd.models.diff.model.playground.Model(\n    fragments,\n    clustering,\n)\nmodel.train_model(fragments, clustering, fold, n_epochs=100)\nmodels[\"original\"] = model\n</pre> model = chd.models.diff.model.playground.Model(     fragments,     clustering, ) model.train_model(fragments, clustering, fold, n_epochs=100) models[\"original\"] = model <pre>  0%|          | 0/400 [00:00&lt;?, ?it/s]</pre> In\u00a0[89]: Copied! <pre>model = chd.models.diff.model.playground.Model(\n    fragments,\n    clustering,\n    cut_embedder_dropout_rate=0.0,\n)\nmodel.train_model(fragments, clustering, fold, n_epochs=100)\nmodels[\"original_nodrop\"] = model\n</pre> model = chd.models.diff.model.playground.Model(     fragments,     clustering,     cut_embedder_dropout_rate=0.0, ) model.train_model(fragments, clustering, fold, n_epochs=100) models[\"original_nodrop\"] = model <pre>  0%|          | 0/400 [00:00&lt;?, ?it/s]</pre> In\u00a0[64]: Copied! <pre>model = chd.models.diff.model.playground.Model(fragments, clustering, cut_embedder=\"direct\")\nmodel.train_model(fragments, clustering, fold, n_epochs=100)\nmodels[\"original_cutdirect\"] = model\n</pre> model = chd.models.diff.model.playground.Model(fragments, clustering, cut_embedder=\"direct\") model.train_model(fragments, clustering, fold, n_epochs=100) models[\"original_cutdirect\"] = model <pre>  0%|          | 0/400 [00:00&lt;?, ?it/s]</pre> In\u00a0[90]: Copied! <pre>scores = []\ngenescores = []\nfor model_id, model in models.items():\n    model = model.eval()\n    prediction_test = model.get_prediction(fragments, clustering, cell_ixs=fold[\"cells_test\"])\n    prediction_validation = model.get_prediction(fragments, clustering, cell_ixs=fold[\"cells_validation\"])\n    prediction_train = model.get_prediction(fragments, clustering, cell_ixs=fold[\"cells_train\"])\n    for phase in [\"test\", \"validation\", \"train\"]:\n        score = {\"model\": model_id, \"phase\": phase}\n        score[\"likelihood_position\"] = (prediction_test[\"likelihood_position\"]).sum().item()\n        score[\"likelihood\"] = (prediction_test[\"likelihood\"]).sum().item()\n        score[\"n_cells\"] = len(fold[f\"cells_{phase}\"])\n        scores.append(score)\n\n        genescore = pd.DataFrame(\n            {\n                \"model\": model_id,\n                \"phase\": phase,\n                \"likelihood\": (prediction_test[\"likelihood\"]).sum(\"cell\").to_pandas(),\n                \"likelihood_position\": (prediction_test[\"likelihood_position\"]).sum(\"cell\").to_pandas(),\n                \"n_cells\": len(fold[f\"cells_{phase}\"]),\n            }\n        )\n        genescores.append(genescore)\nscores = pd.DataFrame(scores).set_index([\"model\", \"phase\"])\ngenescores = (\n    pd.concat([genescores[i] for i in range(len(genescores))], axis=0)\n    .reset_index()\n    .set_index([\"model\", \"phase\", \"gene\"])\n)\n</pre> scores = [] genescores = [] for model_id, model in models.items():     model = model.eval()     prediction_test = model.get_prediction(fragments, clustering, cell_ixs=fold[\"cells_test\"])     prediction_validation = model.get_prediction(fragments, clustering, cell_ixs=fold[\"cells_validation\"])     prediction_train = model.get_prediction(fragments, clustering, cell_ixs=fold[\"cells_train\"])     for phase in [\"test\", \"validation\", \"train\"]:         score = {\"model\": model_id, \"phase\": phase}         score[\"likelihood_position\"] = (prediction_test[\"likelihood_position\"]).sum().item()         score[\"likelihood\"] = (prediction_test[\"likelihood\"]).sum().item()         score[\"n_cells\"] = len(fold[f\"cells_{phase}\"])         scores.append(score)          genescore = pd.DataFrame(             {                 \"model\": model_id,                 \"phase\": phase,                 \"likelihood\": (prediction_test[\"likelihood\"]).sum(\"cell\").to_pandas(),                 \"likelihood_position\": (prediction_test[\"likelihood_position\"]).sum(\"cell\").to_pandas(),                 \"n_cells\": len(fold[f\"cells_{phase}\"]),             }         )         genescores.append(genescore) scores = pd.DataFrame(scores).set_index([\"model\", \"phase\"]) genescores = (     pd.concat([genescores[i] for i in range(len(genescores))], axis=0)     .reset_index()     .set_index([\"model\", \"phase\", \"gene\"]) ) In\u00a0[91]: Copied! <pre>baseline_id = list(models.keys())[0]\nscores[\"lr_position\"] = (scores[\"likelihood_position\"] - scores.loc[baseline_id][\"likelihood_position\"]).values\nscores[\"lr\"] = (scores[\"likelihood\"] - scores.loc[baseline_id][\"likelihood\"]).values\ngenescores[\"lr_position\"] = (\n    genescores[\"likelihood_position\"] - genescores.loc[baseline_id][\"likelihood_position\"]\n).values\ngenescores[\"lr\"] = (genescores[\"likelihood\"] - genescores.loc[baseline_id][\"likelihood\"]).values\n</pre> baseline_id = list(models.keys())[0] scores[\"lr_position\"] = (scores[\"likelihood_position\"] - scores.loc[baseline_id][\"likelihood_position\"]).values scores[\"lr\"] = (scores[\"likelihood\"] - scores.loc[baseline_id][\"likelihood\"]).values genescores[\"lr_position\"] = (     genescores[\"likelihood_position\"] - genescores.loc[baseline_id][\"likelihood_position\"] ).values genescores[\"lr\"] = (genescores[\"likelihood\"] - genescores.loc[baseline_id][\"likelihood\"]).values In\u00a0[92]: Copied! <pre>model_info = pd.DataFrame({\"model\": models.keys()}).set_index(\"model\")\nmodel_info[\"model_type\"] = model_info.index.map(lambda x: \"_\".join(x.split(\"_\")[:-1]))\nmodel_info = model_info.sort_values([\"model_type\"])\nmodel_info[\"ix\"] = np.arange(model_info.shape[0])\n</pre> model_info = pd.DataFrame({\"model\": models.keys()}).set_index(\"model\") model_info[\"model_type\"] = model_info.index.map(lambda x: \"_\".join(x.split(\"_\")[:-1])) model_info = model_info.sort_values([\"model_type\"]) model_info[\"ix\"] = np.arange(model_info.shape[0]) In\u00a0[95]: Copied! <pre>fig = chd.grid.Figure(chd.grid.Wrap(padding_width=0.1))\nheight = len(model_info) * 0.2\n\npanel, ax = fig.main.add(chd.grid.Ax((1, height)))\nplotdata = scores.xs(\"test\", level=\"phase\").loc[model_info.index]\n\nax.barh(plotdata.index, plotdata[\"lr\"])\nax.barh(plotdata.index, plotdata[\"lr_position\"], alpha=0.5)\nax.axvline(0, color=\"black\", linestyle=\"--\", lw=1)\nax.set_title(\"Test\")\nax.set_xlabel(\"Log-likehood ratio\")\n\npanel, ax = fig.main.add(chd.grid.Ax((1, height)))\nplotdata = scores.xs(\"validation\", level=\"phase\").loc[model_info.index]\nax.set_yticks([])\nax.barh(plotdata.index, plotdata[\"lr\"])\nax.barh(plotdata.index, plotdata[\"lr_position\"], alpha=0.5)\nax.axvline(0, color=\"black\", linestyle=\"--\", lw=1)\nax.set_title(\"Validation\")\n\npanel, ax = fig.main.add(chd.grid.Ax((1, height)))\nplotdata = scores.xs(\"train\", level=\"phase\").loc[model_info.index]\nax.set_yticks([])\nax.barh(plotdata.index, plotdata[\"lr\"])\nax.barh(plotdata.index, plotdata[\"lr_position\"], alpha=0.5)\nax.axvline(0, color=\"black\", linestyle=\"--\", lw=1)\nax.set_title(\"Train\")\nfig.plot()\n</pre> fig = chd.grid.Figure(chd.grid.Wrap(padding_width=0.1)) height = len(model_info) * 0.2  panel, ax = fig.main.add(chd.grid.Ax((1, height))) plotdata = scores.xs(\"test\", level=\"phase\").loc[model_info.index]  ax.barh(plotdata.index, plotdata[\"lr\"]) ax.barh(plotdata.index, plotdata[\"lr_position\"], alpha=0.5) ax.axvline(0, color=\"black\", linestyle=\"--\", lw=1) ax.set_title(\"Test\") ax.set_xlabel(\"Log-likehood ratio\")  panel, ax = fig.main.add(chd.grid.Ax((1, height))) plotdata = scores.xs(\"validation\", level=\"phase\").loc[model_info.index] ax.set_yticks([]) ax.barh(plotdata.index, plotdata[\"lr\"]) ax.barh(plotdata.index, plotdata[\"lr_position\"], alpha=0.5) ax.axvline(0, color=\"black\", linestyle=\"--\", lw=1) ax.set_title(\"Validation\")  panel, ax = fig.main.add(chd.grid.Ax((1, height))) plotdata = scores.xs(\"train\", level=\"phase\").loc[model_info.index] ax.set_yticks([]) ax.barh(plotdata.index, plotdata[\"lr\"]) ax.barh(plotdata.index, plotdata[\"lr_position\"], alpha=0.5) ax.axvline(0, color=\"black\", linestyle=\"--\", lw=1) ax.set_title(\"Train\") fig.plot() In\u00a0[74]: Copied! <pre>model = models[\"original\"].eval()\n</pre> model = models[\"original\"].eval() In\u00a0[83]: Copied! <pre>gene_ix = 2\ngene = fragments.var.index[gene_ix]\ncluster_ixs = np.arange(clustering.n_clusters)\n\ncoordinates = torch.linspace(*fragments.regions.window, 1000)\nsizes = torch.linspace(0, 500, 50)\n\ndesign = chd.utils.crossing(\n    coordinate=coordinates,\n    size=sizes,\n    gene_ix=torch.tensor([gene_ix]),\n    cluster_ix=torch.tensor(cluster_ixs),\n)\ndesign[\"coordinate2\"] = design[\"coordinate\"] + design[\"size\"]\ndesign = design.loc[design[\"coordinate2\"] &lt;= fragments.regions.window[1]]\ndesign = design.loc[design[\"coordinate2\"] &gt;= fragments.regions.window[0]]\n</pre> gene_ix = 2 gene = fragments.var.index[gene_ix] cluster_ixs = np.arange(clustering.n_clusters)  coordinates = torch.linspace(*fragments.regions.window, 1000) sizes = torch.linspace(0, 500, 50)  design = chd.utils.crossing(     coordinate=coordinates,     size=sizes,     gene_ix=torch.tensor([gene_ix]),     cluster_ix=torch.tensor(cluster_ixs), ) design[\"coordinate2\"] = design[\"coordinate\"] + design[\"size\"] design = design.loc[design[\"coordinate2\"] &lt;= fragments.regions.window[1]] design = design.loc[design[\"coordinate2\"] &gt;= fragments.regions.window[0]] In\u00a0[84]: Copied! <pre>design[\"prob_left\"], design[\"prob_right\"] = model.evaluate_right(\n    torch.from_numpy(design[\"coordinate\"].values),\n    torch.from_numpy(design[\"coordinate2\"].values),\n    gene_ix=torch.from_numpy(design[\"gene_ix\"].values),\n    window=fragments.regions.window,\n    cluster_ix=torch.from_numpy(design[\"cluster_ix\"].values),\n)\n</pre> design[\"prob_left\"], design[\"prob_right\"] = model.evaluate_right(     torch.from_numpy(design[\"coordinate\"].values),     torch.from_numpy(design[\"coordinate2\"].values),     gene_ix=torch.from_numpy(design[\"gene_ix\"].values),     window=fragments.regions.window,     cluster_ix=torch.from_numpy(design[\"cluster_ix\"].values), ) In\u00a0[88]: Copied! <pre>fig = chd.grid.Figure(chd.grid.Grid(padding_height=0))\nwidth = 10\npanel, ax = fig.main.add_under(chd.grid.Panel((width, 0.5)))\nplotdata = design.loc[design[\"size\"] == 0].set_index([\"cluster_ix\", \"coordinate\"])[[\"prob_left\"]]\nfor cluster_ix, plotdata_cluster in plotdata.groupby(\"cluster_ix\"):\n    plotdata_cluster = plotdata_cluster.droplevel(\"cluster_ix\").sort_index()\n    ax.plot(plotdata_cluster.index, np.exp(plotdata_cluster[\"prob_left\"]), label=cluster_ix)\nax.set_xlim(*fragments.regions.window)\nax.set_xticks([])\n\nplotdata = simulation.peaks.query(\"gene == @design.gene_ix.iloc[0]\").sort_values(\"center\")\n\nax2 = panel.add_twinx()\nax2.scatter(plotdata[\"center\"], [0] * len(plotdata), c=plotdata[\"size_mean\"])\nax2.set_xlim(*fragments.regions.window)\n\npanel, ax = fig.main.add_under(chd.grid.Panel((width, 2)))\nplotdata = np.exp(design.groupby([\"size\", \"coordinate\"]).mean()[\"prob_right\"].unstack())\nax.matshow(plotdata, aspect=\"auto\", extent=(*fragments.regions.window, *plotdata.index[[-1, 0]]), cmap=\"viridis\")\nax.set_xticks([])\n# ax.set_yticks(np.arange(plotdata.shape[0]))\n# ax.set_yticklabels(plotdata.index)\n\npanel = fig.main.add_under(chd.data.motifscan.plot.Motifs(simulation.motifscan, gene, width=width))\n\nfig.plot()\n</pre> fig = chd.grid.Figure(chd.grid.Grid(padding_height=0)) width = 10 panel, ax = fig.main.add_under(chd.grid.Panel((width, 0.5))) plotdata = design.loc[design[\"size\"] == 0].set_index([\"cluster_ix\", \"coordinate\"])[[\"prob_left\"]] for cluster_ix, plotdata_cluster in plotdata.groupby(\"cluster_ix\"):     plotdata_cluster = plotdata_cluster.droplevel(\"cluster_ix\").sort_index()     ax.plot(plotdata_cluster.index, np.exp(plotdata_cluster[\"prob_left\"]), label=cluster_ix) ax.set_xlim(*fragments.regions.window) ax.set_xticks([])  plotdata = simulation.peaks.query(\"gene == @design.gene_ix.iloc[0]\").sort_values(\"center\")  ax2 = panel.add_twinx() ax2.scatter(plotdata[\"center\"], [0] * len(plotdata), c=plotdata[\"size_mean\"]) ax2.set_xlim(*fragments.regions.window)  panel, ax = fig.main.add_under(chd.grid.Panel((width, 2))) plotdata = np.exp(design.groupby([\"size\", \"coordinate\"]).mean()[\"prob_right\"].unstack()) ax.matshow(plotdata, aspect=\"auto\", extent=(*fragments.regions.window, *plotdata.index[[-1, 0]]), cmap=\"viridis\") ax.set_xticks([]) # ax.set_yticks(np.arange(plotdata.shape[0])) # ax.set_yticklabels(plotdata.index)  panel = fig.main.add_under(chd.data.motifscan.plot.Motifs(simulation.motifscan, gene, width=width))  fig.plot() In\u00a0[86]: Copied! <pre>main = chd.grid.Grid(padding_height=0.1)\nfig = chd.grid.Figure(main)\n\nnbins = np.array(model.mixture.transform.nbins)\nbincuts = np.concatenate([[0], np.cumsum(nbins)])\nbinmids = bincuts[:-1] + nbins / 2\n\nax = main[0, 0] = chd.grid.Ax((10, 0.25))\nax = ax.ax\nplotdata = (model.mixture.transform.unnormalized_heights.data.cpu().numpy())[[gene_ix]]\nax.imshow(plotdata, aspect=\"auto\")\nax.set_yticks([])\nfor b in bincuts:\n    ax.axvline(b - 0.5, color=\"black\", lw=0.5)\nax.set_xlim(0 - 0.5, plotdata.shape[1] - 0.5)\nax.set_xticks([])\nax.set_ylabel(\"$h_0$\", rotation=0, ha=\"right\", va=\"center\")\n\nax = main[1, 0] = chd.grid.Ax(dim=(10, model.n_clusters * 0.25))\nax = ax.ax\nplotdata = model.decoder.delta_height_weight.data[gene_ix].cpu().numpy()\nax.imshow(plotdata, aspect=\"auto\", cmap=mpl.cm.RdBu_r, vmax=np.log(2), vmin=np.log(1 / 2))\nax.set_yticks(range(len(clustering.cluster_info)))\nax.set_yticklabels(clustering.cluster_info.index, rotation=0, ha=\"right\")\nfor b in bincuts:\n    ax.axvline(b - 0.5, color=\"black\", lw=0.5)\nax.set_xlim(-0.5, plotdata.shape[1] - 0.5)\n\nax.set_xticks(bincuts - 0.5, minor=True)\nax.set_xticks(binmids - 0.5)\nax.set_xticklabels(nbins)\nax.xaxis.set_tick_params(length=0)\nax.xaxis.set_tick_params(length=5, which=\"minor\")\nax.set_ylabel(\"$\\Delta h$\", rotation=0, ha=\"right\", va=\"center\")\n\nax.set_xlabel(\"Resolution\")\n\nfig.plot()\n</pre> main = chd.grid.Grid(padding_height=0.1) fig = chd.grid.Figure(main)  nbins = np.array(model.mixture.transform.nbins) bincuts = np.concatenate([[0], np.cumsum(nbins)]) binmids = bincuts[:-1] + nbins / 2  ax = main[0, 0] = chd.grid.Ax((10, 0.25)) ax = ax.ax plotdata = (model.mixture.transform.unnormalized_heights.data.cpu().numpy())[[gene_ix]] ax.imshow(plotdata, aspect=\"auto\") ax.set_yticks([]) for b in bincuts:     ax.axvline(b - 0.5, color=\"black\", lw=0.5) ax.set_xlim(0 - 0.5, plotdata.shape[1] - 0.5) ax.set_xticks([]) ax.set_ylabel(\"$h_0$\", rotation=0, ha=\"right\", va=\"center\")  ax = main[1, 0] = chd.grid.Ax(dim=(10, model.n_clusters * 0.25)) ax = ax.ax plotdata = model.decoder.delta_height_weight.data[gene_ix].cpu().numpy() ax.imshow(plotdata, aspect=\"auto\", cmap=mpl.cm.RdBu_r, vmax=np.log(2), vmin=np.log(1 / 2)) ax.set_yticks(range(len(clustering.cluster_info))) ax.set_yticklabels(clustering.cluster_info.index, rotation=0, ha=\"right\") for b in bincuts:     ax.axvline(b - 0.5, color=\"black\", lw=0.5) ax.set_xlim(-0.5, plotdata.shape[1] - 0.5)  ax.set_xticks(bincuts - 0.5, minor=True) ax.set_xticks(binmids - 0.5) ax.set_xticklabels(nbins) ax.xaxis.set_tick_params(length=0) ax.xaxis.set_tick_params(length=5, which=\"minor\") ax.set_ylabel(\"$\\Delta h$\", rotation=0, ha=\"right\", va=\"center\")  ax.set_xlabel(\"Resolution\")  fig.plot()"},{"location":"benchmark/diff/simulated/#tryout","title":"Tryout\u00b6","text":""},{"location":"benchmark/diff/simulated/#train","title":"Train\u00b6","text":""},{"location":"benchmark/diff/simulated/#score","title":"Score\u00b6","text":""},{"location":"benchmark/diff/simulated/#interpret","title":"Interpret\u00b6","text":""},{"location":"benchmark/diff/timing/","title":"Timing","text":"In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nimport seaborn as sns\n\nsns.set_style(\"ticks\")\n%config InlineBackend.figure_format='retina'\n\nimport tqdm.auto as tqdm\nimport torch\nimport os\nimport time\n</pre> %load_ext autoreload %autoreload 2  import numpy as np import pandas as pd  import matplotlib.pyplot as plt import matplotlib as mpl  import seaborn as sns  sns.set_style(\"ticks\") %config InlineBackend.figure_format='retina'  import tqdm.auto as tqdm import torch import os import time In\u00a0[2]: Copied! <pre>import chromatinhd as chd\n\nchd.set_default_device(\"cuda:1\")\n</pre> import chromatinhd as chd  chd.set_default_device(\"cuda:1\") In\u00a0[3]: Copied! <pre>dataset_folder_original = chd.get_output() / \"datasets\" / \"pbmc10k\"\ntranscriptome_original = chd.data.Transcriptome(dataset_folder_original / \"transcriptome\")\nfragments_original = chd.data.Fragments(dataset_folder_original / \"fragments\" / \"10k10k\")\n</pre> dataset_folder_original = chd.get_output() / \"datasets\" / \"pbmc10k\" transcriptome_original = chd.data.Transcriptome(dataset_folder_original / \"transcriptome\") fragments_original = chd.data.Fragments(dataset_folder_original / \"fragments\" / \"10k10k\") In\u00a0[4]: Copied! <pre>genes_oi = transcriptome_original.var.sort_values(\"dispersions_norm\", ascending=False).head(30).index\nregions = fragments_original.regions.filter_genes(genes_oi)\nfragments = fragments_original.filter_genes(regions)\nfragments.create_cellxgene_indptr()\ntranscriptome = transcriptome_original.filter_genes(regions.coordinates.index)\n</pre> genes_oi = transcriptome_original.var.sort_values(\"dispersions_norm\", ascending=False).head(30).index regions = fragments_original.regions.filter_genes(genes_oi) fragments = fragments_original.filter_genes(regions) fragments.create_cellxgene_indptr() transcriptome = transcriptome_original.filter_genes(regions.coordinates.index) In\u00a0[5]: Copied! <pre>folds = chd.data.folds.Folds()\nfolds.sample_cells(fragments, 5)\n</pre> folds = chd.data.folds.Folds() folds.sample_cells(fragments, 5) In\u00a0[6]: Copied! <pre>clustering = chd.data.Clustering.from_labels(transcriptome_original.obs[\"celltype\"])\n</pre> clustering = chd.data.Clustering.from_labels(transcriptome_original.obs[\"celltype\"]) In\u00a0[7]: Copied! <pre>fold = folds[0]\n</pre> fold = folds[0] In\u00a0[8]: Copied! <pre>models = {}\nscores = []\n</pre> models = {} scores = [] In\u00a0[10]: Copied! <pre>import logging\n\nlogger = chd.models.diff.trainer.trainer.logger\nlogger.setLevel(logging.DEBUG)\nlogger.handlers = []\n# logger.handlers = [logging.StreamHandler()]\n</pre> import logging  logger = chd.models.diff.trainer.trainer.logger logger.setLevel(logging.DEBUG) logger.handlers = [] # logger.handlers = [logging.StreamHandler()] In\u00a0[11]: Copied! <pre>devices = pd.DataFrame({\"device\": [\"cuda:0\", \"cuda:1\", \"cpu\"]}).set_index(\"device\")\nfor device in devices.index:\n    if device != \"cpu\":\n        devices.loc[device, \"label\"] = torch.cuda.get_device_properties(device).name\n    else:\n        devices.loc[device, \"label\"] = os.popen(\"lscpu\").read().split(\"\\n\")[13].split(\": \")[-1].lstrip()\n</pre> devices = pd.DataFrame({\"device\": [\"cuda:0\", \"cuda:1\", \"cpu\"]}).set_index(\"device\") for device in devices.index:     if device != \"cpu\":         devices.loc[device, \"label\"] = torch.cuda.get_device_properties(device).name     else:         devices.loc[device, \"label\"] = os.popen(\"lscpu\").read().split(\"\\n\")[13].split(\": \")[-1].lstrip() In\u00a0[12]: Copied! <pre>scores = pd.DataFrame({\"device\": devices.index}).set_index(\"device\")\n</pre> scores = pd.DataFrame({\"device\": devices.index}).set_index(\"device\") In\u00a0[14]: Copied! <pre>for device in devices.index:\n    start = time.time()\n    model = chd.models.diff.model.cutnf.Model(\n        fragments,\n        clustering,\n    )\n    model.train_model(fragments, clustering, fold, n_epochs=10, device=device)\n    models[device] = model\n    end = time.time()\n    scores.loc[device, \"train\"] = end - start\n</pre> for device in devices.index:     start = time.time()     model = chd.models.diff.model.cutnf.Model(         fragments,         clustering,     )     model.train_model(fragments, clustering, fold, n_epochs=10, device=device)     models[device] = model     end = time.time()     scores.loc[device, \"train\"] = end - start <pre>  0%|          | 0/410 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/410 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/410 [00:00&lt;?, ?it/s]</pre> In\u00a0[15]: Copied! <pre>for device in devices.index:\n    genepositional = chd.models.diff.interpret.genepositional.GenePositional(\n        path=chd.get_output() / \"interpret\" / \"genepositional\"\n    )\n\n    start = time.time()\n    genepositional.score(fragments, clustering, [models[device]], force=True, device=device)\n    end = time.time()\n    scores.loc[device, \"inference\"] = end - start\n</pre> for device in devices.index:     genepositional = chd.models.diff.interpret.genepositional.GenePositional(         path=chd.get_output() / \"interpret\" / \"genepositional\"     )      start = time.time()     genepositional.score(fragments, clustering, [models[device]], force=True, device=device)     end = time.time()     scores.loc[device, \"inference\"] = end - start <pre>  0%|          | 0/30 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/30 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/30 [00:00&lt;?, ?it/s]</pre> In\u00a0[16]: Copied! <pre>fig = chd.grid.Figure(chd.grid.Wrap(padding_width=0.1))\nheight = len(scores) * 0.2\n\nplotdata = scores.copy().loc[devices.index]\n\npanel, ax = fig.main.add(chd.grid.Ax((1, height)))\nax.barh(plotdata.index, plotdata[\"train\"])\nax.set_yticks(np.arange(len(devices)))\nax.set_yticklabels(devices.label)\nax.axvline(0, color=\"black\", linestyle=\"--\", lw=1)\nax.set_title(\"Training\")\nax.set_xlabel(\"seconds\")\n\npanel, ax = fig.main.add(chd.grid.Ax((1, height)))\nax.barh(plotdata.index, plotdata[\"inference\"])\nax.axvline(0, color=\"black\", linestyle=\"--\", lw=1)\nax.set_title(\"Inference\")\nax.set_yticks([])\nfig.plot()\n</pre> fig = chd.grid.Figure(chd.grid.Wrap(padding_width=0.1)) height = len(scores) * 0.2  plotdata = scores.copy().loc[devices.index]  panel, ax = fig.main.add(chd.grid.Ax((1, height))) ax.barh(plotdata.index, plotdata[\"train\"]) ax.set_yticks(np.arange(len(devices))) ax.set_yticklabels(devices.label) ax.axvline(0, color=\"black\", linestyle=\"--\", lw=1) ax.set_title(\"Training\") ax.set_xlabel(\"seconds\")  panel, ax = fig.main.add(chd.grid.Ax((1, height))) ax.barh(plotdata.index, plotdata[\"inference\"]) ax.axvline(0, color=\"black\", linestyle=\"--\", lw=1) ax.set_title(\"Inference\") ax.set_yticks([]) fig.plot()"},{"location":"quickstart/0_install/","title":"Installation","text":"<p>ChromatinHD requires python 3.8. Currently, installation is only possible using pip. A conda version is coming soon.</p> <p><code>torch</code> and <code>torch-scatter</code> dependencies require special care, because their installation depends on the cuda version. As such, an installation in the form of <code>pip install torch</code> is prone to fail.</p> <p>First, follow instructions at https://pytorch.org/get-started/locally/ to install torch, given your cuda version.</p> <p>Second, follow instructions at https://github.com/rusty1s/pytorch_scatter?tab=readme-ov-file#binaries to install torch-scatter, given your pytorch and cuda version.</p> <p>Finally, ChromatinHD can be installed using <code>pip install chromatinhd</code></p> <p>To use GPU acceleration, ensure that a PyTorch version was installed with cuda enabled:</p> In\u00a0[2]: Copied! <pre>import torch\n\ntorch.cuda.is_available()  # should return True\ntorch.cuda.device_count()  # should be &gt;= 1\n</pre> import torch  torch.cuda.is_available()  # should return True torch.cuda.device_count()  # should be &gt;= 1 Out[2]: <pre>2</pre> <p>If not, follow the instructions at https://pytorch.org/get-started/locally/. You may have to re-install PyTorch.</p> In\u00a0[\u00a0]: hide_output Copied! <pre>import chromatinhd as chd\n</pre> import chromatinhd as chd <p>Coming soon</p> <p>I get an error <code>ModuleNotFoundError: No module named 'torch'</code></p> <p>An open issue with one our dependencies, <code>torch-scatter</code>, is that it requires to have torch installed.</p> <p>This can be fixed by first installing torch (https://pytorch.org/get-started/locally/), followed by installing pytorch-scatter</p>"},{"location":"quickstart/0_install/#installation","title":"Installation\u00b6","text":""},{"location":"quickstart/0_install/#using-pip","title":"Using pip\u00b6","text":""},{"location":"quickstart/0_install/#using-conda","title":"Using conda\u00b6","text":""},{"location":"quickstart/0_install/#frequently-asked-questions","title":"Frequently asked questions\u00b6","text":""},{"location":"quickstart/1_data/","title":"Data preparation","text":"In\u00a0[\u00a0]: hide_output Copied! <pre>import chromatinhd as chd\n</pre> import chromatinhd as chd <p>To speed up training and inference, ChromatinHD stores several intermediate files to disk. This includes preprocessed data and models. These will be stored in the example folder.</p> In\u00a0[19]: Copied! <pre>import pathlib\n\ndataset_folder = pathlib.Path(\"example\")\ndataset_folder.mkdir(exist_ok=True)\n</pre> import pathlib  dataset_folder = pathlib.Path(\"example\") dataset_folder.mkdir(exist_ok=True) <p>For this quickstart, we will use a tiny example dataset extracted from the 10X multiome PBMC example data, in which we only retained 50 genes. In your real situations, we typically want to use all genes. We'll copy over both the h5ad for the transcriptomics data, and the fragments.tsv for the accessibility data.</p> In\u00a0[21]: Copied! <pre>import pkg_resources\nimport shutil\n\nDATA_PATH = pathlib.Path(pkg_resources.resource_filename(\"chromatinhd\", \"data/examples/pbmc10ktiny/\"))\n\n# copy all files from data path to dataset folder\nfor file in DATA_PATH.iterdir():\n    shutil.copy(file, dataset_folder / file.name)\n</pre> import pkg_resources import shutil  DATA_PATH = pathlib.Path(pkg_resources.resource_filename(\"chromatinhd\", \"data/examples/pbmc10ktiny/\"))  # copy all files from data path to dataset folder for file in DATA_PATH.iterdir():     shutil.copy(file, dataset_folder / file.name) In\u00a0[22]: Copied! <pre>!ls {dataset_folder}\n</pre> !ls {dataset_folder} <pre>fragments.tsv  fragments.tsv.gz  fragments.tsv.gz.tbi  transcriptome.h5ad\n</pre> In\u00a0[23]: Copied! <pre>import scanpy as sc\n\nadata = sc.read(dataset_folder / \"transcriptome.h5ad\")\n</pre> import scanpy as sc  adata = sc.read(dataset_folder / \"transcriptome.h5ad\") In\u00a0[24]: Copied! <pre>transcriptome = chd.data.Transcriptome.from_adata(adata, path=dataset_folder / \"transcriptome\")\n</pre> transcriptome = chd.data.Transcriptome.from_adata(adata, path=dataset_folder / \"transcriptome\") In\u00a0[25]: Copied! <pre>transcriptome\n</pre> transcriptome Out[25]: example/transcriptome (chromatinhd.data.transcriptome.transcriptome.Transcriptome)<ul><li> var</li><li> obs</li><li>adata</li><li> layers ( X [10291,51], 2.0Mb)</li></ul> Batch effects <p>       Currently, none of the ChromatinHD models directly supports batch effects, although this will likely be added in the future. If you have batch effects, the current recommended workflow depends on the source of the batch effect:     <ul> <li>If it mainly comes from ambient mRNA, we recommend to use the corrected data. The reason is that this batch effect will likely not be present in the ATAC-seq data.</li> <li>If it mainly comes from biological differences (e.g. cell stress, patient differences, ...), we recommend to use the uncorrected data. The reason is that this batch effect will likely be reflected in the ATAC-seq data as well, given that the genes are truly differentially regulated between the cells.</li> </ul> </p> <p>ChromatinHD defines a set of regions of interest, typically surrounding transcription start sites of a gene. Since we typically do not know which transcription start sites are used, we can either use the canonical ones (as determined by e.g. ENCODE) or use the ATAC-seq data to select the one that is most open. We will use the latter option here.</p> <p>We first get the transcripts for each gene. We extract this from biomart using the ensembl gene ids, which in this case are used as the index of the <code>transcriptome.var</code>.</p> In\u00a0[26]: Copied! <pre>transcripts = chd.biomart.get_transcripts(chd.biomart.Dataset.from_genome(\"GRCh38\"), gene_ids=transcriptome.var.index)\nfragments_file = dataset_folder / \"fragments.tsv.gz\"\ntranscripts = chd.data.regions.select_tss_from_fragments(transcripts, fragments_file)\ntranscripts.head()\n</pre> transcripts = chd.biomart.get_transcripts(chd.biomart.Dataset.from_genome(\"GRCh38\"), gene_ids=transcriptome.var.index) fragments_file = dataset_folder / \"fragments.tsv.gz\" transcripts = chd.data.regions.select_tss_from_fragments(transcripts, fragments_file) transcripts.head() <pre>  0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/537 [00:00&lt;?, ?it/s]</pre> Out[26]: ensembl_transcript_id start end strand symbol transcript_biotype chrom tss n_fragments gene ENSG00000132465 ENST00000254801 70655541 70666508 -1 JCHAIN protein_coding chr4 70666508 43 ENSG00000107317 ENST00000640716 136975092 136979021 1 PTGDS protein_coding chr9 136975092 374 ENSG00000134532 ENST00000446891 23895852 24562487 -1 SOX5 protein_coding chr12 24562487 135 ENSG00000196628 ENST00000564403 55227936 55589839 -1 TCF4 protein_coding chr18 55589839 1526 ENSG00000184226 ENST00000377861 67201015 67230445 -1 PCDH9 protein_coding chr13 67230445 1982 <p>Now we can define the regions around the TSS. In this case we choose -10kb and +10kb around a TSS, although in real situations this will typically be much bigger (e.g. -100kb - +100kb)</p> In\u00a0[27]: Copied! <pre>regions = chd.data.Regions.from_transcripts(\n    transcripts,\n    path=dataset_folder / \"regions\",\n    window=[-100000, 100000],\n)\nregions\n</pre> regions = chd.data.Regions.from_transcripts(     transcripts,     path=dataset_folder / \"regions\",     window=[-100000, 100000], ) regions Out[27]: example/regions (chromatinhd.data.regions.Regions)<ul><li> coordinates</li><li>window</li></ul> Gene vs TSS coordinates <p>The coordinates of the canonical transcript often do not correspond to the gene annotation that are used for e.g. RNA-seq analysis. The reason is that gene coordinates are defined based on the largest transcript in both ends.   </p> <p>The fragment file should be indexed with tabix:</p> In\u00a0[28]: Copied! <pre>if not (dataset_folder / \"fragments.tsv.gz.tbi\").exists():\n    import pysam\n    pysam.tabix_index(str(dataset_folder / \"fragments.tsv.gz\"), preset = \"bed\")\n</pre> if not (dataset_folder / \"fragments.tsv.gz.tbi\").exists():     import pysam     pysam.tabix_index(str(dataset_folder / \"fragments.tsv.gz\"), preset = \"bed\") <p>We now create a ChromatinHD fragments object using the <code>fragments.tsv</code> file and a set of regions. This will populate a set of tensors on disk containing information on where each fragment lies within the region and to which cell and region it belongs:</p> In\u00a0[29]: Copied! <pre>fragments = chd.data.Fragments.from_fragments_tsv(\n    dataset_folder / \"fragments.tsv.gz\",\n    regions,\n    obs=transcriptome.obs,\n    path=dataset_folder / \"fragments\",\n)\nfragments\n</pre> fragments = chd.data.Fragments.from_fragments_tsv(     dataset_folder / \"fragments.tsv.gz\",     regions,     obs=transcriptome.obs,     path=dataset_folder / \"fragments\", ) fragments <pre>Processing fragments:   0%|          | 0/51 [00:00&lt;?, ?it/s]</pre> Out[29]: example/fragments (chromatinhd.data.fragments.fragments.Fragments)<ul><li> regions</li><li> coordinates [805384,2], 6.1Mb</li><li> mapping [805384,2], 6.1Mb</li><li> var</li><li> obs</li></ul> <p>During training or inference of any models, we often require fast access to all fragments belong to a particular cell and region. This can be sped up by precalculating pointers to each region and cell combination:</p> In\u00a0[30]: Copied! <pre>fragments.create_regionxcell_indptr()\n</pre> fragments.create_regionxcell_indptr() Out[30]: example/fragments (chromatinhd.data.fragments.fragments.Fragments)<ul><li> regions</li><li> coordinates [805384,2], 6.1Mb</li><li> mapping [805384,2], 6.1Mb</li><li> regionxcell_indptr [524842], 4.0Mb</li><li> var</li><li> obs</li></ul> How data is stored in ChromatinHD <p>We use zarr  format to store data, and either TensorStore  or Xarray  to load data as needed.    <p>The final set of data are the training folds that will be used to train - and test - the model. For basic models this is simply done by randomly sampling cells.</p> In\u00a0[31]: Copied! <pre>folds = chd.data.folds.Folds(dataset_folder / \"folds\" / \"5x1\").sample_cells(fragments, 5, 1)\nfolds\n</pre> folds = chd.data.folds.Folds(dataset_folder / \"folds\" / \"5x1\").sample_cells(fragments, 5, 1) folds Out[31]: example/folds/5x1 (chromatinhd.data.folds.folds.Folds)<ul><li>folds</li></ul> In\u00a0[32]: Copied! <pre>folds = chd.data.folds.Folds(dataset_folder / \"folds\" / \"5x5\").sample_cells(fragments, 5, 5)\nfolds\n</pre> folds = chd.data.folds.Folds(dataset_folder / \"folds\" / \"5x5\").sample_cells(fragments, 5, 5) folds Out[32]: example/folds/5x5 (chromatinhd.data.folds.folds.Folds)<ul><li>folds</li></ul> <p>Although only needed for some models, e.g. ChromatinHD-diff, for interpretation it can be helpful to store some clustering.</p> In\u00a0[33]: Copied! <pre>clustering = chd.data.Clustering.from_labels(adata.obs[\"celltype\"], path=dataset_folder / \"clustering\")\nclustering\n</pre> clustering = chd.data.Clustering.from_labels(adata.obs[\"celltype\"], path=dataset_folder / \"clustering\") clustering Out[33]: example/clustering (chromatinhd.data.clustering.clustering.Clustering)<ul><li>labels</li><li> indices [10291], 10.0Kb</li><li> var</li></ul> <p>We can also scan for motifs and store it on disk, to be used to link transcription factors to particular regions of interest. Models that use this data directly are forthcoming.</p> <p>Let's first download the HOCOMOCO motif data. This is a simple wrapper function that downloads and processes relevant motif data from the HOCOMOCO website.</p> In\u00a0[48]: Copied! <pre>import chromatinhd.data.motifscan.download\npwms, motifs = chd.data.motifscan.download.get_hocomoco(dataset_folder / \"motifs\", organism = \"human\")\nmotifs = motifs.query(\"quality in ['A', 'B']\")\nmotifs = motifs.loc[~motifs.index.isin([\"ZN384.H12CORE.0.PSM.A\"])]\n</pre> import chromatinhd.data.motifscan.download pwms, motifs = chd.data.motifscan.download.get_hocomoco(dataset_folder / \"motifs\", organism = \"human\") motifs = motifs.query(\"quality in ['A', 'B']\") motifs = motifs.loc[~motifs.index.isin([\"ZN384.H12CORE.0.PSM.A\"])] <p>You also need to provide the location where the genome fasta file is stored. In our case this is located at /data/genome/GRCh38/, which was installed using <code>genomepy.install_genome(\"GRCh38\", genomes_dir = \"/data/genome/\")</code>.</p> In\u00a0[49]: Copied! <pre>import genomepy\n\ngenomepy.install_genome(\"GRCh38\", genomes_dir=\"/data/genome/\")\n\nfasta_file = \"/data/genome/GRCh38/GRCh38.fa\"\n</pre> import genomepy  genomepy.install_genome(\"GRCh38\", genomes_dir=\"/data/genome/\")  fasta_file = \"/data/genome/GRCh38/GRCh38.fa\" <p>Motifs can than be scanned within the regions as follows:</p> In\u00a0[50]: Copied! <pre>motifscan = chd.data.Motifscan.from_pwms(\n    pwms,\n    regions,\n    motifs=motifs,\n    cutoff_col=\"cutoff_0.0001\",\n    fasta_file=fasta_file,\n    path=dataset_folder / \"motifscan\",\n)\nmotifscan\n</pre> motifscan = chd.data.Motifscan.from_pwms(     pwms,     regions,     motifs=motifs,     cutoff_col=\"cutoff_0.0001\",     fasta_file=fasta_file,     path=dataset_folder / \"motifscan\", ) motifscan <pre>  0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>0it [00:00, ?it/s]</pre> <pre>0it [00:00, ?it/s]</pre> Out[50]: example/motifscan (chromatinhd.data.motifscan.motifscan.Motifscan)<ul><li> regions</li><li> coordinates [3514943], 26.8Mb</li><li> region_indices [3514943], 13.4Mb</li><li> indices [3514943], 13.4Mb</li><li> scores [3514943], 13.4Mb</li><li> strands [3514943], 3.4Mb</li><li> motifs</li></ul> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"quickstart/1_data/#data-preparation","title":"Data preparation\u00b6","text":""},{"location":"quickstart/1_data/#transcriptomics","title":"Transcriptomics\u00b6","text":""},{"location":"quickstart/1_data/#regions-of-interest","title":"Regions of interest\u00b6","text":""},{"location":"quickstart/1_data/#atac-seq","title":"ATAC-seq\u00b6","text":"<p>ChromatinHD simply requires a <code>fragments.tsv</code> file. This contains for each fragment its chromosome, start, end and cell barcode.</p> <ul> <li>When using Cellranger, this file will be produced by the pipeline.</li> <li>If you have a bam file, you can use sinto to create the fragment file</li> </ul>"},{"location":"quickstart/1_data/#training-folds","title":"Training folds\u00b6","text":""},{"location":"quickstart/1_data/#optional-data","title":"Optional data\u00b6","text":""},{"location":"quickstart/1_data/#clusters","title":"Clusters\u00b6","text":""},{"location":"quickstart/1_data/#motif-scan","title":"Motif scan\u00b6","text":""},{"location":"quickstart/2_pred/","title":"ChromatinHD-pred","text":"In\u00a0[\u00a0]: hide_output Copied! <pre>import chromatinhd as chd\nimport matplotlib.pyplot as plt\n</pre> import chromatinhd as chd import matplotlib.pyplot as plt <p>ChromatinHD-pred uses accessibility fragments to predict gene expression. As such, it can detect features such as broad or narrow positioning of fragments, or fragment sizes, that are predictive for gene expression.</p> <p>We first load in all the input data which was created in the data preparation tutorial.</p> In\u00a0[3]: Copied! <pre>import pathlib\n\ndataset_folder = pathlib.Path(\"example\")\nfragments = chd.data.Fragments(dataset_folder / \"fragments\")\ntranscriptome = chd.data.Transcriptome(dataset_folder / \"transcriptome\")\nfolds = chd.data.folds.Folds(dataset_folder / \"folds\" / \"5x1\")\n</pre> import pathlib  dataset_folder = pathlib.Path(\"example\") fragments = chd.data.Fragments(dataset_folder / \"fragments\") transcriptome = chd.data.Transcriptome(dataset_folder / \"transcriptome\") folds = chd.data.folds.Folds(dataset_folder / \"folds\" / \"5x1\") <p>The basic ChromatinHD-pred model</p> In\u00a0[4]: Copied! <pre>models = chd.models.pred.model.multiscale.Models(dataset_folder / \"models\", reset=True)\n</pre> models = chd.models.pred.model.multiscale.Models(dataset_folder / \"models\", reset=True) In\u00a0[\u00a0]: hide_output Copied! <pre>models.train_models(\n    fragments=fragments, transcriptome=transcriptome, folds=folds, regions_oi=transcriptome.gene_id([\"CCL4\", \"IRF1\"])\n)\n</pre> models.train_models(     fragments=fragments, transcriptome=transcriptome, folds=folds, regions_oi=transcriptome.gene_id([\"CCL4\", \"IRF1\"]) ) <p>We will first check whether the model learned something, by comparing the predictive performance with a baseline</p> In\u00a0[6]: Copied! <pre>gene_cors = models.get_region_cors(fragments, transcriptome, folds)\ngene_cors[\"symbol\"] = gene_cors.index.map(transcriptome.symbol)\n</pre> gene_cors = models.get_region_cors(fragments, transcriptome, folds) gene_cors[\"symbol\"] = gene_cors.index.map(transcriptome.symbol) In\u00a0[7]: Copied! <pre>gene_cors\n</pre> gene_cors Out[7]: cor cor_n_fragments deltacor symbol gene ENSG00000275302 0.728446 0.409938 0.318507 CCL4 ENSG00000275302 0.752379 0.404049 0.348330 CCL4 ENSG00000275302 0.766739 0.409273 0.357466 CCL4 ENSG00000275302 0.804127 0.476547 0.327579 CCL4 ENSG00000275302 0.727691 0.434370 0.293321 CCL4 ENSG00000125347 0.731183 0.243202 0.487980 IRF1 ENSG00000125347 0.736824 0.228264 0.508560 IRF1 ENSG00000125347 0.697314 0.212770 0.484544 IRF1 ENSG00000125347 0.736628 0.201388 0.535240 IRF1 ENSG00000125347 0.697519 0.211218 0.486301 IRF1 In\u00a0[8]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(4, 4))\n\nfor name, group in gene_cors.iterrows():\n    ax.plot([0, 1], group[[\"cor_n_fragments\", \"cor\"]], color=\"#3338\", zorder=0, marker=\"o\", markersize=2)\nax.boxplot(\n    gene_cors[[\"cor_n_fragments\", \"cor\"]].values,\n    positions=[0, 1],\n    widths=0.1,\n    showfliers=False,\n    showmeans=True,\n    meanline=True,\n    meanprops={\"color\": \"red\", \"linewidth\": 2},\n)\nax.set_xticks([0, 1])\nax.set_ylim(0)\nax.set_xticklabels([\"# fragments\", \"ChromatinHD-pred\"])\nax.set_ylabel(\"$cor$\")\n</pre> import pandas as pd import matplotlib.pyplot as plt  fig, ax = plt.subplots(figsize=(4, 4))  for name, group in gene_cors.iterrows():     ax.plot([0, 1], group[[\"cor_n_fragments\", \"cor\"]], color=\"#3338\", zorder=0, marker=\"o\", markersize=2) ax.boxplot(     gene_cors[[\"cor_n_fragments\", \"cor\"]].values,     positions=[0, 1],     widths=0.1,     showfliers=False,     showmeans=True,     meanline=True,     meanprops={\"color\": \"red\", \"linewidth\": 2}, ) ax.set_xticks([0, 1]) ax.set_ylim(0) ax.set_xticklabels([\"# fragments\", \"ChromatinHD-pred\"]) ax.set_ylabel(\"$cor$\") Out[8]: <pre>Text(0, 0.5, '$cor$')</pre> <p>To determine which regions were important for the model to predict gene expression, we will censor fragments from windows of various sizes, and then check whether the model performance on a set of test cells decreased. This functionality is implemented in the <code>GeneMultiWindow</code> class. This will only run the censoring for a subset of genes to speed up interpretation.</p> In\u00a0[9]: Copied! <pre>censorer = chd.models.pred.interpret.MultiWindowCensorer(fragments.regions.window)\nregionmultiwindow = chd.models.pred.interpret.RegionMultiWindow.create(\n    path = models.path / \"interpret\" / \"regionmultiwindow\",\n    folds = folds,\n    transcriptome = transcriptome,\n    censorer = censorer,\n    fragments = fragments,\n)\n</pre> censorer = chd.models.pred.interpret.MultiWindowCensorer(fragments.regions.window) regionmultiwindow = chd.models.pred.interpret.RegionMultiWindow.create(     path = models.path / \"interpret\" / \"regionmultiwindow\",     folds = folds,     transcriptome = transcriptome,     censorer = censorer,     fragments = fragments, ) In\u00a0[10]: Copied! <pre>regionmultiwindow.score(\n    models = models,\n    regions = transcriptome.gene_id(\n        [\n            \"CCL4\",\n            \"IRF1\",\n        ]\n    ),\n    folds = folds,\n)\n</pre> regionmultiwindow.score(     models = models,     regions = transcriptome.gene_id(         [             \"CCL4\",             \"IRF1\",         ]     ),     folds = folds, ) <pre>  0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> Out[10]: example/models/interpret/regionmultiwindow (chromatinhd.models.pred.interpret.regionmultiwindow.RegionMultiWindow)<ul><li> scores 816.5Kb</li><li> interpolation 161.0Kb</li><li>censorer</li></ul> In\u00a0[11]: Copied! <pre>regionmultiwindow.interpolate()\n</pre> regionmultiwindow.interpolate() <pre>  0%|          | 0/51 [00:00&lt;?, ?it/s]</pre> Out[11]: example/models/interpret/regionmultiwindow (chromatinhd.models.pred.interpret.regionmultiwindow.RegionMultiWindow)<ul><li> scores 816.5Kb</li><li> interpolation 506.7Kb</li><li>censorer</li></ul> <p>We can visualize the predictivity as follows. This shows which regions of the genome are positively and negatively associated with gene expression.</p> In\u00a0[12]: Copied! <pre>symbol = \"IRF1\"\n\nfig = chd.grid.Figure(chd.grid.Grid(padding_height=0.05))\nwidth = 10\n\nregion = fragments.regions.coordinates.loc[transcriptome.gene_id(symbol)]\npanel_genes = chd.plot.genome.genes.Genes.from_region(region, width=width, genome = \"GRCh38\")\nfig.main.add_under(panel_genes)\n\npanel_pileup = chd.models.pred.plot.Pileup.from_regionmultiwindow(\n    regionmultiwindow, transcriptome.gene_id(symbol), width=width\n)\nfig.main.add_under(panel_pileup)\n\npanel_predictivity = chd.models.pred.plot.Predictivity.from_regionmultiwindow(\n    regionmultiwindow, transcriptome.gene_id(symbol), width=width\n)\nfig.main.add_under(panel_predictivity)\n\nfig.plot()\n</pre> symbol = \"IRF1\"  fig = chd.grid.Figure(chd.grid.Grid(padding_height=0.05)) width = 10  region = fragments.regions.coordinates.loc[transcriptome.gene_id(symbol)] panel_genes = chd.plot.genome.genes.Genes.from_region(region, width=width, genome = \"GRCh38\") fig.main.add_under(panel_genes)  panel_pileup = chd.models.pred.plot.Pileup.from_regionmultiwindow(     regionmultiwindow, transcriptome.gene_id(symbol), width=width ) fig.main.add_under(panel_pileup)  panel_predictivity = chd.models.pred.plot.Predictivity.from_regionmultiwindow(     regionmultiwindow, transcriptome.gene_id(symbol), width=width ) fig.main.add_under(panel_predictivity)  fig.plot() <p>Given that accessibility can be sparse, we often simply visualize the predictivity in regions with at least a minimum of accessibility.</p> <p>Let's first select regions based on the number of fragments. Regions that are close together will be merged.</p> In\u00a0[13]: Copied! <pre>symbol = \"IRF1\"\n# symbol = \"CCL4\"\ngene_id = transcriptome.gene_id(symbol)\n</pre> symbol = \"IRF1\" # symbol = \"CCL4\" gene_id = transcriptome.gene_id(symbol) In\u00a0[14]: Copied! <pre># decrease the lost_cutoff to see more regions\nregions = regionmultiwindow.select_regions(gene_id, lost_cutoff = 0.15)\nbreaking = chd.grid.Breaking(regions)\n</pre> # decrease the lost_cutoff to see more regions regions = regionmultiwindow.select_regions(gene_id, lost_cutoff = 0.15) breaking = chd.grid.Breaking(regions) In\u00a0[15]: Copied! <pre>fig = chd.grid.Figure(chd.grid.Grid(padding_height=0.05))\n\nregion = fragments.regions.coordinates.loc[transcriptome.gene_id(symbol)]\npanel_genes = chd.plot.genome.genes.GenesBroken.from_region(region, breaking=breaking, genome = \"GRCh38\")\nfig.main.add_under(panel_genes)\n\npanel_pileup = chd.models.pred.plot.PileupBroken.from_regionmultiwindow(\n    regionmultiwindow, gene_id, breaking=breaking\n)\nfig.main.add_under(panel_pileup)\n\npanel_predictivity = chd.models.pred.plot.PredictivityBroken.from_regionmultiwindow(\n    regionmultiwindow, gene_id, breaking=breaking, ymax = -0.1\n)\nfig.main.add_under(panel_predictivity)\n\nfig.plot()\n</pre> fig = chd.grid.Figure(chd.grid.Grid(padding_height=0.05))  region = fragments.regions.coordinates.loc[transcriptome.gene_id(symbol)] panel_genes = chd.plot.genome.genes.GenesBroken.from_region(region, breaking=breaking, genome = \"GRCh38\") fig.main.add_under(panel_genes)  panel_pileup = chd.models.pred.plot.PileupBroken.from_regionmultiwindow(     regionmultiwindow, gene_id, breaking=breaking ) fig.main.add_under(panel_pileup)  panel_predictivity = chd.models.pred.plot.PredictivityBroken.from_regionmultiwindow(     regionmultiwindow, gene_id, breaking=breaking, ymax = -0.1 ) fig.main.add_under(panel_predictivity)  fig.plot() <p>In a similar fashion we can determine the co-predictivity per position.</p> In\u00a0[16]: Copied! <pre>censorer = chd.models.pred.interpret.WindowCensorer(fragments.regions.window)\nregionpairwindow = chd.models.pred.interpret.RegionPairWindow(models.path / \"interpret\" / \"regionpairwindow\", reset=True)\nregionpairwindow.score(models, censorer = censorer, folds = folds, fragments = fragments)\n</pre> censorer = chd.models.pred.interpret.WindowCensorer(fragments.regions.window) regionpairwindow = chd.models.pred.interpret.RegionPairWindow(models.path / \"interpret\" / \"regionpairwindow\", reset=True) regionpairwindow.score(models, censorer = censorer, folds = folds, fragments = fragments) <pre>  0%|          | 0/51 [00:00&lt;?, ?it/s]</pre> Out[16]: example/models/interpret/regionpairwindow (chromatinhd.models.pred.interpret.regionpairwindow.RegionPairWindow)<ul><li>design</li><li> scores ( ENSG00000275302 31.4Kb,  ENSG00000125347 40.5Kb)</li><li> interaction ( ENSG00000275302 [5,1000,1000], 4.0Mb,  ENSG00000125347 [5,1000,1000], 11.7Mb)</li></ul> In\u00a0[17]: Copied! <pre>symbol = \"IRF1\"\n# symbol = \"CCL4\"\ngene_id = transcriptome.gene_id(symbol)\n</pre> symbol = \"IRF1\" # symbol = \"CCL4\" gene_id = transcriptome.gene_id(symbol) In\u00a0[18]: Copied! <pre>windows = regionmultiwindow.select_regions(gene_id, lost_cutoff = 0.2)\nbreaking = chd.grid.Breaking(windows)\n</pre> windows = regionmultiwindow.select_regions(gene_id, lost_cutoff = 0.2) breaking = chd.grid.Breaking(windows) In\u00a0[19]: Copied! <pre>fig = chd.grid.Figure(chd.grid.Grid(padding_height=0.05))\nwidth = 10\n\n# genes\nregion = fragments.regions.coordinates.loc[gene_id]\npanel_genes = chd.plot.genome.genes.GenesBroken.from_region(region, breaking = breaking)\nfig.main.add_under(panel_genes)\n\n# pileup\npanel_pileup = chd.models.pred.plot.PileupBroken.from_regionmultiwindow(\n    regionmultiwindow, gene_id, breaking = breaking,\n)\nfig.main.add_under(panel_pileup)\n\n# predictivity\npanel_predictivity = chd.models.pred.plot.PredictivityBroken.from_regionmultiwindow(\n    regionmultiwindow, gene_id, breaking=breaking\n)\nfig.main.add_under(panel_predictivity)\n\n# copredictivity\npanel_copredictivity = chd.models.pred.plot.CopredictivityBroken.from_regionpairwindow(\n    regionpairwindow, gene_id, breaking = breaking\n)\nfig.main.add_under(panel_copredictivity, padding = 0.)\n\nfig.plot()\n</pre> fig = chd.grid.Figure(chd.grid.Grid(padding_height=0.05)) width = 10  # genes region = fragments.regions.coordinates.loc[gene_id] panel_genes = chd.plot.genome.genes.GenesBroken.from_region(region, breaking = breaking) fig.main.add_under(panel_genes)  # pileup panel_pileup = chd.models.pred.plot.PileupBroken.from_regionmultiwindow(     regionmultiwindow, gene_id, breaking = breaking, ) fig.main.add_under(panel_pileup)  # predictivity panel_predictivity = chd.models.pred.plot.PredictivityBroken.from_regionmultiwindow(     regionmultiwindow, gene_id, breaking=breaking ) fig.main.add_under(panel_predictivity)  # copredictivity panel_copredictivity = chd.models.pred.plot.CopredictivityBroken.from_regionpairwindow(     regionpairwindow, gene_id, breaking = breaking ) fig.main.add_under(panel_copredictivity, padding = 0.)  fig.plot() In\u00a0[23]: Copied! <pre>plotdata = regionpairwindow.get_plotdata(gene_id, windows = windows).sort_values(\"cor\")\nplotdata[\"deltacor_min\"] = plotdata[[\"deltacor1\", \"deltacor2\"]].values.min(1)\nplotdata[\"deltacor_max\"] = plotdata[[\"deltacor1\", \"deltacor2\"]].values.max(1)\nplotdata[\"deltacor_prod\"] = plotdata[\"deltacor1\"] * plotdata[\"deltacor2\"]\nplotdata[\"deltacor_sum\"] = plotdata[\"deltacor1\"] + plotdata[\"deltacor2\"]\n\nfig, ax = plt.subplots()\nax.scatter(plotdata[\"deltacor_prod\"].abs(), plotdata[\"cor\"].abs())\n</pre> plotdata = regionpairwindow.get_plotdata(gene_id, windows = windows).sort_values(\"cor\") plotdata[\"deltacor_min\"] = plotdata[[\"deltacor1\", \"deltacor2\"]].values.min(1) plotdata[\"deltacor_max\"] = plotdata[[\"deltacor1\", \"deltacor2\"]].values.max(1) plotdata[\"deltacor_prod\"] = plotdata[\"deltacor1\"] * plotdata[\"deltacor2\"] plotdata[\"deltacor_sum\"] = plotdata[\"deltacor1\"] + plotdata[\"deltacor2\"]  fig, ax = plt.subplots() ax.scatter(plotdata[\"deltacor_prod\"].abs(), plotdata[\"cor\"].abs()) Out[23]: <pre>&lt;matplotlib.collections.PathCollection at 0x7f60e85b8640&gt;</pre> In\u00a0[27]: Copied! <pre>predictive_windows = regionmultiwindow.extract_predictive_windows()\n</pre> predictive_windows = regionmultiwindow.extract_predictive_windows() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"quickstart/2_pred/#chromatinhd-pred","title":"ChromatinHD-pred\u00b6","text":""},{"location":"quickstart/2_pred/#train-the-models","title":"Train the models\u00b6","text":""},{"location":"quickstart/2_pred/#some-quality-checks","title":"Some quality checks\u00b6","text":""},{"location":"quickstart/2_pred/#predictivity-per-position","title":"Predictivity per position\u00b6","text":""},{"location":"quickstart/2_pred/#visualizing-predictivity","title":"Visualizing predictivity\u00b6","text":""},{"location":"quickstart/2_pred/#co-predictivity","title":"Co-predictivity\u00b6","text":""},{"location":"quickstart/2_pred/#visualization-of-co-predictivity","title":"Visualization of co-predictivity\u00b6","text":""},{"location":"quickstart/2_pred/#extract-predictive-regions","title":"Extract predictive regions\u00b6","text":""},{"location":"quickstart/3_diff/","title":"ChromatinHD-diff","text":"In\u00a0[\u00a0]: hide_output Copied! <pre>import chromatinhd as chd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport pandas as pd\nimport numpy as np\n</pre> import chromatinhd as chd import matplotlib.pyplot as plt import matplotlib as mpl import pandas as pd import numpy as np <p>ChromatinHD-pred uses accessibility fragments to predict gene expression. As such, it can detect features such as broad or narrow positioning of fragments, or fragment sizes, that are predictive for gene expression.</p> <p>We first load in all the input data which was created in the data preparation tutorial.</p> In\u00a0[3]: Copied! <pre>import pathlib\n\ndataset_folder = pathlib.Path(\"example\")\nfragments = chd.data.Fragments(dataset_folder / \"fragments\")\ntranscriptome = chd.data.Transcriptome(dataset_folder / \"transcriptome\")\nfolds = chd.data.folds.Folds(dataset_folder / \"folds\" / \"5x1\")\nclustering = chd.data.Clustering(dataset_folder / \"clustering\")\n</pre> import pathlib  dataset_folder = pathlib.Path(\"example\") fragments = chd.data.Fragments(dataset_folder / \"fragments\") transcriptome = chd.data.Transcriptome(dataset_folder / \"transcriptome\") folds = chd.data.folds.Folds(dataset_folder / \"folds\" / \"5x1\") clustering = chd.data.Clustering(dataset_folder / \"clustering\") <p>The basic ChromatinHD-diff model</p> In\u00a0[4]: Copied! <pre>models = chd.models.diff.model.binary.Models(dataset_folder / \"models\" / \"diff\", reset=True)\n</pre> models = chd.models.diff.model.binary.Models(dataset_folder / \"models\" / \"diff\", reset=True) In\u00a0[\u00a0]: hide_output Copied! <pre>models.train_models(fragments = fragments, clustering = clustering, folds = folds)\n</pre> models.train_models(fragments = fragments, clustering = clustering, folds = folds) <p>To determine which windows, whether they are small or large, are differential, we use the <code>chromatinhd.models.diff.interpret.regionpositional.RegionPositional</code> class. This will extract the probability distribution of finding a cut site at a particular position in a cluster, and compare them across clusters.</p> In\u00a0[6]: Copied! <pre>import chromatinhd.models.diff.interpret.regionpositional\n</pre> import chromatinhd.models.diff.interpret.regionpositional In\u00a0[7]: Copied! <pre>clustering.cluster_info.index.name = \"cluster\"\n</pre> clustering.cluster_info.index.name = \"cluster\" In\u00a0[8]: Copied! <pre>regionpositional = chromatinhd.models.diff.interpret.regionpositional.RegionPositional(\n    path=models.path / \"interpret\" / \"regionpositional\",\n)\nregionpositional.score(\n    fragments = fragments,\n    clustering = clustering,\n    models = models,\n    force=True,\n)\n</pre> regionpositional = chromatinhd.models.diff.interpret.regionpositional.RegionPositional(     path=models.path / \"interpret\" / \"regionpositional\", ) regionpositional.score(     fragments = fragments,     clustering = clustering,     models = models,     force=True, ) <pre>  0%|          | 0/51 [00:00&lt;?, ?it/s]</pre> Out[8]: example/models/diff/interpret/regionpositional (chromatinhd.models.diff.interpret.regionpositional.RegionPositional)<ul><li> regions</li><li> probs ( ENSG00000132465 [13,4001], 362.5Kb,  ENSG00000107317 [13,4001], 379.3Kb,  ENSG00000134532 [13,4001], 330.4Kb, ...)</li></ul> <p>To avoid overplotting, we will plot only the window close to the TSS.</p> In\u00a0[9]: Copied! <pre>symbol = \"IRF1\"\ngene_id = transcriptome.gene_id(symbol)\n</pre> symbol = \"IRF1\" gene_id = transcriptome.gene_id(symbol) In\u00a0[10]: Copied! <pre>window = [-10000, 10000]\n</pre> window = [-10000, 10000] In\u00a0[11]: Copied! <pre>fig = chd.grid.Figure(chd.grid.Grid(padding_height=0.05, padding_width=0.05))\nwidth = 10\n\nregion = fragments.regions.coordinates.loc[transcriptome.gene_id(symbol)]\npanel_genes = chd.plot.genome.genes.Genes.from_region(region, width=width, window = window)\nfig.main.add_under(panel_genes)\n\npanel_differential = chd.models.diff.plot.Differential.from_regionpositional(\n    transcriptome.gene_id(symbol), regionpositional, cluster_info=clustering.cluster_info, panel_height=0.5, width=width, window = window\n)\nfig.main.add_under(panel_differential)\n\npanel_expression = chd.models.diff.plot.DifferentialExpression.from_transcriptome(\n    transcriptome=transcriptome, clustering=clustering, gene=transcriptome.gene_id(symbol), panel_height=0.5\n)\nfig.main.add_right(panel_expression, row=panel_differential)\n\nfig.plot()\n</pre> fig = chd.grid.Figure(chd.grid.Grid(padding_height=0.05, padding_width=0.05)) width = 10  region = fragments.regions.coordinates.loc[transcriptome.gene_id(symbol)] panel_genes = chd.plot.genome.genes.Genes.from_region(region, width=width, window = window) fig.main.add_under(panel_genes)  panel_differential = chd.models.diff.plot.Differential.from_regionpositional(     transcriptome.gene_id(symbol), regionpositional, cluster_info=clustering.cluster_info, panel_height=0.5, width=width, window = window ) fig.main.add_under(panel_differential)  panel_expression = chd.models.diff.plot.DifferentialExpression.from_transcriptome(     transcriptome=transcriptome, clustering=clustering, gene=transcriptome.gene_id(symbol), panel_height=0.5 ) fig.main.add_right(panel_expression, row=panel_differential)  fig.plot() In\u00a0[20]: Copied! <pre>windows = regionpositional.select_windows(gene_id)\nbreaking = chd.grid.Breaking(windows)\n</pre> windows = regionpositional.select_windows(gene_id) breaking = chd.grid.Breaking(windows) In\u00a0[21]: Copied! <pre>fig = chd.grid.Figure(chd.grid.Grid(padding_height=0.05, padding_width=0.05))\nwidth = 10\n\nregion = fragments.regions.coordinates.loc[transcriptome.gene_id(symbol)]\npanel_genes = chd.plot.genome.genes.GenesBroken.from_region(region, breaking=breaking)\nfig.main.add_under(panel_genes)\n\npanel_differential = chd.models.diff.plot.DifferentialBroken.from_regionpositional(\n    transcriptome.gene_id(symbol), regionpositional, cluster_info=clustering.cluster_info, breaking=breaking, window = window\n)\nfig.main.add_under(panel_differential)\n\npanel_expression = chd.models.diff.plot.DifferentialExpression.from_transcriptome(\n    transcriptome=transcriptome, clustering=clustering, gene=transcriptome.gene_id(symbol)\n)\nfig.main.add_right(panel_expression, row=panel_differential)\n\nfig.plot()\n</pre> fig = chd.grid.Figure(chd.grid.Grid(padding_height=0.05, padding_width=0.05)) width = 10  region = fragments.regions.coordinates.loc[transcriptome.gene_id(symbol)] panel_genes = chd.plot.genome.genes.GenesBroken.from_region(region, breaking=breaking) fig.main.add_under(panel_genes)  panel_differential = chd.models.diff.plot.DifferentialBroken.from_regionpositional(     transcriptome.gene_id(symbol), regionpositional, cluster_info=clustering.cluster_info, breaking=breaking, window = window ) fig.main.add_under(panel_differential)  panel_expression = chd.models.diff.plot.DifferentialExpression.from_transcriptome(     transcriptome=transcriptome, clustering=clustering, gene=transcriptome.gene_id(symbol) ) fig.main.add_right(panel_expression, row=panel_differential)  fig.plot() <p>We can load in the motifscan that was pre-calculated before:</p> In\u00a0[60]: Copied! <pre>motifscan = chd.data.Motifscan(\n    path=dataset_folder / \"motifscan\",\n)\n</pre> motifscan = chd.data.Motifscan(     path=dataset_folder / \"motifscan\", ) <p>To do enrichment, we need windows with increased and decreased accessibility. We will first determine windows of interest, which have at least a minimum of accessibility (<code>slices</code>). Within these, we will determine which windows are differentially accessible within a cell type.</p> In\u00a0[61]: Copied! <pre>slices = regionpositional.calculate_slices()\ndifferential_slices = regionpositional.calculate_differential_slices(slices)\n</pre> slices = regionpositional.calculate_slices() differential_slices = regionpositional.calculate_differential_slices(slices) <pre>  0%|          | 0/51 [00:00&lt;?, ?it/s]</pre> In\u00a0[62]: Copied! <pre>slicescores = differential_slices.get_slice_scores(regions = fragments.regions, clustering = clustering)\n\nslicescores[\"slice\"] = pd.Categorical(slicescores[\"region_ix\"].astype(str) + \":\" + slicescores[\"start\"].astype(str) + \"-\" + slicescores[\"end\"].astype(str))\nslices = slicescores.groupby(\"slice\")[[\"region_ix\", \"start\", \"end\"]].first()\n</pre> slicescores = differential_slices.get_slice_scores(regions = fragments.regions, clustering = clustering)  slicescores[\"slice\"] = pd.Categorical(slicescores[\"region_ix\"].astype(str) + \":\" + slicescores[\"start\"].astype(str) + \"-\" + slicescores[\"end\"].astype(str)) slices = slicescores.groupby(\"slice\")[[\"region_ix\", \"start\", \"end\"]].first() In\u00a0[63]: Copied! <pre>motifscan.create_region_indptr()\n</pre> motifscan.create_region_indptr() In\u00a0[64]: Copied! <pre>slicecounts = motifscan.count_slices(slices)\nenrichment = chd.models.diff.interpret.enrichment.enrichment_cluster_vs_clusters(slicescores, slicecounts)\nenrichment[\"log_odds\"] = np.log(enrichment[\"odds\"])\n</pre> slicecounts = motifscan.count_slices(slices) enrichment = chd.models.diff.interpret.enrichment.enrichment_cluster_vs_clusters(slicescores, slicecounts) enrichment[\"log_odds\"] = np.log(enrichment[\"odds\"]) <pre>Counting slices:   0%|          | 0/1812 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/13 [00:00&lt;?, ?it/s]</pre> In\u00a0[65]: Copied! <pre>motifs_enriched = motifscan.motifs.loc[\n    enrichment.query(\"q_value &lt; 0.05\").sort_values(\"odds\", ascending = False).groupby(\"cluster\").head(2).index.get_level_values(\"motif\").unique()\n]\nmotifs_enriched = motifs_enriched.loc[~pd.isnull(motifs_enriched[\"symbol\"])]\n</pre> motifs_enriched = motifscan.motifs.loc[     enrichment.query(\"q_value &lt; 0.05\").sort_values(\"odds\", ascending = False).groupby(\"cluster\").head(2).index.get_level_values(\"motif\").unique() ] motifs_enriched = motifs_enriched.loc[~pd.isnull(motifs_enriched[\"symbol\"])] In\u00a0[66]: Copied! <pre>fig, ax = plt.subplots()\nnorm = mpl.colors.Normalize(-2, 2)\nplotdata = enrichment[\"log_odds\"].unstack().loc[:, motifs_enriched.index].T\nax.matshow(plotdata, norm = norm, cmap = mpl.cm.RdBu_r)\n\nax.set_yticks(range(len(motifs_enriched)))\nax.set_yticklabels(motifs_enriched[\"symbol\"])\n\nax.set_xticks(range(len(clustering.var)))\nax.set_xticklabels(clustering.var.loc[plotdata.columns].index, rotation = 90)\n;\n</pre> fig, ax = plt.subplots() norm = mpl.colors.Normalize(-2, 2) plotdata = enrichment[\"log_odds\"].unstack().loc[:, motifs_enriched.index].T ax.matshow(plotdata, norm = norm, cmap = mpl.cm.RdBu_r)  ax.set_yticks(range(len(motifs_enriched))) ax.set_yticklabels(motifs_enriched[\"symbol\"])  ax.set_xticks(range(len(clustering.var))) ax.set_xticklabels(clustering.var.loc[plotdata.columns].index, rotation = 90) ; Out[66]: <pre>''</pre> <p>Because we are focusing on only 50 genes in this quickstart, this enrichment can be biased and of low quality. For a more reliable enrichment, we suggest to use at least 500 regions for you analyses.</p> In\u00a0[67]: Copied! <pre># symbol = \"IRF1\"; cluster_oi = \"CD14+ Monocytes\"\n# symbol = \"CCL4\"; cluster_oi = \"CD8 activated T\"\nsymbol = \"EBF1\"; cluster_oi = \"memory B\"\n</pre> # symbol = \"IRF1\"; cluster_oi = \"CD14+ Monocytes\" # symbol = \"CCL4\"; cluster_oi = \"CD8 activated T\" symbol = \"EBF1\"; cluster_oi = \"memory B\" In\u00a0[68]: Copied! <pre>gene_id = transcriptome.gene_id(symbol)\n\nwindows = regionpositional.select_windows(gene_id)\nbreaking = chd.grid.Breaking(windows)\n</pre> gene_id = transcriptome.gene_id(symbol)  windows = regionpositional.select_windows(gene_id) breaking = chd.grid.Breaking(windows) In\u00a0[69]: Copied! <pre>cluster_info = clustering.var.loc[[cluster_oi]]\n</pre> cluster_info = clustering.var.loc[[cluster_oi]] <p>We will group the enriched motifs based on the similarity in motif counts.</p> In\u00a0[70]: Copied! <pre>merge_cutoff = 0.3\nq_value_cutoff = 0.05\nodds_cutoff = 1.2\n\nslicecors = pd.DataFrame(np.corrcoef(slicecounts.T), index = slicecounts.columns, columns = slicecounts.columns)\nenrichment_oi = enrichment.loc[cluster_oi].query(\"q_value &lt; @q_value_cutoff\").query(\"odds &gt; @odds_cutoff\").sort_values(\"q_value\")\nmotif_grouping = {}\nfor motif_id in enrichment_oi.index:\n    slicecors_oi = slicecors.loc[motif_id, list(motif_grouping.keys())]\n    if (slicecors_oi &lt; merge_cutoff).all():\n        motif_grouping[motif_id] = [motif_id]\n        enrichment_oi.loc[motif_id, \"group\"] = motif_id\n    else:\n        group = slicecors_oi.sort_values(ascending = False).index[0]\n        motif_grouping[group].append(motif_id)\n        enrichment_oi.loc[motif_id, \"group\"] = group\n</pre> merge_cutoff = 0.3 q_value_cutoff = 0.05 odds_cutoff = 1.2  slicecors = pd.DataFrame(np.corrcoef(slicecounts.T), index = slicecounts.columns, columns = slicecounts.columns) enrichment_oi = enrichment.loc[cluster_oi].query(\"q_value &lt; @q_value_cutoff\").query(\"odds &gt; @odds_cutoff\").sort_values(\"q_value\") motif_grouping = {} for motif_id in enrichment_oi.index:     slicecors_oi = slicecors.loc[motif_id, list(motif_grouping.keys())]     if (slicecors_oi &lt; merge_cutoff).all():         motif_grouping[motif_id] = [motif_id]         enrichment_oi.loc[motif_id, \"group\"] = motif_id     else:         group = slicecors_oi.sort_values(ascending = False).index[0]         motif_grouping[group].append(motif_id)         enrichment_oi.loc[motif_id, \"group\"] = group <pre>/data/peak_free_atac/software/peak_free_atac/lib/python3.9/site-packages/numpy/lib/function_base.py:2853: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/data/peak_free_atac/software/peak_free_atac/lib/python3.9/site-packages/numpy/lib/function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n</pre> In\u00a0[71]: Copied! <pre>motif_info = motifscan.motifs.loc[enrichment_oi.index].join(enrichment_oi)\n</pre> motif_info = motifscan.motifs.loc[enrichment_oi.index].join(enrichment_oi) In\u00a0[72]: Copied! <pre>fig = chd.grid.Figure(chd.grid.Grid(padding_height=0.05, padding_width=0.05))\nwidth = 10\n\nregion = fragments.regions.coordinates.loc[gene_id]\npanel_genes = chd.plot.genome.genes.GenesBroken.from_region(region, breaking=breaking)\nfig.main.add_under(panel_genes)\n\npanel_differential = chd.models.diff.plot.DifferentialBroken.from_regionpositional(\n    gene_id, regionpositional, cluster_info=cluster_info, breaking=breaking, window = window\n)\nfig.main.add_under(panel_differential)\n\npanel_expression = chd.models.diff.plot.DifferentialExpression.from_transcriptome(\n    transcriptome=transcriptome, clustering=clustering, gene=gene_id, cluster_info=cluster_info,\n)\nfig.main.add_right(panel_expression, row=panel_differential)\n\npanel_motifs = chd.data.motifscan.plot.GroupedMotifsBroken(\n    gene = gene_id, motifscan=motifscan, motifs_oi = motif_info, breaking = breaking,\n)\nfig.main.add_under(panel_motifs)\n\nfig.plot()\n</pre> fig = chd.grid.Figure(chd.grid.Grid(padding_height=0.05, padding_width=0.05)) width = 10  region = fragments.regions.coordinates.loc[gene_id] panel_genes = chd.plot.genome.genes.GenesBroken.from_region(region, breaking=breaking) fig.main.add_under(panel_genes)  panel_differential = chd.models.diff.plot.DifferentialBroken.from_regionpositional(     gene_id, regionpositional, cluster_info=cluster_info, breaking=breaking, window = window ) fig.main.add_under(panel_differential)  panel_expression = chd.models.diff.plot.DifferentialExpression.from_transcriptome(     transcriptome=transcriptome, clustering=clustering, gene=gene_id, cluster_info=cluster_info, ) fig.main.add_right(panel_expression, row=panel_differential)  panel_motifs = chd.data.motifscan.plot.GroupedMotifsBroken(     gene = gene_id, motifscan=motifscan, motifs_oi = motif_info, breaking = breaking, ) fig.main.add_under(panel_motifs)  fig.plot() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"quickstart/3_diff/#chromatinhd-diff","title":"ChromatinHD-diff\u00b6","text":""},{"location":"quickstart/3_diff/#train-the-models","title":"Train the models\u00b6","text":""},{"location":"quickstart/3_diff/#interpret-positionally","title":"Interpret positionally\u00b6","text":""},{"location":"quickstart/3_diff/#plot-differential-accessibility-at-a-specific-window","title":"Plot differential accessibility at a specific window\u00b6","text":""},{"location":"quickstart/3_diff/#plot-differential-accessibility-for-all-windows","title":"Plot differential accessibility for all windows\u00b6","text":""},{"location":"quickstart/3_diff/#differential-tfbss","title":"Differential TFBSs\u00b6","text":""},{"location":"quickstart/3_diff/#visualize-binding-sites-and-differential-accessibility-for-an-specific-gene-cell-cluster","title":"Visualize binding sites and differential accessibility for an specific gene &amp; cell cluster\u00b6","text":""},{"location":"reference/data/clustering/","title":"Clustering","text":""},{"location":"reference/data/clustering/#chromatinhd.data.clustering.Clustering","title":"<code>chromatinhd.data.clustering.Clustering</code>","text":"<p>         Bases: <code>Flow</code></p> Source code in <code>src/chromatinhd/data/clustering/clustering.py</code> <pre><code>class Clustering(Flow):\n    labels: pd.DataFrame = Stored()\n    \"Labels for each cell.\"\n\n    indices: np.array = Tensorstore(dtype=\"&lt;i4\")\n    \"Indices for each cell.\"\n\n    var: pd.DataFrame = StoredDataFrame(index_name=\"cluster\")\n    \"Information for each cluster, such as a label, color, ...\"\n\n    @classmethod\n    def from_labels(\n        cls,\n        labels: pd.Series,\n        var: pd.DataFrame = None,\n        path: PathLike = None,\n        overwrite=False,\n    ) -&gt; Clustering:\n\"\"\"\n        Create a Clustering object from a series of labels.\n\n        Parameters:\n            labels:\n                Series of labels for each cell, with index corresponding to cell\n                names.\n            path:\n                Folder where the clustering information will be stored.\n            overwrite:\n                Whether to overwrite the clustering information if it already\n                exists.\n\n        Returns:\n            Clustering object.\n\n        \"\"\"\n        self = cls(path, reset=overwrite)\n\n        if not overwrite and self.o.labels.exists(self):\n            return self\n\n        if not isinstance(labels, pd.Series):\n            labels = pd.Series(labels).astype(\"category\")\n        elif not labels.dtype.name == \"category\":\n            labels = labels.astype(\"category\")\n        self.labels = labels\n        self.indices = labels.cat.codes.values\n\n        if var is None:\n            var = (\n                pd.DataFrame(\n                    {\n                        \"cluster\": labels.cat.categories,\n                        \"label\": labels.cat.categories,\n                    }\n                )\n                .set_index(\"cluster\")\n                .loc[labels.cat.categories]\n            )\n            var[\"n_cells\"] = labels.value_counts()\n        else:\n            var = var.reindex(labels.cat.categories)\n            var[\"label\"] = labels.cat.categories\n        self.var = var\n        return self\n\n    @property\n    def n_clusters(self):\n        return len(self.labels.cat.categories)\n\n    # temporarily link cluster_info to var\n    @property\n    def cluster_info(self):\n        return self.var\n\n    @cluster_info.setter\n    def cluster_info(self, cluster_info):\n        self.var = cluster_info\n</code></pre>"},{"location":"reference/data/clustering/#chromatinhd.data.clustering.clustering.Clustering.indices","title":"<code>indices: np.array = Tensorstore(dtype='&lt;i4')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Indices for each cell.</p>"},{"location":"reference/data/clustering/#chromatinhd.data.clustering.clustering.Clustering.labels","title":"<code>labels: pd.DataFrame = Stored()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Labels for each cell.</p>"},{"location":"reference/data/clustering/#chromatinhd.data.clustering.clustering.Clustering.var","title":"<code>var: pd.DataFrame = StoredDataFrame(index_name='cluster')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Information for each cluster, such as a label, color, ...</p>"},{"location":"reference/data/clustering/#chromatinhd.data.clustering.clustering.Clustering.from_labels","title":"<code>from_labels(labels, var=None, path=None, overwrite=False)</code>  <code>classmethod</code>","text":"<p>Create a Clustering object from a series of labels.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>pd.Series</code> <p>Series of labels for each cell, with index corresponding to cell names.</p> required <code>path</code> <code>PathLike</code> <p>Folder where the clustering information will be stored.</p> <code>None</code> <code>overwrite</code> <p>Whether to overwrite the clustering information if it already exists.</p> <code>False</code> <p>Returns:</p> Type Description <code>Clustering</code> <p>Clustering object.</p> Source code in <code>src/chromatinhd/data/clustering/clustering.py</code> <pre><code>@classmethod\ndef from_labels(\n    cls,\n    labels: pd.Series,\n    var: pd.DataFrame = None,\n    path: PathLike = None,\n    overwrite=False,\n) -&gt; Clustering:\n\"\"\"\n    Create a Clustering object from a series of labels.\n\n    Parameters:\n        labels:\n            Series of labels for each cell, with index corresponding to cell\n            names.\n        path:\n            Folder where the clustering information will be stored.\n        overwrite:\n            Whether to overwrite the clustering information if it already\n            exists.\n\n    Returns:\n        Clustering object.\n\n    \"\"\"\n    self = cls(path, reset=overwrite)\n\n    if not overwrite and self.o.labels.exists(self):\n        return self\n\n    if not isinstance(labels, pd.Series):\n        labels = pd.Series(labels).astype(\"category\")\n    elif not labels.dtype.name == \"category\":\n        labels = labels.astype(\"category\")\n    self.labels = labels\n    self.indices = labels.cat.codes.values\n\n    if var is None:\n        var = (\n            pd.DataFrame(\n                {\n                    \"cluster\": labels.cat.categories,\n                    \"label\": labels.cat.categories,\n                }\n            )\n            .set_index(\"cluster\")\n            .loc[labels.cat.categories]\n        )\n        var[\"n_cells\"] = labels.value_counts()\n    else:\n        var = var.reindex(labels.cat.categories)\n        var[\"label\"] = labels.cat.categories\n    self.var = var\n    return self\n</code></pre>"},{"location":"reference/data/folds/","title":"Folds","text":""},{"location":"reference/data/folds/#chromatinhd.data.folds.Folds","title":"<code>chromatinhd.data.folds.Folds</code>","text":"<p>         Bases: <code>Flow</code></p> <p>Folds of multiple cell and reion combinations</p> Source code in <code>src/chromatinhd/data/folds/folds.py</code> <pre><code>class Folds(Flow):\n\"\"\"\n    Folds of multiple cell and reion combinations\n    \"\"\"\n\n    folds: dict = Stored()\n\"\"\"The folds\"\"\"\n\n    def sample_cells(\n        self,\n        fragments: Fragments,\n        n_folds: int,\n        n_repeats: int = 1,\n        overwrite: bool = False,\n        seed: int = 1,\n    ):\n\"\"\"\n        Sample cells and regions into folds\n\n        Parameters:\n            fragments:\n                the fragments\n            n_folds:\n                the number of folds\n            n_repeats:\n                the number of repeats\n            overwrite:\n                whether to overwrite existing folds\n        \"\"\"\n        if not overwrite and self.get(\"folds\").exists(self):\n            return self\n\n        folds = []\n\n        for repeat_ix in range(n_repeats):\n            generator = np.random.RandomState(repeat_ix * seed)\n\n            cells_all = generator.permutation(fragments.n_cells)\n\n            cell_bins = np.floor((np.arange(len(cells_all)) / (len(cells_all) / n_folds)))\n\n            for i in range(n_folds):\n                cells_train = cells_all[cell_bins != i]\n                cells_validation_test = cells_all[cell_bins == i]\n                cells_validation = cells_validation_test[: (len(cells_validation_test) // 2)]\n                cells_test = cells_validation_test[(len(cells_validation_test) // 2) :]\n\n                folds.append(\n                    {\n                        \"cells_train\": cells_train,\n                        \"cells_validation\": cells_validation,\n                        \"cells_test\": cells_test,\n                        \"repeat\": repeat_ix,\n                    }\n                )\n        self.folds = folds\n\n        return self\n\n    def sample_cellxregion(\n        self,\n        fragments: Fragments,\n        n_folds: int,\n        n_repeats: int = 1,\n        stratify_by_chromosome=True,\n        overwrite: bool = False,\n    ):\n\"\"\"\n        Sample cells and regions into folds\n\n        Parameters:\n            fragments:\n                the fragments\n            n_folds:\n                the number of folds\n            n_repeats:\n                the number of repeats\n            overwrite:\n                whether to overwrite existing folds\n        \"\"\"\n        if not overwrite and self.get(\"folds\").exists(self):\n            return self\n\n        folds = []\n\n        for repeat_ix in range(n_repeats):\n            generator = np.random.RandomState(repeat_ix)\n\n            cells_all = generator.permutation(fragments.n_cells)\n\n            cell_bins = np.floor((np.arange(len(cells_all)) / (len(cells_all) / n_folds)))\n\n            regions_all = np.arange(fragments.n_regions)\n\n            if stratify_by_chromosome:\n                chr_column = \"chr\" if \"chr\" in fragments.regions.coordinates.columns else \"chrom\"\n                chr_order = generator.permutation(fragments.regions.coordinates[chr_column].unique())\n                region_chrs = pd.Categorical(\n                    fragments.regions.coordinates[chr_column].astype(str), categories=chr_order\n                ).codes\n                region_bins = np.floor((region_chrs / (len(chr_order) / n_folds))).astype(int)\n            else:\n                region_bins = np.floor((np.arange(len(regions_all)) / (len(regions_all) / n_folds)))\n\n            for i in range(n_folds):\n                cells_train = cells_all[cell_bins != i]\n                cells_validation_test = cells_all[cell_bins == i]\n                cells_validation = cells_validation_test[: (len(cells_validation_test) // 2)]\n                cells_test = cells_validation_test[(len(cells_validation_test) // 2) :]\n\n                regions_train = regions_all[region_bins != i]\n                regions_validation_test = generator.permutation(regions_all[region_bins == i])\n                regions_validation = regions_validation_test[: (len(regions_validation_test) // 2)]\n                regions_test = regions_validation_test[(len(regions_validation_test) // 2) :]\n\n                folds.append(\n                    {\n                        \"cells_train\": cells_train,\n                        \"cells_validation\": cells_validation,\n                        \"cells_test\": cells_test,\n                        \"regions_train\": regions_train,\n                        \"regions_validation\": regions_validation,\n                        \"regions_test\": regions_test,\n                        \"repeat\": repeat_ix,\n                    }\n                )\n        self.folds = folds\n        return self\n\n    def __getitem__(self, ix):\n        return self.folds[ix]\n\n    def __len__(self):\n        return len(self.folds)\n</code></pre>"},{"location":"reference/data/folds/#chromatinhd.data.folds.folds.Folds.folds","title":"<code>folds: dict = Stored()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The folds</p>"},{"location":"reference/data/folds/#chromatinhd.data.folds.folds.Folds.sample_cells","title":"<code>sample_cells(fragments, n_folds, n_repeats=1, overwrite=False, seed=1)</code>","text":"<p>Sample cells and regions into folds</p> <p>Parameters:</p> Name Type Description Default <code>fragments</code> <code>Fragments</code> <p>the fragments</p> required <code>n_folds</code> <code>int</code> <p>the number of folds</p> required <code>n_repeats</code> <code>int</code> <p>the number of repeats</p> <code>1</code> <code>overwrite</code> <code>bool</code> <p>whether to overwrite existing folds</p> <code>False</code> Source code in <code>src/chromatinhd/data/folds/folds.py</code> <pre><code>def sample_cells(\n    self,\n    fragments: Fragments,\n    n_folds: int,\n    n_repeats: int = 1,\n    overwrite: bool = False,\n    seed: int = 1,\n):\n\"\"\"\n    Sample cells and regions into folds\n\n    Parameters:\n        fragments:\n            the fragments\n        n_folds:\n            the number of folds\n        n_repeats:\n            the number of repeats\n        overwrite:\n            whether to overwrite existing folds\n    \"\"\"\n    if not overwrite and self.get(\"folds\").exists(self):\n        return self\n\n    folds = []\n\n    for repeat_ix in range(n_repeats):\n        generator = np.random.RandomState(repeat_ix * seed)\n\n        cells_all = generator.permutation(fragments.n_cells)\n\n        cell_bins = np.floor((np.arange(len(cells_all)) / (len(cells_all) / n_folds)))\n\n        for i in range(n_folds):\n            cells_train = cells_all[cell_bins != i]\n            cells_validation_test = cells_all[cell_bins == i]\n            cells_validation = cells_validation_test[: (len(cells_validation_test) // 2)]\n            cells_test = cells_validation_test[(len(cells_validation_test) // 2) :]\n\n            folds.append(\n                {\n                    \"cells_train\": cells_train,\n                    \"cells_validation\": cells_validation,\n                    \"cells_test\": cells_test,\n                    \"repeat\": repeat_ix,\n                }\n            )\n    self.folds = folds\n\n    return self\n</code></pre>"},{"location":"reference/data/folds/#chromatinhd.data.folds.folds.Folds.sample_cellxregion","title":"<code>sample_cellxregion(fragments, n_folds, n_repeats=1, stratify_by_chromosome=True, overwrite=False)</code>","text":"<p>Sample cells and regions into folds</p> <p>Parameters:</p> Name Type Description Default <code>fragments</code> <code>Fragments</code> <p>the fragments</p> required <code>n_folds</code> <code>int</code> <p>the number of folds</p> required <code>n_repeats</code> <code>int</code> <p>the number of repeats</p> <code>1</code> <code>overwrite</code> <code>bool</code> <p>whether to overwrite existing folds</p> <code>False</code> Source code in <code>src/chromatinhd/data/folds/folds.py</code> <pre><code>def sample_cellxregion(\n    self,\n    fragments: Fragments,\n    n_folds: int,\n    n_repeats: int = 1,\n    stratify_by_chromosome=True,\n    overwrite: bool = False,\n):\n\"\"\"\n    Sample cells and regions into folds\n\n    Parameters:\n        fragments:\n            the fragments\n        n_folds:\n            the number of folds\n        n_repeats:\n            the number of repeats\n        overwrite:\n            whether to overwrite existing folds\n    \"\"\"\n    if not overwrite and self.get(\"folds\").exists(self):\n        return self\n\n    folds = []\n\n    for repeat_ix in range(n_repeats):\n        generator = np.random.RandomState(repeat_ix)\n\n        cells_all = generator.permutation(fragments.n_cells)\n\n        cell_bins = np.floor((np.arange(len(cells_all)) / (len(cells_all) / n_folds)))\n\n        regions_all = np.arange(fragments.n_regions)\n\n        if stratify_by_chromosome:\n            chr_column = \"chr\" if \"chr\" in fragments.regions.coordinates.columns else \"chrom\"\n            chr_order = generator.permutation(fragments.regions.coordinates[chr_column].unique())\n            region_chrs = pd.Categorical(\n                fragments.regions.coordinates[chr_column].astype(str), categories=chr_order\n            ).codes\n            region_bins = np.floor((region_chrs / (len(chr_order) / n_folds))).astype(int)\n        else:\n            region_bins = np.floor((np.arange(len(regions_all)) / (len(regions_all) / n_folds)))\n\n        for i in range(n_folds):\n            cells_train = cells_all[cell_bins != i]\n            cells_validation_test = cells_all[cell_bins == i]\n            cells_validation = cells_validation_test[: (len(cells_validation_test) // 2)]\n            cells_test = cells_validation_test[(len(cells_validation_test) // 2) :]\n\n            regions_train = regions_all[region_bins != i]\n            regions_validation_test = generator.permutation(regions_all[region_bins == i])\n            regions_validation = regions_validation_test[: (len(regions_validation_test) // 2)]\n            regions_test = regions_validation_test[(len(regions_validation_test) // 2) :]\n\n            folds.append(\n                {\n                    \"cells_train\": cells_train,\n                    \"cells_validation\": cells_validation,\n                    \"cells_test\": cells_test,\n                    \"regions_train\": regions_train,\n                    \"regions_validation\": regions_validation,\n                    \"regions_test\": regions_test,\n                    \"repeat\": repeat_ix,\n                }\n            )\n    self.folds = folds\n    return self\n</code></pre>"},{"location":"reference/data/fragments/","title":"Fragments","text":""},{"location":"reference/data/fragments/#chromatinhd.data.fragments.Fragments","title":"<code>chromatinhd.data.fragments.Fragments</code>","text":"<p>         Bases: <code>Flow</code></p> <p>Fragments positioned within regions. Fragments are sorted by the region, position within the region (left cut site) and cell.</p> <p>The object can also store several precalculated tensors that are used for efficient loading of fragments. See create_regionxcell_indptr for more information.</p> Source code in <code>src/chromatinhd/data/fragments/fragments.py</code> <pre><code>class Fragments(Flow):\n\"\"\"\n    Fragments positioned within regions. Fragments are sorted by the region, position within the region (left cut site) and cell.\n\n    The object can also store several precalculated tensors that are used for efficient loading of fragments. See create_regionxcell_indptr for more information.\n    \"\"\"\n\n    regions: Regions = Linked()\n\"\"\"The regions in which (part of) the fragments are located and centered.\"\"\"\n\n    coordinates: TensorstoreInstance = Tensorstore(dtype=\"&lt;i4\", chunks=[100000, 2], compression=None, shape=[0, 2])\n\"\"\"Coordinates of the two cut sites.\"\"\"\n\n    mapping: TensorstoreInstance = Tensorstore(dtype=\"&lt;i4\", chunks=[100000, 2], compression=\"blosc\", shape=[0, 2])\n\"\"\"Mapping of a fragment to a cell (first column) and a region (second column)\"\"\"\n\n    regionxcell_indptr: np.ndarray = Tensorstore(dtype=\"&lt;i8\", chunks=[100000], compression=\"blosc\")\n\"\"\"Index pointers to the regionxcell fragment positions\"\"\"\n\n    def create_regionxcell_indptr(self, overwrite=False) -&gt; Fragments:\n\"\"\"\n        Creates pointers to each individual region x cell combination from the mapping tensor.\n\n        Returns:\n            The same object with `regionxcell_indptr` populated\n        \"\"\"\n\n        if self.o.regionxcell_indptr.exists(self) and not overwrite:\n            return self\n\n        regionxcell_ix = self.mapping[:, 1] * self.n_cells + self.mapping[:, 0]\n\n        if not (np.diff(regionxcell_ix) &gt;= 0).all():\n            raise ValueError(\"Fragments should be ordered by regionxcell (ascending)\")\n\n        if not self.mapping[:, 0].max() &lt; self.n_cells:\n            raise ValueError(\"First column of mapping should be smaller than the number of cells\")\n\n        n_regionxcell = self.n_regions * self.n_cells\n        regionxcell_indptr = np.pad(np.cumsum(np.bincount(regionxcell_ix, minlength=n_regionxcell), 0), (1, 0))\n        assert self.coordinates.shape[0] == regionxcell_indptr[-1]\n        if not (np.diff(regionxcell_indptr) &gt;= 0).all():\n            raise ValueError(\"Fragments should be ordered by regionxcell (ascending)\")\n        self.regionxcell_indptr[:] = regionxcell_indptr\n\n        return self\n\n    var: pd.DataFrame = TSV()\n\"\"\"DataFrame containing information about regions.\"\"\"\n\n    obs: pd.DataFrame = TSV()\n\"\"\"DataFrame containing information about cells.\"\"\"\n\n    @functools.cached_property\n    def n_regions(self):\n\"\"\"Number of regions\"\"\"\n        return self.var.shape[0]\n\n    @functools.cached_property\n    def n_cells(self):\n\"\"\"Number of cells\"\"\"\n        return self.obs.shape[0]\n\n    @property\n    def local_cellxregion_ix(self):\n        return self.mapping[:, 0] * self.n_regions + self.mapping[:, 1]\n\n    def estimate_fragment_per_cellxregion(self):\n        return math.ceil(self.coordinates.shape[0] / self.n_cells / self.n_regions)\n\n    @class_or_instancemethod\n    def from_fragments_tsv(\n        cls,\n        fragments_file: PathLike,\n        regions: Regions,\n        obs: pd.DataFrame,\n        cell_column: str = None,\n        path: PathLike = None,\n        overwrite: bool = False,\n        reuse: bool = True,\n        batch_size: int = 1e6,\n    ) -&gt; Fragments:\n\"\"\"\n        Create a Fragments object from a fragments tsv file\n\n        Parameters:\n            fragments_file:\n                Location of the fragments tab-separate file created by e.g. CellRanger or sinto\n            obs:\n                DataFrame containing information about cells.\n                The index should be the cell names as present in the fragments file.\n                Alternatively, the column containing cell ids can be specified using the `cell_column` argument.\n            regions:\n                Regions from which the fragments will be extracted.\n            cell_column:\n                Column name in the `obs` DataFrame containing the cell names.\n                If None, the index of the `obs` DataFrame is used.\n            path:\n                Folder in which the fragments data will be stored.\n            overwrite:\n                Whether to overwrite the data if it already exists.\n            reuse:\n                Whether to reuse existing data if it exists.\n            batch_size:\n                Number of fragments to process before saving. Lower this number if you run out of memory.\n        Returns:\n            A new Fragments object\n        \"\"\"\n\n        if isinstance(fragments_file, str):\n            fragments_file = pathlib.Path(fragments_file)\n        if isinstance(path, str):\n            path = pathlib.Path(path)\n        if not fragments_file.exists():\n            raise FileNotFoundError(f\"File {fragments_file} does not exist\")\n        if not overwrite and path.exists() and not reuse:\n            raise FileExistsError(\n                f\"Folder {path} already exists, use `overwrite=True` to overwrite, or `reuse=True` to reuse existing data\"\n            )\n\n        # regions information\n        var = pd.DataFrame(index=regions.coordinates.index)\n        var[\"ix\"] = np.arange(var.shape[0])\n\n        # cell information\n        obs = obs.copy()\n        obs[\"ix\"] = np.arange(obs.shape[0])\n        if cell_column is None:\n            cell_to_cell_ix = obs[\"ix\"].to_dict()\n        else:\n            cell_to_cell_ix = obs.set_index(cell_column)[\"ix\"].to_dict()\n\n        self = cls.create(path=path, obs=obs, var=var, regions=regions, reset=overwrite)\n\n        # read the fragments file\n        try:\n            import pysam\n        except ImportError as e:\n            raise ImportError(\n                \"pysam is required to read fragments files. Install using `pip install pysam` or `conda install -c bioconda pysam`\"\n            ) from e\n        fragments_tabix = pysam.TabixFile(str(fragments_file))\n\n        # process regions\n        pbar = tqdm.tqdm(\n            enumerate(regions.coordinates.iterrows()),\n            total=regions.coordinates.shape[0],\n            leave=False,\n            desc=\"Processing fragments\",\n        )\n\n        self.mapping.open_creator()\n        self.coordinates.open_creator()\n\n        mapping_processed = []\n        coordinates_processed = []\n\n        for region_ix, (region_id, region_info) in pbar:\n            pbar.set_description(f\"{region_id}\")\n\n            strand = region_info[\"strand\"]\n            if \"tss\" in region_info:\n                tss = region_info[\"tss\"]\n            else:\n                tss = region_info[\"start\"]\n\n            coordinates_raw, mapping_raw = _fetch_fragments_region(\n                fragments_tabix=fragments_tabix,\n                chrom=region_info[\"chrom\"],\n                start=region_info[\"start\"],\n                end=region_info[\"end\"],\n                tss=tss,\n                strand=strand,\n                cell_to_cell_ix=cell_to_cell_ix,\n                region_ix=region_ix,\n            )\n\n            mapping_processed.append(mapping_raw)\n            coordinates_processed.append(coordinates_raw)\n\n            if sum(mapping_raw.shape[0] for mapping_raw in mapping_processed) &gt;= batch_size:\n                self.mapping.extend(np.concatenate(mapping_processed).astype(np.int32))\n                self.coordinates.extend(np.concatenate(coordinates_processed).astype(np.int32))\n                mapping_processed = []\n                coordinates_processed = []\n\n            del mapping_raw\n            del coordinates_raw\n\n        if len(mapping_processed) &gt; 0:\n            self.mapping.extend(np.concatenate(mapping_processed).astype(np.int32))\n            self.coordinates.extend(np.concatenate(coordinates_processed).astype(np.int32))\n\n        return self\n\n    @class_or_instancemethod\n    def from_multiple_fragments_tsv(\n        cls,\n        fragments_files: [PathLike],\n        regions: Regions,\n        obs: pd.DataFrame,\n        cell_column: str = \"cell_original\",\n        batch_column: str = \"batch\",\n        path: PathLike = None,\n        overwrite: bool = False,\n        reuse: bool = True,\n        batch_size: int = 1e6,\n    ) -&gt; Fragments:\n\"\"\"\n        Create a Fragments object from multiple fragments tsv file\n\n        Parameters:\n            fragments_files:\n                Location of the fragments tab-separate file created by e.g. CellRanger or sinto\n            obs:\n                DataFrame containing information about cells.\n                The index should be the cell names as present in the fragments file.\n                Alternatively, the column containing cell ids can be specified using the `cell_column` argument.\n            regions:\n                Regions from which the fragments will be extracted.\n            cell_column:\n                Column name in the `obs` DataFrame containing the cell names.\n            batch_column:\n                Column name in the `obs` DataFrame containing the batch indices.\n                If None, will default to batch\n            path:\n                Folder in which the fragments data will be stored.\n            overwrite:\n                Whether to overwrite the data if it already exists.\n            reuse:\n                Whether to reuse existing data if it exists.\n            batch_size:\n                Number of fragments to process before saving. Lower this number if you run out of memory.\n        Returns:\n            A new Fragments object\n        \"\"\"\n\n        if not isinstance(fragments_files, (list, tuple)):\n            fragments_files = [fragments_files]\n\n        fragments_files_ = []\n        for fragments_file in fragments_files:\n            if isinstance(fragments_file, str):\n                fragments_file = pathlib.Path(fragments_file)\n            if not fragments_file.exists():\n                raise FileNotFoundError(f\"File {fragments_file} does not exist\")\n            fragments_files_.append(fragments_file)\n        fragments_files = fragments_files_\n\n        if not overwrite and path.exists() and not reuse:\n            raise FileExistsError(\n                f\"Folder {path} already exists, use `overwrite=True` to overwrite, or `reuse=True` to reuse existing data\"\n            )\n\n        # regions information\n        var = pd.DataFrame(index=regions.coordinates.index)\n        var[\"ix\"] = np.arange(var.shape[0])\n\n        # cell information\n        obs = obs.copy()\n        obs[\"ix\"] = np.arange(obs.shape[0])\n\n        if batch_column is None:\n            raise ValueError(\"batch_column should be specified\")\n        if batch_column not in obs.columns:\n            raise ValueError(f\"Column {batch_column} not in obs\")\n        if obs[batch_column].dtype != \"int\":\n            raise ValueError(f\"Column {batch_column} should be an integer column\")\n        if not obs[batch_column].max() == len(fragments_files) - 1:\n            raise ValueError(f\"Column {batch_column} should contain values between 0 and {len(fragments_files) - 1}\")\n        cell_to_cell_ix_batches = [\n            obs.loc[obs[batch_column] == batch].set_index(cell_column)[\"ix\"] for batch in obs[batch_column].unique()\n        ]\n\n        self = cls.create(path=path, obs=obs, var=var, regions=regions, reset=overwrite)\n\n        # read the fragments file\n        try:\n            import pysam\n        except ImportError as e:\n            raise ImportError(\n                \"pysam is required to read fragments files. Install using `pip install pysam` or `conda install -c bioconda pysam`\"\n            ) from e\n        fragments_tabix_batches = [pysam.TabixFile(str(fragments_file)) for fragments_file in fragments_files]\n\n        # process regions\n        pbar = tqdm.tqdm(\n            enumerate(regions.coordinates.iterrows()),\n            total=regions.coordinates.shape[0],\n            leave=False,\n            desc=\"Processing fragments\",\n        )\n\n        self.mapping.open_creator()\n        self.coordinates.open_creator()\n\n        mapping_processed = []\n        coordinates_processed = []\n\n        for region_ix, (region_id, region_info) in pbar:\n            pbar.set_description(f\"{region_id}\")\n\n            strand = region_info[\"strand\"]\n            if \"tss\" in region_info:\n                tss = region_info[\"tss\"]\n            else:\n                tss = region_info[\"start\"]\n\n            mapping_raw = []\n            coordinates_raw = []\n\n            for fragments_tabix, cell_to_cell_ix in zip(fragments_tabix_batches, cell_to_cell_ix_batches):\n                coordinates_raw_batch, mapping_raw_batch = _fetch_fragments_region(\n                    fragments_tabix=fragments_tabix,\n                    chrom=region_info[\"chrom\"],\n                    start=region_info[\"start\"],\n                    end=region_info[\"end\"],\n                    tss=tss,\n                    strand=strand,\n                    cell_to_cell_ix=cell_to_cell_ix,\n                    region_ix=region_ix,\n                )\n                print(len(coordinates_raw_batch))\n                mapping_raw.append(mapping_raw_batch)\n                coordinates_raw.append(coordinates_raw_batch)\n\n            mapping_raw = np.concatenate(mapping_raw)\n            coordinates_raw = np.concatenate(coordinates_raw)\n\n            # sort by region, coordinate (of left cut sites), and cell\n            sorted_idx = np.lexsort((coordinates_raw[:, 0], mapping_raw[:, 0], mapping_raw[:, 1]))\n            mapping_raw = mapping_raw[sorted_idx]\n            coordinates_raw = coordinates_raw[sorted_idx]\n\n            mapping_processed.append(mapping_raw)\n            coordinates_processed.append(coordinates_raw)\n\n            if sum(mapping_raw.shape[0] for mapping_raw in mapping_processed) &gt;= batch_size:\n                self.mapping.extend(np.concatenate(mapping_processed).astype(np.int32))\n                self.coordinates.extend(np.concatenate(coordinates_processed).astype(np.int32))\n                mapping_processed = []\n                coordinates_processed = []\n\n            del mapping_raw\n            del coordinates_raw\n\n        if len(mapping_processed) &gt; 0:\n            self.mapping.extend(np.concatenate(mapping_processed).astype(np.int32))\n            self.coordinates.extend(np.concatenate(coordinates_processed).astype(np.int32))\n\n        return self\n\n    @class_or_instancemethod\n    def from_alignments(\n        cls,\n        obs: pd.DataFrame,\n        regions: Regions,\n        file_column: str = \"path\",\n        alignment_column: str = None,\n        remove_duplicates: bool = None,\n        path: PathLike = None,\n        overwrite: bool = False,\n        reuse: bool = True,\n        batch_size: int = 10e6,\n        paired: bool = True,\n    ) -&gt; Fragments:\n\"\"\"\n        Create a Fragments object from multiple alignment (bam/sam) files, each pertaining to a single cell or sample\n\n        Parameters:\n            obs:\n                DataFrame containing information about cells/samples.\n                The index will be used as the name of the cell for future reference.\n                The DataFrame should contain a column with the path to the alignment file (specified in the file_column) or a column with the alignment object itself (specified in the alignment_column).\n                Any additional data about cells/samples can be stored in this dataframe as well.\n            regions:\n                Regions from which the fragments will be extracted.\n            file_column:\n                Column name in the `obs` DataFrame containing the path to the alignment file.\n            alignment_column:\n                Column name in the `obs` DataFrame containing the alignment object.\n                If None, the alignment object will be loaded using the `file_column`.\n            remove_duplicates:\n                Whether to remove duplicate fragments within a sample or cell. This is commonly done for single-cell data, but not necessarily for bulk data. If the data is paired, duplicates will be removed by default. If the data is single-end, duplicates will not be removed by default.\n            path:\n                Folder in which the fragments data will be stored.\n            overwrite:\n                Whether to overwrite the data if it already exists.\n            reuse:\n                Whether to reuse existing data if it exists.\n            batch_size:\n                Number of fragments to process before saving. Lower this number if you run out of memory. Increase the number if you want to speed up the process, particularly if disk I/O is slow.\n            paired:\n                Whether the reads are paired-end or single-end. If paired, the coordinates of the two cut sites will be stored. If single-end, only the coordinate of only one cut site will be stored. Note that this also affects the default value of `remove_duplicates`.\n        \"\"\"\n        # regions information\n        var = pd.DataFrame(index=regions.coordinates.index)\n        var[\"ix\"] = np.arange(var.shape[0])\n\n        # check path and overwrite\n        if path is not None:\n            if isinstance(path, str):\n                path = pathlib.Path(path)\n            if not overwrite and path.exists() and not reuse:\n                raise FileExistsError(\n                    f\"Folder {path} already exists, use `overwrite=True` to overwrite, or `reuse=True` to reuse existing data\"\n                )\n        if overwrite:\n            reuse = False\n\n        self = cls.create(path=path, obs=obs, var=var, regions=regions, reset=overwrite)\n\n        if reuse:\n            return self\n\n        # load alignment files\n        try:\n            import pysam\n        except ImportError as e:\n            raise ImportError(\n                \"pysam is required to read alignment files. Install using `pip install pysam` or `conda install -c bioconda pysam`\"\n            ) from e\n\n        if alignment_column is None:\n            if file_column not in obs.columns:\n                raise ValueError(f\"Column {file_column} not in obs\")\n            alignments = {}\n            for cell, cell_info in obs.iterrows():\n                alignments[cell] = pysam.Samfile(cell_info[file_column], \"rb\")\n        else:\n            if alignment_column not in obs.columns:\n                raise ValueError(f\"Column {alignment_column} not in obs\")\n            alignments = obs[alignment_column].to_dict()\n\n        # process regions\n        pbar = tqdm.tqdm(\n            enumerate(regions.coordinates.iterrows()),\n            total=regions.coordinates.shape[0],\n            leave=False,\n            desc=\"Processing fragments\",\n        )\n\n        self.mapping.open_creator()\n        self.coordinates.open_creator()\n\n        mapping_processed = []\n        coordinates_processed = []\n\n        for region_ix, (region_id, region_info) in pbar:\n            pbar.set_description(f\"{region_id}\")\n\n            chrom = region_info[\"chrom\"]\n            start = region_info[\"start\"]\n            end = region_info[\"end\"]\n            strand = region_info[\"strand\"]\n            if \"tss\" in region_info:\n                tss = region_info[\"tss\"]\n            else:\n                tss = region_info[\"start\"]\n\n            # process cell/sample\n            for cell_ix, (cell_id, alignment) in enumerate(alignments.items()):\n                if paired:\n                    coordinates_raw = _process_paired(\n                        alignment=alignment,\n                        chrom=chrom,\n                        start=start,\n                        end=end,\n                        remove_duplicates=True if remove_duplicates is None else remove_duplicates,\n                    )\n                    coordinates_raw = (np.array(coordinates_raw).reshape(-1, 2).astype(np.int32) - tss) * strand\n                else:\n                    coordinates_raw = _process_single(\n                        alignment=alignment,\n                        chrom=chrom,\n                        start=start,\n                        end=end,\n                        remove_duplicates=False if remove_duplicates is None else remove_duplicates,\n                    )\n                    coordinates_raw = (np.array(coordinates_raw).reshape(-1, 1).astype(np.int32) - tss) * strand\n\n                mapping_raw = np.stack(\n                    [\n                        np.repeat(cell_ix, len(coordinates_raw)),\n                        np.repeat(region_ix, len(coordinates_raw)),\n                    ],\n                    axis=1,\n                )\n\n                # sort by region, coordinate (of left cut sites), and cell\n                sorted_idx = np.lexsort((coordinates_raw[:, 0], mapping_raw[:, 0], mapping_raw[:, 1]))\n                mapping_raw = mapping_raw[sorted_idx]\n                coordinates_raw = coordinates_raw[sorted_idx]\n\n                if len(mapping_raw) &gt; 0:\n                    mapping_processed.append(mapping_raw)\n                    coordinates_processed.append(coordinates_raw)\n\n                if sum(mapping_raw.shape[0] for mapping_raw in mapping_processed) &gt;= batch_size:\n                    self.mapping.extend(np.concatenate(mapping_processed).astype(np.int32))\n                    self.coordinates.extend(np.concatenate(coordinates_processed).astype(np.int32))\n                    mapping_processed = []\n                    coordinates_processed = []\n\n                del mapping_raw\n                del coordinates_raw\n\n        # add final fragments\n        if len(mapping_processed) &gt; 0:\n            self.mapping.extend(np.concatenate(mapping_processed).astype(np.int32))\n            self.coordinates.extend(np.concatenate(coordinates_processed).astype(np.int32))\n\n        return self\n\n    def filter_regions(self, regions: Regions, path: PathLike = None, overwrite=True) -&gt; Fragments:\n\"\"\"\n        Filter based on new regions\n\n        Parameters:\n            regions:\n                Regions to filter.\n        Returns:\n            A new Fragments object\n        \"\"\"\n\n        # check if new regions are a subset of the existing ones\n        if not regions.coordinates.index.isin(self.regions.coordinates.index).all():\n            raise ValueError(\"New regions should be a subset of the existing ones\")\n\n        # filter region info\n        self.regions.coordinates[\"ix\"] = np.arange(self.regions.coordinates.shape[0])\n        regions.coordinates[\"ix\"] = self.regions.coordinates[\"ix\"].loc[regions.coordinates.index]\n\n        var = self.regions.coordinates.copy()\n        var[\"original_ix\"] = np.arange(var.shape[0])\n        var = var.loc[regions.coordinates.index].copy()\n        var[\"ix\"] = np.arange(var.shape[0])\n\n        # filter coordinates/mapping\n        fragments_oi = np.isin(self.mapping[:, 1], regions.coordinates[\"ix\"])\n\n        mapping = self.mapping[fragments_oi].copy()\n        mapping[:, 1] = var.set_index(\"original_ix\").loc[mapping[:, 1], \"ix\"].values\n        coordinates = self.coordinates[fragments_oi].copy()\n\n        # Sort `coordinates` and `mapping` according to `mapping`\n        sorted_idx = np.lexsort((coordinates[:, 0], mapping[:, 0], mapping[:, 1]))\n        mapping = mapping[sorted_idx]\n        coordinates = coordinates[sorted_idx]\n\n        fragments = Fragments.create(\n            coordinates=coordinates, mapping=mapping, regions=regions, var=var, obs=self.obs, path=path, reset=overwrite\n        )\n\n        return fragments\n\n    def filter_cells(self, cells, path: PathLike = None) -&gt; Fragments:\n\"\"\"\n        Filter based on new cells\n\n        Parameters:\n            cells:\n                Cells to filter.\n        Returns:\n            A new Fragments object\n        \"\"\"\n\n        # check if new cells are a subset of the existing ones\n        if not pd.Series(cells).isin(self.obs.index).all():\n            raise ValueError(\"New cells should be a subset of the existing ones\")\n\n        # filter region info\n        self.obs[\"ix\"] = np.arange(self.obs.shape[0])\n        obs = self.obs.loc[cells].copy()\n        obs[\"original_ix\"] = self.obs[\"ix\"].loc[obs.index]\n        obs[\"ix\"] = np.arange(obs.shape[0])\n\n        # filter coordinates/mapping\n        fragments_oi = np.isin(self.mapping[:, 0], obs[\"original_ix\"])\n\n        mapping = self.mapping[fragments_oi].copy()\n        mapping[:, 0] = obs.set_index(\"original_ix\").loc[mapping[:, 0], \"ix\"].values\n        coordinates = self.coordinates[fragments_oi]\n\n        # Sort `coordinates` and `mapping` according to `mapping`\n        sorted_idx = np.lexsort((coordinates[:, 0], mapping[:, 0], mapping[:, 1]))\n        mapping = mapping[sorted_idx]\n        coordinates = coordinates[sorted_idx]\n\n        return Fragments.create(\n            coordinates=coordinates, mapping=mapping, regions=self.regions, var=self.var, obs=obs, path=path\n        )\n\n    @property\n    def counts(self):\n\"\"\"\n        Counts of fragments per cell x region\n        \"\"\"\n        cellxregion_ix = self.mapping[:, 0] * self.n_regions + self.mapping[:, 1]\n        counts = np.bincount(cellxregion_ix, minlength=self.n_cells * self.n_regions).reshape(\n            self.n_cells, self.n_regions\n        )\n        return counts\n\n    @property\n    def nonzero(self):\n        return self.counts &gt; 0\n\n    _single_region_cache = None\n\n    def get_cache(self, region_oi):\n        if self._single_region_cache is None:\n            self._single_region_cache = {}\n        if region_oi not in self._single_region_cache:\n            region = self.var.index.get_loc(region_oi)\n            regionxcell_indptr = self.regionxcell_indptr.open_reader(\n                {\"context\": {\"data_copy_concurrency\": {\"limit\": 1}}}\n            )[region * self.n_cells : (region + 1) * self.n_cells + 1]\n            coordinates = self.coordinates.open_reader(\n                {\n                    \"context\": {\n                        \"data_copy_concurrency\": {\"limit\": 1},\n                    }\n                }\n            )[regionxcell_indptr[0] : regionxcell_indptr[-1]]\n            regionxcell_indptr = regionxcell_indptr - regionxcell_indptr[0]\n\n            self._single_region_cache[region_oi] = {\n                \"regionxcell_indptr\": regionxcell_indptr,\n                \"coordinates\": coordinates,\n            }\n        return self._single_region_cache[region_oi]\n\n    _libsize = None\n\n    @property\n    def libsize(self):\n        if self._libsize is None:\n            self._libsize = np.bincount(self.mapping[:, 0], minlength=self.n_cells)\n        return self._libsize\n</code></pre>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.fragments.Fragments.coordinates","title":"<code>coordinates: TensorstoreInstance = Tensorstore(dtype='&lt;i4', chunks=[100000, 2], compression=None, shape=[0, 2])</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Coordinates of the two cut sites.</p>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.fragments.Fragments.counts","title":"<code>counts</code>  <code>property</code>","text":"<p>Counts of fragments per cell x region</p>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.fragments.Fragments.mapping","title":"<code>mapping: TensorstoreInstance = Tensorstore(dtype='&lt;i4', chunks=[100000, 2], compression='blosc', shape=[0, 2])</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Mapping of a fragment to a cell (first column) and a region (second column)</p>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.fragments.Fragments.n_cells","title":"<code>n_cells</code>  <code>property</code> <code>cached</code>","text":"<p>Number of cells</p>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.fragments.Fragments.n_regions","title":"<code>n_regions</code>  <code>property</code> <code>cached</code>","text":"<p>Number of regions</p>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.fragments.Fragments.obs","title":"<code>obs: pd.DataFrame = TSV()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>DataFrame containing information about cells.</p>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.fragments.Fragments.regions","title":"<code>regions: Regions = Linked()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The regions in which (part of) the fragments are located and centered.</p>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.fragments.Fragments.regionxcell_indptr","title":"<code>regionxcell_indptr: np.ndarray = Tensorstore(dtype='&lt;i8', chunks=[100000], compression='blosc')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Index pointers to the regionxcell fragment positions</p>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.fragments.Fragments.var","title":"<code>var: pd.DataFrame = TSV()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>DataFrame containing information about regions.</p>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.fragments.Fragments.create_regionxcell_indptr","title":"<code>create_regionxcell_indptr(overwrite=False)</code>","text":"<p>Creates pointers to each individual region x cell combination from the mapping tensor.</p> <p>Returns:</p> Type Description <code>Fragments</code> <p>The same object with <code>regionxcell_indptr</code> populated</p> Source code in <code>src/chromatinhd/data/fragments/fragments.py</code> <pre><code>def create_regionxcell_indptr(self, overwrite=False) -&gt; Fragments:\n\"\"\"\n    Creates pointers to each individual region x cell combination from the mapping tensor.\n\n    Returns:\n        The same object with `regionxcell_indptr` populated\n    \"\"\"\n\n    if self.o.regionxcell_indptr.exists(self) and not overwrite:\n        return self\n\n    regionxcell_ix = self.mapping[:, 1] * self.n_cells + self.mapping[:, 0]\n\n    if not (np.diff(regionxcell_ix) &gt;= 0).all():\n        raise ValueError(\"Fragments should be ordered by regionxcell (ascending)\")\n\n    if not self.mapping[:, 0].max() &lt; self.n_cells:\n        raise ValueError(\"First column of mapping should be smaller than the number of cells\")\n\n    n_regionxcell = self.n_regions * self.n_cells\n    regionxcell_indptr = np.pad(np.cumsum(np.bincount(regionxcell_ix, minlength=n_regionxcell), 0), (1, 0))\n    assert self.coordinates.shape[0] == regionxcell_indptr[-1]\n    if not (np.diff(regionxcell_indptr) &gt;= 0).all():\n        raise ValueError(\"Fragments should be ordered by regionxcell (ascending)\")\n    self.regionxcell_indptr[:] = regionxcell_indptr\n\n    return self\n</code></pre>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.fragments.Fragments.filter_cells","title":"<code>filter_cells(cells, path=None)</code>","text":"<p>Filter based on new cells</p> <p>Parameters:</p> Name Type Description Default <code>cells</code> <p>Cells to filter.</p> required <p>Returns:</p> Type Description <code>Fragments</code> <p>A new Fragments object</p> Source code in <code>src/chromatinhd/data/fragments/fragments.py</code> <pre><code>def filter_cells(self, cells, path: PathLike = None) -&gt; Fragments:\n\"\"\"\n    Filter based on new cells\n\n    Parameters:\n        cells:\n            Cells to filter.\n    Returns:\n        A new Fragments object\n    \"\"\"\n\n    # check if new cells are a subset of the existing ones\n    if not pd.Series(cells).isin(self.obs.index).all():\n        raise ValueError(\"New cells should be a subset of the existing ones\")\n\n    # filter region info\n    self.obs[\"ix\"] = np.arange(self.obs.shape[0])\n    obs = self.obs.loc[cells].copy()\n    obs[\"original_ix\"] = self.obs[\"ix\"].loc[obs.index]\n    obs[\"ix\"] = np.arange(obs.shape[0])\n\n    # filter coordinates/mapping\n    fragments_oi = np.isin(self.mapping[:, 0], obs[\"original_ix\"])\n\n    mapping = self.mapping[fragments_oi].copy()\n    mapping[:, 0] = obs.set_index(\"original_ix\").loc[mapping[:, 0], \"ix\"].values\n    coordinates = self.coordinates[fragments_oi]\n\n    # Sort `coordinates` and `mapping` according to `mapping`\n    sorted_idx = np.lexsort((coordinates[:, 0], mapping[:, 0], mapping[:, 1]))\n    mapping = mapping[sorted_idx]\n    coordinates = coordinates[sorted_idx]\n\n    return Fragments.create(\n        coordinates=coordinates, mapping=mapping, regions=self.regions, var=self.var, obs=obs, path=path\n    )\n</code></pre>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.fragments.Fragments.filter_regions","title":"<code>filter_regions(regions, path=None, overwrite=True)</code>","text":"<p>Filter based on new regions</p> <p>Parameters:</p> Name Type Description Default <code>regions</code> <code>Regions</code> <p>Regions to filter.</p> required <p>Returns:</p> Type Description <code>Fragments</code> <p>A new Fragments object</p> Source code in <code>src/chromatinhd/data/fragments/fragments.py</code> <pre><code>def filter_regions(self, regions: Regions, path: PathLike = None, overwrite=True) -&gt; Fragments:\n\"\"\"\n    Filter based on new regions\n\n    Parameters:\n        regions:\n            Regions to filter.\n    Returns:\n        A new Fragments object\n    \"\"\"\n\n    # check if new regions are a subset of the existing ones\n    if not regions.coordinates.index.isin(self.regions.coordinates.index).all():\n        raise ValueError(\"New regions should be a subset of the existing ones\")\n\n    # filter region info\n    self.regions.coordinates[\"ix\"] = np.arange(self.regions.coordinates.shape[0])\n    regions.coordinates[\"ix\"] = self.regions.coordinates[\"ix\"].loc[regions.coordinates.index]\n\n    var = self.regions.coordinates.copy()\n    var[\"original_ix\"] = np.arange(var.shape[0])\n    var = var.loc[regions.coordinates.index].copy()\n    var[\"ix\"] = np.arange(var.shape[0])\n\n    # filter coordinates/mapping\n    fragments_oi = np.isin(self.mapping[:, 1], regions.coordinates[\"ix\"])\n\n    mapping = self.mapping[fragments_oi].copy()\n    mapping[:, 1] = var.set_index(\"original_ix\").loc[mapping[:, 1], \"ix\"].values\n    coordinates = self.coordinates[fragments_oi].copy()\n\n    # Sort `coordinates` and `mapping` according to `mapping`\n    sorted_idx = np.lexsort((coordinates[:, 0], mapping[:, 0], mapping[:, 1]))\n    mapping = mapping[sorted_idx]\n    coordinates = coordinates[sorted_idx]\n\n    fragments = Fragments.create(\n        coordinates=coordinates, mapping=mapping, regions=regions, var=var, obs=self.obs, path=path, reset=overwrite\n    )\n\n    return fragments\n</code></pre>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.fragments.Fragments.from_alignments","title":"<code>from_alignments(obs, regions, file_column='path', alignment_column=None, remove_duplicates=None, path=None, overwrite=False, reuse=True, batch_size=10000000.0, paired=True)</code>","text":"<p>Create a Fragments object from multiple alignment (bam/sam) files, each pertaining to a single cell or sample</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>pd.DataFrame</code> <p>DataFrame containing information about cells/samples. The index will be used as the name of the cell for future reference. The DataFrame should contain a column with the path to the alignment file (specified in the file_column) or a column with the alignment object itself (specified in the alignment_column). Any additional data about cells/samples can be stored in this dataframe as well.</p> required <code>regions</code> <code>Regions</code> <p>Regions from which the fragments will be extracted.</p> required <code>file_column</code> <code>str</code> <p>Column name in the <code>obs</code> DataFrame containing the path to the alignment file.</p> <code>'path'</code> <code>alignment_column</code> <code>str</code> <p>Column name in the <code>obs</code> DataFrame containing the alignment object. If None, the alignment object will be loaded using the <code>file_column</code>.</p> <code>None</code> <code>remove_duplicates</code> <code>bool</code> <p>Whether to remove duplicate fragments within a sample or cell. This is commonly done for single-cell data, but not necessarily for bulk data. If the data is paired, duplicates will be removed by default. If the data is single-end, duplicates will not be removed by default.</p> <code>None</code> <code>path</code> <code>PathLike</code> <p>Folder in which the fragments data will be stored.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the data if it already exists.</p> <code>False</code> <code>reuse</code> <code>bool</code> <p>Whether to reuse existing data if it exists.</p> <code>True</code> <code>batch_size</code> <code>int</code> <p>Number of fragments to process before saving. Lower this number if you run out of memory. Increase the number if you want to speed up the process, particularly if disk I/O is slow.</p> <code>10000000.0</code> <code>paired</code> <code>bool</code> <p>Whether the reads are paired-end or single-end. If paired, the coordinates of the two cut sites will be stored. If single-end, only the coordinate of only one cut site will be stored. Note that this also affects the default value of <code>remove_duplicates</code>.</p> <code>True</code> Source code in <code>src/chromatinhd/data/fragments/fragments.py</code> <pre><code>@class_or_instancemethod\ndef from_alignments(\n    cls,\n    obs: pd.DataFrame,\n    regions: Regions,\n    file_column: str = \"path\",\n    alignment_column: str = None,\n    remove_duplicates: bool = None,\n    path: PathLike = None,\n    overwrite: bool = False,\n    reuse: bool = True,\n    batch_size: int = 10e6,\n    paired: bool = True,\n) -&gt; Fragments:\n\"\"\"\n    Create a Fragments object from multiple alignment (bam/sam) files, each pertaining to a single cell or sample\n\n    Parameters:\n        obs:\n            DataFrame containing information about cells/samples.\n            The index will be used as the name of the cell for future reference.\n            The DataFrame should contain a column with the path to the alignment file (specified in the file_column) or a column with the alignment object itself (specified in the alignment_column).\n            Any additional data about cells/samples can be stored in this dataframe as well.\n        regions:\n            Regions from which the fragments will be extracted.\n        file_column:\n            Column name in the `obs` DataFrame containing the path to the alignment file.\n        alignment_column:\n            Column name in the `obs` DataFrame containing the alignment object.\n            If None, the alignment object will be loaded using the `file_column`.\n        remove_duplicates:\n            Whether to remove duplicate fragments within a sample or cell. This is commonly done for single-cell data, but not necessarily for bulk data. If the data is paired, duplicates will be removed by default. If the data is single-end, duplicates will not be removed by default.\n        path:\n            Folder in which the fragments data will be stored.\n        overwrite:\n            Whether to overwrite the data if it already exists.\n        reuse:\n            Whether to reuse existing data if it exists.\n        batch_size:\n            Number of fragments to process before saving. Lower this number if you run out of memory. Increase the number if you want to speed up the process, particularly if disk I/O is slow.\n        paired:\n            Whether the reads are paired-end or single-end. If paired, the coordinates of the two cut sites will be stored. If single-end, only the coordinate of only one cut site will be stored. Note that this also affects the default value of `remove_duplicates`.\n    \"\"\"\n    # regions information\n    var = pd.DataFrame(index=regions.coordinates.index)\n    var[\"ix\"] = np.arange(var.shape[0])\n\n    # check path and overwrite\n    if path is not None:\n        if isinstance(path, str):\n            path = pathlib.Path(path)\n        if not overwrite and path.exists() and not reuse:\n            raise FileExistsError(\n                f\"Folder {path} already exists, use `overwrite=True` to overwrite, or `reuse=True` to reuse existing data\"\n            )\n    if overwrite:\n        reuse = False\n\n    self = cls.create(path=path, obs=obs, var=var, regions=regions, reset=overwrite)\n\n    if reuse:\n        return self\n\n    # load alignment files\n    try:\n        import pysam\n    except ImportError as e:\n        raise ImportError(\n            \"pysam is required to read alignment files. Install using `pip install pysam` or `conda install -c bioconda pysam`\"\n        ) from e\n\n    if alignment_column is None:\n        if file_column not in obs.columns:\n            raise ValueError(f\"Column {file_column} not in obs\")\n        alignments = {}\n        for cell, cell_info in obs.iterrows():\n            alignments[cell] = pysam.Samfile(cell_info[file_column], \"rb\")\n    else:\n        if alignment_column not in obs.columns:\n            raise ValueError(f\"Column {alignment_column} not in obs\")\n        alignments = obs[alignment_column].to_dict()\n\n    # process regions\n    pbar = tqdm.tqdm(\n        enumerate(regions.coordinates.iterrows()),\n        total=regions.coordinates.shape[0],\n        leave=False,\n        desc=\"Processing fragments\",\n    )\n\n    self.mapping.open_creator()\n    self.coordinates.open_creator()\n\n    mapping_processed = []\n    coordinates_processed = []\n\n    for region_ix, (region_id, region_info) in pbar:\n        pbar.set_description(f\"{region_id}\")\n\n        chrom = region_info[\"chrom\"]\n        start = region_info[\"start\"]\n        end = region_info[\"end\"]\n        strand = region_info[\"strand\"]\n        if \"tss\" in region_info:\n            tss = region_info[\"tss\"]\n        else:\n            tss = region_info[\"start\"]\n\n        # process cell/sample\n        for cell_ix, (cell_id, alignment) in enumerate(alignments.items()):\n            if paired:\n                coordinates_raw = _process_paired(\n                    alignment=alignment,\n                    chrom=chrom,\n                    start=start,\n                    end=end,\n                    remove_duplicates=True if remove_duplicates is None else remove_duplicates,\n                )\n                coordinates_raw = (np.array(coordinates_raw).reshape(-1, 2).astype(np.int32) - tss) * strand\n            else:\n                coordinates_raw = _process_single(\n                    alignment=alignment,\n                    chrom=chrom,\n                    start=start,\n                    end=end,\n                    remove_duplicates=False if remove_duplicates is None else remove_duplicates,\n                )\n                coordinates_raw = (np.array(coordinates_raw).reshape(-1, 1).astype(np.int32) - tss) * strand\n\n            mapping_raw = np.stack(\n                [\n                    np.repeat(cell_ix, len(coordinates_raw)),\n                    np.repeat(region_ix, len(coordinates_raw)),\n                ],\n                axis=1,\n            )\n\n            # sort by region, coordinate (of left cut sites), and cell\n            sorted_idx = np.lexsort((coordinates_raw[:, 0], mapping_raw[:, 0], mapping_raw[:, 1]))\n            mapping_raw = mapping_raw[sorted_idx]\n            coordinates_raw = coordinates_raw[sorted_idx]\n\n            if len(mapping_raw) &gt; 0:\n                mapping_processed.append(mapping_raw)\n                coordinates_processed.append(coordinates_raw)\n\n            if sum(mapping_raw.shape[0] for mapping_raw in mapping_processed) &gt;= batch_size:\n                self.mapping.extend(np.concatenate(mapping_processed).astype(np.int32))\n                self.coordinates.extend(np.concatenate(coordinates_processed).astype(np.int32))\n                mapping_processed = []\n                coordinates_processed = []\n\n            del mapping_raw\n            del coordinates_raw\n\n    # add final fragments\n    if len(mapping_processed) &gt; 0:\n        self.mapping.extend(np.concatenate(mapping_processed).astype(np.int32))\n        self.coordinates.extend(np.concatenate(coordinates_processed).astype(np.int32))\n\n    return self\n</code></pre>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.fragments.Fragments.from_fragments_tsv","title":"<code>from_fragments_tsv(fragments_file, regions, obs, cell_column=None, path=None, overwrite=False, reuse=True, batch_size=1000000.0)</code>","text":"<p>Create a Fragments object from a fragments tsv file</p> <p>Parameters:</p> Name Type Description Default <code>fragments_file</code> <code>PathLike</code> <p>Location of the fragments tab-separate file created by e.g. CellRanger or sinto</p> required <code>obs</code> <code>pd.DataFrame</code> <p>DataFrame containing information about cells. The index should be the cell names as present in the fragments file. Alternatively, the column containing cell ids can be specified using the <code>cell_column</code> argument.</p> required <code>regions</code> <code>Regions</code> <p>Regions from which the fragments will be extracted.</p> required <code>cell_column</code> <code>str</code> <p>Column name in the <code>obs</code> DataFrame containing the cell names. If None, the index of the <code>obs</code> DataFrame is used.</p> <code>None</code> <code>path</code> <code>PathLike</code> <p>Folder in which the fragments data will be stored.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the data if it already exists.</p> <code>False</code> <code>reuse</code> <code>bool</code> <p>Whether to reuse existing data if it exists.</p> <code>True</code> <code>batch_size</code> <code>int</code> <p>Number of fragments to process before saving. Lower this number if you run out of memory.</p> <code>1000000.0</code> <p>Returns:</p> Type Description <code>Fragments</code> <p>A new Fragments object</p> Source code in <code>src/chromatinhd/data/fragments/fragments.py</code> <pre><code>@class_or_instancemethod\ndef from_fragments_tsv(\n    cls,\n    fragments_file: PathLike,\n    regions: Regions,\n    obs: pd.DataFrame,\n    cell_column: str = None,\n    path: PathLike = None,\n    overwrite: bool = False,\n    reuse: bool = True,\n    batch_size: int = 1e6,\n) -&gt; Fragments:\n\"\"\"\n    Create a Fragments object from a fragments tsv file\n\n    Parameters:\n        fragments_file:\n            Location of the fragments tab-separate file created by e.g. CellRanger or sinto\n        obs:\n            DataFrame containing information about cells.\n            The index should be the cell names as present in the fragments file.\n            Alternatively, the column containing cell ids can be specified using the `cell_column` argument.\n        regions:\n            Regions from which the fragments will be extracted.\n        cell_column:\n            Column name in the `obs` DataFrame containing the cell names.\n            If None, the index of the `obs` DataFrame is used.\n        path:\n            Folder in which the fragments data will be stored.\n        overwrite:\n            Whether to overwrite the data if it already exists.\n        reuse:\n            Whether to reuse existing data if it exists.\n        batch_size:\n            Number of fragments to process before saving. Lower this number if you run out of memory.\n    Returns:\n        A new Fragments object\n    \"\"\"\n\n    if isinstance(fragments_file, str):\n        fragments_file = pathlib.Path(fragments_file)\n    if isinstance(path, str):\n        path = pathlib.Path(path)\n    if not fragments_file.exists():\n        raise FileNotFoundError(f\"File {fragments_file} does not exist\")\n    if not overwrite and path.exists() and not reuse:\n        raise FileExistsError(\n            f\"Folder {path} already exists, use `overwrite=True` to overwrite, or `reuse=True` to reuse existing data\"\n        )\n\n    # regions information\n    var = pd.DataFrame(index=regions.coordinates.index)\n    var[\"ix\"] = np.arange(var.shape[0])\n\n    # cell information\n    obs = obs.copy()\n    obs[\"ix\"] = np.arange(obs.shape[0])\n    if cell_column is None:\n        cell_to_cell_ix = obs[\"ix\"].to_dict()\n    else:\n        cell_to_cell_ix = obs.set_index(cell_column)[\"ix\"].to_dict()\n\n    self = cls.create(path=path, obs=obs, var=var, regions=regions, reset=overwrite)\n\n    # read the fragments file\n    try:\n        import pysam\n    except ImportError as e:\n        raise ImportError(\n            \"pysam is required to read fragments files. Install using `pip install pysam` or `conda install -c bioconda pysam`\"\n        ) from e\n    fragments_tabix = pysam.TabixFile(str(fragments_file))\n\n    # process regions\n    pbar = tqdm.tqdm(\n        enumerate(regions.coordinates.iterrows()),\n        total=regions.coordinates.shape[0],\n        leave=False,\n        desc=\"Processing fragments\",\n    )\n\n    self.mapping.open_creator()\n    self.coordinates.open_creator()\n\n    mapping_processed = []\n    coordinates_processed = []\n\n    for region_ix, (region_id, region_info) in pbar:\n        pbar.set_description(f\"{region_id}\")\n\n        strand = region_info[\"strand\"]\n        if \"tss\" in region_info:\n            tss = region_info[\"tss\"]\n        else:\n            tss = region_info[\"start\"]\n\n        coordinates_raw, mapping_raw = _fetch_fragments_region(\n            fragments_tabix=fragments_tabix,\n            chrom=region_info[\"chrom\"],\n            start=region_info[\"start\"],\n            end=region_info[\"end\"],\n            tss=tss,\n            strand=strand,\n            cell_to_cell_ix=cell_to_cell_ix,\n            region_ix=region_ix,\n        )\n\n        mapping_processed.append(mapping_raw)\n        coordinates_processed.append(coordinates_raw)\n\n        if sum(mapping_raw.shape[0] for mapping_raw in mapping_processed) &gt;= batch_size:\n            self.mapping.extend(np.concatenate(mapping_processed).astype(np.int32))\n            self.coordinates.extend(np.concatenate(coordinates_processed).astype(np.int32))\n            mapping_processed = []\n            coordinates_processed = []\n\n        del mapping_raw\n        del coordinates_raw\n\n    if len(mapping_processed) &gt; 0:\n        self.mapping.extend(np.concatenate(mapping_processed).astype(np.int32))\n        self.coordinates.extend(np.concatenate(coordinates_processed).astype(np.int32))\n\n    return self\n</code></pre>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.fragments.Fragments.from_multiple_fragments_tsv","title":"<code>from_multiple_fragments_tsv(fragments_files, regions, obs, cell_column='cell_original', batch_column='batch', path=None, overwrite=False, reuse=True, batch_size=1000000.0)</code>","text":"<p>Create a Fragments object from multiple fragments tsv file</p> <p>Parameters:</p> Name Type Description Default <code>fragments_files</code> <code>[PathLike]</code> <p>Location of the fragments tab-separate file created by e.g. CellRanger or sinto</p> required <code>obs</code> <code>pd.DataFrame</code> <p>DataFrame containing information about cells. The index should be the cell names as present in the fragments file. Alternatively, the column containing cell ids can be specified using the <code>cell_column</code> argument.</p> required <code>regions</code> <code>Regions</code> <p>Regions from which the fragments will be extracted.</p> required <code>cell_column</code> <code>str</code> <p>Column name in the <code>obs</code> DataFrame containing the cell names.</p> <code>'cell_original'</code> <code>batch_column</code> <code>str</code> <p>Column name in the <code>obs</code> DataFrame containing the batch indices. If None, will default to batch</p> <code>'batch'</code> <code>path</code> <code>PathLike</code> <p>Folder in which the fragments data will be stored.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the data if it already exists.</p> <code>False</code> <code>reuse</code> <code>bool</code> <p>Whether to reuse existing data if it exists.</p> <code>True</code> <code>batch_size</code> <code>int</code> <p>Number of fragments to process before saving. Lower this number if you run out of memory.</p> <code>1000000.0</code> <p>Returns:</p> Type Description <code>Fragments</code> <p>A new Fragments object</p> Source code in <code>src/chromatinhd/data/fragments/fragments.py</code> <pre><code>@class_or_instancemethod\ndef from_multiple_fragments_tsv(\n    cls,\n    fragments_files: [PathLike],\n    regions: Regions,\n    obs: pd.DataFrame,\n    cell_column: str = \"cell_original\",\n    batch_column: str = \"batch\",\n    path: PathLike = None,\n    overwrite: bool = False,\n    reuse: bool = True,\n    batch_size: int = 1e6,\n) -&gt; Fragments:\n\"\"\"\n    Create a Fragments object from multiple fragments tsv file\n\n    Parameters:\n        fragments_files:\n            Location of the fragments tab-separate file created by e.g. CellRanger or sinto\n        obs:\n            DataFrame containing information about cells.\n            The index should be the cell names as present in the fragments file.\n            Alternatively, the column containing cell ids can be specified using the `cell_column` argument.\n        regions:\n            Regions from which the fragments will be extracted.\n        cell_column:\n            Column name in the `obs` DataFrame containing the cell names.\n        batch_column:\n            Column name in the `obs` DataFrame containing the batch indices.\n            If None, will default to batch\n        path:\n            Folder in which the fragments data will be stored.\n        overwrite:\n            Whether to overwrite the data if it already exists.\n        reuse:\n            Whether to reuse existing data if it exists.\n        batch_size:\n            Number of fragments to process before saving. Lower this number if you run out of memory.\n    Returns:\n        A new Fragments object\n    \"\"\"\n\n    if not isinstance(fragments_files, (list, tuple)):\n        fragments_files = [fragments_files]\n\n    fragments_files_ = []\n    for fragments_file in fragments_files:\n        if isinstance(fragments_file, str):\n            fragments_file = pathlib.Path(fragments_file)\n        if not fragments_file.exists():\n            raise FileNotFoundError(f\"File {fragments_file} does not exist\")\n        fragments_files_.append(fragments_file)\n    fragments_files = fragments_files_\n\n    if not overwrite and path.exists() and not reuse:\n        raise FileExistsError(\n            f\"Folder {path} already exists, use `overwrite=True` to overwrite, or `reuse=True` to reuse existing data\"\n        )\n\n    # regions information\n    var = pd.DataFrame(index=regions.coordinates.index)\n    var[\"ix\"] = np.arange(var.shape[0])\n\n    # cell information\n    obs = obs.copy()\n    obs[\"ix\"] = np.arange(obs.shape[0])\n\n    if batch_column is None:\n        raise ValueError(\"batch_column should be specified\")\n    if batch_column not in obs.columns:\n        raise ValueError(f\"Column {batch_column} not in obs\")\n    if obs[batch_column].dtype != \"int\":\n        raise ValueError(f\"Column {batch_column} should be an integer column\")\n    if not obs[batch_column].max() == len(fragments_files) - 1:\n        raise ValueError(f\"Column {batch_column} should contain values between 0 and {len(fragments_files) - 1}\")\n    cell_to_cell_ix_batches = [\n        obs.loc[obs[batch_column] == batch].set_index(cell_column)[\"ix\"] for batch in obs[batch_column].unique()\n    ]\n\n    self = cls.create(path=path, obs=obs, var=var, regions=regions, reset=overwrite)\n\n    # read the fragments file\n    try:\n        import pysam\n    except ImportError as e:\n        raise ImportError(\n            \"pysam is required to read fragments files. Install using `pip install pysam` or `conda install -c bioconda pysam`\"\n        ) from e\n    fragments_tabix_batches = [pysam.TabixFile(str(fragments_file)) for fragments_file in fragments_files]\n\n    # process regions\n    pbar = tqdm.tqdm(\n        enumerate(regions.coordinates.iterrows()),\n        total=regions.coordinates.shape[0],\n        leave=False,\n        desc=\"Processing fragments\",\n    )\n\n    self.mapping.open_creator()\n    self.coordinates.open_creator()\n\n    mapping_processed = []\n    coordinates_processed = []\n\n    for region_ix, (region_id, region_info) in pbar:\n        pbar.set_description(f\"{region_id}\")\n\n        strand = region_info[\"strand\"]\n        if \"tss\" in region_info:\n            tss = region_info[\"tss\"]\n        else:\n            tss = region_info[\"start\"]\n\n        mapping_raw = []\n        coordinates_raw = []\n\n        for fragments_tabix, cell_to_cell_ix in zip(fragments_tabix_batches, cell_to_cell_ix_batches):\n            coordinates_raw_batch, mapping_raw_batch = _fetch_fragments_region(\n                fragments_tabix=fragments_tabix,\n                chrom=region_info[\"chrom\"],\n                start=region_info[\"start\"],\n                end=region_info[\"end\"],\n                tss=tss,\n                strand=strand,\n                cell_to_cell_ix=cell_to_cell_ix,\n                region_ix=region_ix,\n            )\n            print(len(coordinates_raw_batch))\n            mapping_raw.append(mapping_raw_batch)\n            coordinates_raw.append(coordinates_raw_batch)\n\n        mapping_raw = np.concatenate(mapping_raw)\n        coordinates_raw = np.concatenate(coordinates_raw)\n\n        # sort by region, coordinate (of left cut sites), and cell\n        sorted_idx = np.lexsort((coordinates_raw[:, 0], mapping_raw[:, 0], mapping_raw[:, 1]))\n        mapping_raw = mapping_raw[sorted_idx]\n        coordinates_raw = coordinates_raw[sorted_idx]\n\n        mapping_processed.append(mapping_raw)\n        coordinates_processed.append(coordinates_raw)\n\n        if sum(mapping_raw.shape[0] for mapping_raw in mapping_processed) &gt;= batch_size:\n            self.mapping.extend(np.concatenate(mapping_processed).astype(np.int32))\n            self.coordinates.extend(np.concatenate(coordinates_processed).astype(np.int32))\n            mapping_processed = []\n            coordinates_processed = []\n\n        del mapping_raw\n        del coordinates_raw\n\n    if len(mapping_processed) &gt; 0:\n        self.mapping.extend(np.concatenate(mapping_processed).astype(np.int32))\n        self.coordinates.extend(np.concatenate(coordinates_processed).astype(np.int32))\n\n    return self\n</code></pre>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.FragmentsView","title":"<code>chromatinhd.data.fragments.FragmentsView</code>","text":"<p>         Bases: <code>Flow</code></p> <p>A view of fragments based on regions that are a subset of the parent fragments. In a typical use case, the parent contains fragments for all chromosomes, while this the view focuses on specific regions.</p> <p>Only fragments that are fully inclusive (i.e., both left and right cut sites) within a region will be selected.</p> Source code in <code>src/chromatinhd/data/fragments/view.py</code> <pre><code>class FragmentsView(Flow):\n\"\"\"\n    A view of fragments based on regions that are a subset of the parent fragments. In a typical use case, the parent contains fragments for all chromosomes, while this the view focuses on specific regions.\n\n    Only fragments that are fully inclusive (i.e., both left and right cut sites) within a region will be selected.\n    \"\"\"\n\n    regionxcell_indptr: TensorstoreInstance = Tensorstore(\n        dtype=\"&lt;i8\", chunks=[100000], compression=\"blosc\", shape=(0, 2)\n    )\n\"\"\"Index of fragments in the parent fragments object\"\"\"\n\n    parent: Fragments = Linked()\n\"\"\"The parent fragments object from which this view is created\"\"\"\n\n    regions: Regions = Linked()\n\"\"\"The regions object\"\"\"\n\n    parentregion_column: str = Stored()\n\n    @classmethod\n    def from_fragments(\n        cls,\n        parent: Fragments,\n        regions: Regions,\n        parentregion_column: str = \"chrom\",\n        obs: pd.DataFrame = None,\n        var: pd.DataFrame = None,\n        path: PathLike = None,\n        overwrite: bool = False,\n    ):\n\"\"\"\n        Creates a fragments view from a parent fragments object and a regions object\n\n        Parameters:\n            parent:\n                Parent fragments object. If a fragments view is provided, the parent of the parent will be used.\n            regions:\n                Regions object\n            obs:\n                DataFrame containing information about cells, will be copied from the fragments object if not provided\n            parentregion_column:\n                Column in the regions coordinates that links each new region to the regions of the original fragments. This is typically the chromosome column. This column should be present in both `parent.regions.coordinates` and `regions.coordinates`\n            path:\n                Path to store the fragments view\n        \"\"\"\n\n        if isinstance(parent, FragmentsView):\n            while isinstance(parent, FragmentsView):\n                parent = parent.parent\n        if not isinstance(parent, Fragments):\n            raise ValueError(\"parent should be a Fragments object\")\n        if not isinstance(regions, Regions):\n            raise ValueError(\"regions should be a Regions object\", regions.__class__)\n\n        # dummy proofing\n        if parentregion_column not in regions.coordinates.columns:\n            raise ValueError(\n                f\"Column {parentregion_column} not in regions coordinates. Available columns are {regions.coordinates.columns}\"\n            )\n        if parentregion_column not in parent.regions.coordinates.columns:\n            raise ValueError(\n                f\"Column {parentregion_column} not in fragments regions coordinates. Available columns are {parent.regions.coordinates.columns}\"\n            )\n        if not (regions.coordinates[parentregion_column].isin(parent.regions.coordinates[parentregion_column])).all():\n            raise ValueError(\n                f\"Not all regions are present in the parent fragments. Missing regions: {regions.coordinates[parentregion_column][~regions.coordinates[parentregion_column].isin(parent.regions.coordinates[parentregion_column])]}\"\n            )\n\n        self = cls.create(\n            parent=parent,\n            regions=regions,\n            path=path,\n            parentregion_column=parentregion_column,\n            obs=parent.obs if obs is None else obs,\n            var=regions.coordinates if var is None else var,\n            reset=overwrite,\n        )\n\n        return self\n\n    def create_regionxcell_indptr(\n        self,\n        inclusive: tuple = (True, False),\n        overwrite=True,\n    ) -&gt; FragmentsView:\n\"\"\"\n        Create index pointers (left and right) for the fragments associated to each regionxcell combination\n\n        Parameters:\n            batch_size:\n                Number of regions to wait before saving the intermediate results and freeing up memory. Reduce this number to avoid running out of memory.\n            inclusive:\n                Whether to only include fragments that are only partially overlapping with the region. Must be a tuple indicating left and/or right inclusivity.\n            overwrite:\n                Whether to overwrite the existing index pointers.\n\n        Returns:\n            Same object but with the `regionxcell_indptr` populated\n\n        \"\"\"\n        if self.regionxcell_indptr.exists() and not overwrite:\n            return self\n\n        mapping = self.parent.mapping[:]\n        coordinates = self.parent.coordinates[:]\n\n        # convert regions in parent to parent region ixs\n        parentregion_to_parentregion_ix = self.parent.regions.coordinates.index.get_loc\n\n        # reset the tensorstores\n        regionxcell_indptr = np.zeros((len(self.regions.coordinates) * len(self.obs), 2), dtype=np.int64)\n\n        # index pointers from parentregions to fragments\n        parentregion_fragment_indptr = indices_to_indptr(mapping[:, 1], self.parent.n_regions)\n\n        pbar = tqdm.tqdm(\n            total=len(self.regions.coordinates),\n            leave=False,\n            desc=\"Processing regions\",\n        )\n\n        self.regions.coordinates[\"ix\"] = np.arange(len(self.regions))\n\n        for parentregion, subcoordinates in self.regions.coordinates.groupby(self.parentregion_column):\n            # extract in which parent region we need to look\n            parentregion_ix = parentregion_to_parentregion_ix(parentregion)\n\n            parentregion_start_ix, parentregion_end_ix = (\n                parentregion_fragment_indptr[parentregion_ix],\n                parentregion_fragment_indptr[parentregion_ix + 1],\n            )\n\n            # extract parent's mapping and coordinates\n            coordinates_parentregion = coordinates[parentregion_start_ix:parentregion_end_ix]\n            cellmapping_parentregion = mapping[parentregion_start_ix:parentregion_end_ix, 0]\n\n            cell_indptr_parentregion = indices_to_indptr(cellmapping_parentregion, self.n_cells, dtype=np.int64)\n\n            for region_id, region in subcoordinates.iterrows():\n                region_ix = region[\"ix\"]\n\n                if pbar is not None:\n                    pbar.update(1)\n                    pbar.set_description(f\"Processing region {region_id}\")\n\n                # extract fragments lying within the region\n                # depending on the strand, we either need to include the start or the end\n                region[\"start_inclusive\"] = region[\"start\"] + (region[\"strand\"] == -1)\n                region[\"end_inclusive\"] = region[\"end\"] + (region[\"strand\"] == -1)\n\n                if inclusive == (True, False):\n                    fragments_oi = (coordinates_parentregion[:, 0] &gt;= region[\"start_inclusive\"]) &amp; (\n                        coordinates_parentregion[:, 0] &lt; region[\"end_inclusive\"]\n                    )\n                    fragments_excluded_left = coordinates_parentregion[:, 0] &lt; region[\"start_inclusive\"]\n                else:\n                    raise NotImplementedError(\"For now, only left-inclusive fragments are supported\")\n\n                n_excluded_left = np.bincount(cellmapping_parentregion[fragments_excluded_left], minlength=self.n_cells)\n                n_included = np.bincount(cellmapping_parentregion[fragments_oi], minlength=self.n_cells)\n\n                cell_indptr_left = cell_indptr_parentregion[:-1] + n_excluded_left\n                cell_indptr_right = cell_indptr_parentregion[:-1] + n_excluded_left + n_included\n\n                cell_indptr = np.stack([cell_indptr_left, cell_indptr_right], axis=1)\n\n                regionxcell_indptr[region_ix * self.n_cells : (region_ix + 1) * self.n_cells] = (\n                    cell_indptr + parentregion_start_ix\n                )\n\n        regionxcell_indptr_writer = self.regionxcell_indptr.open_creator(\n            shape=(len(self.regions.coordinates) * len(self.obs), 2)\n        )\n        regionxcell_indptr_writer[:] = regionxcell_indptr\n\n        return self\n\n    def create_regionxcell_indptr2(\n        self,\n        inclusive: tuple = (True, False),\n        overwrite=True,\n    ) -&gt; FragmentsView:\n\"\"\"\n        Create index pointers (left and right) for the fragments associated to each regionxcell combination. This implementation is faster if there are many samples with a lot of fragments (e.g. minibulk data)\n\n        Parameters:\n            batch_size:\n                Number of regions to wait before saving the intermediate results and freeing up memory. Reduce this number to avoid running out of memory.\n            inclusive:\n                Whether to only include fragments that are only partially overlapping with the region. Must be a tuple indicating left and/or right inclusivity.\n            overwrite:\n                Whether to overwrite the existing index pointers.\n\n        Returns:\n            Same object but with the `regionxcell_indptr` populated\n\n        \"\"\"\n        if self.regionxcell_indptr.exists() and not overwrite:\n            return self\n\n        mapping = self.parent.mapping[:]\n        coordinates = self.parent.coordinates[:]\n\n        # convert regions in parent to parent region ixs\n        parentregion_to_parentregion_ix = self.parent.regions.coordinates.index.get_loc\n\n        # reset the tensorstores\n        regionxcell_indptr = np.zeros((len(self.regions.coordinates) * len(self.obs), 2), dtype=np.int64)\n\n        # index pointers from parentregions to fragments\n        parent_regionxcell_indptr = indices_to_indptr(\n            mapping[:, 1] * self.parent.n_cells + mapping[:, 0], self.parent.n_regions * self.parent.n_cells\n        )\n\n        pbar = tqdm.tqdm(\n            total=len(self.regions.coordinates) * len(self.obs),\n            leave=False,\n            desc=\"Processing regions\",\n        )\n\n        self.regions.coordinates[\"ix\"] = np.arange(len(self.regions))\n        self.regions.coordinates[\"start_inclusive\"] = self.regions.coordinates[\"start\"] + (\n            self.regions.coordinates[\"strand\"] == -1\n        )\n        self.regions.coordinates[\"end_inclusive\"] = self.regions.coordinates[\"end\"] + (\n            self.regions.coordinates[\"strand\"] == -1\n        )\n\n        lastupdate = time.time()\n        i = 0\n\n        for parentregion, subcoordinates in self.regions.coordinates.groupby(self.parentregion_column):\n            # extract in which parent region we need to look\n            parentregion_ix = parentregion_to_parentregion_ix(parentregion)\n\n            for cell_ix in range(self.n_cells):\n                # extract the parent region coordinates per cell\n                parentregionxcell_ix = parentregion_ix * self.parent.n_cells + cell_ix\n                parentregion_start_ix, parentregion_end_ix = parent_regionxcell_indptr[\n                    parentregionxcell_ix : parentregionxcell_ix + 2\n                ]\n\n                coordinates_parentregion = coordinates[parentregion_start_ix:parentregion_end_ix]\n\n                for region_id, region in subcoordinates.iterrows():\n                    # extract fragments lying within the region\n                    region_ix = region[\"ix\"]\n                    regionxcell_ix = region_ix * self.n_cells + cell_ix\n\n                    if (pbar is not None) and (time.time() - lastupdate &gt; 1):\n                        pbar.update(i + 1)\n                        i = 0\n                        pbar.set_description(f\"Processing region {region_id} {cell_ix}\")\n                        lastupdate = time.time()\n                    else:\n                        i += 1\n\n                    # extract fragments lying within the region\n                    if inclusive == (True, False):\n                        n_excluded_left = np.searchsorted(coordinates_parentregion[:, 0], region[\"start_inclusive\"])\n                        n_included = (\n                            np.searchsorted(coordinates_parentregion[:, 0], region[\"end_inclusive\"]) - n_excluded_left\n                        )\n                    else:\n                        raise NotImplementedError(\"For now, only left-inclusive fragments are supported\")\n\n                    regionxcell_indptr[regionxcell_ix] = [\n                        parentregion_start_ix + n_excluded_left,\n                        parentregion_start_ix + n_excluded_left + n_included,\n                    ]\n\n        regionxcell_indptr_writer = self.regionxcell_indptr.open_creator(\n            shape=(len(self.regions.coordinates) * len(self.obs), 2)\n        )\n        regionxcell_indptr_writer[:] = regionxcell_indptr\n\n        return self\n\n    def filter_regions(self, regions: Regions, path: PathLike = None, overwrite=True) -&gt; Fragments:\n\"\"\"\n        Filter based on new regions\n\n        Parameters:\n            regions:\n                Regions to filter.\n        Returns:\n            A new Fragments object\n        \"\"\"\n\n        # check if new regions are a subset of the existing ones\n        if not regions.coordinates.index.isin(self.regions.coordinates.index).all():\n            raise ValueError(\"New regions should be a subset of the existing ones\")\n\n        # create new fragments\n        fragments = FragmentsView.create(\n            parent=self.parent,\n            regions=regions,\n            path=path,\n            parentregion_column=self.parentregion_column,\n            obs=self.obs,\n            var=regions.coordinates,\n            reset=overwrite,\n        )\n\n        return fragments\n\n    var: pd.DataFrame = TSV()\n\"\"\"DataFrame containing information about regions.\"\"\"\n\n    obs: pd.DataFrame = TSV()\n\"\"\"DataFrame containing information about cells.\"\"\"\n\n    @functools.cached_property\n    def n_regions(self):\n\"\"\"Number of regions\"\"\"\n        return self.var.shape[0]\n\n    @functools.cached_property\n    def n_cells(self):\n\"\"\"Number of cells\"\"\"\n        return self.obs.shape[0]\n\n    def estimate_fragment_per_cellxregion(self) -&gt; int:\n\"\"\"\n        Estimate the expected number of fragments per regionxcell combination. This is used to estimate the buffer size for loading fragments.\n        \"\"\"\n        return math.ceil((self.regionxcell_indptr[:, 1] - self.regionxcell_indptr[:, 0]).astype(float).mean())\n\n    @property\n    def coordinates(self):\n\"\"\"\n        Coordinates of the fragments, equal to the parent coordinates\n        \"\"\"\n        return self.parent.coordinates\n\n    @property\n    def mapping(self):\n\"\"\"\n        Mapping of the fragments, equal to the parent mapping\n        \"\"\"\n        return self.parent.mapping\n\n    @property\n    def counts(self):\n\"\"\"\n        Number of fragments per region and cell\n        \"\"\"\n        return (self.regionxcell_indptr[:, 1] - self.regionxcell_indptr[:, 0]).reshape(self.regions.n_regions, -1).T\n\n    _cache = None\n\n    def get_cache(self, region_oi):\n\"\"\"\n        Get the cache for a specific region\n        \"\"\"\n\n        if self._cache is None:\n            self._cache = {}\n\n        if region_oi in self._cache:\n            return self._cache[region_oi]\n\n        region_ix = self.regions.coordinates.index.get_loc(region_oi)\n        regionxcell_ixs = region_ix * self.n_cells + np.arange(self.n_cells)\n\n        indptrs = self.regionxcell_indptr[regionxcell_ixs]\n\n        coordinates_reader = self.parent.coordinates.open_reader()\n\n        n = []\n        i = 0\n        coordinates = []\n        for start, end in indptrs:\n            coordinates.append(coordinates_reader[start:end])\n            i += end - start\n            n.append(end - start)\n\n        coordinates = np.concatenate(coordinates)\n        local_cellxregion_ix = np.repeat(np.arange(len(indptrs)), n)\n        regionxcell_indptr = indices_to_indptr(local_cellxregion_ix, len(indptrs), dtype=np.int64)\n\n        self._cache[region_oi] = {\n            \"regionxcell_indptr\": regionxcell_indptr,\n            \"coordinates\": coordinates,\n        }\n\n        return self._cache[region_oi]\n\n    _libsize = None\n\n    @property\n    def libsize(self):\n        if self._libsize is None:\n            self._libsize = np.bincount(self.mapping[:, 0], minlength=self.n_cells)\n        return self._libsize\n</code></pre>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.view.FragmentsView.coordinates","title":"<code>coordinates</code>  <code>property</code>","text":"<p>Coordinates of the fragments, equal to the parent coordinates</p>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.view.FragmentsView.counts","title":"<code>counts</code>  <code>property</code>","text":"<p>Number of fragments per region and cell</p>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.view.FragmentsView.mapping","title":"<code>mapping</code>  <code>property</code>","text":"<p>Mapping of the fragments, equal to the parent mapping</p>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.view.FragmentsView.n_cells","title":"<code>n_cells</code>  <code>property</code> <code>cached</code>","text":"<p>Number of cells</p>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.view.FragmentsView.n_regions","title":"<code>n_regions</code>  <code>property</code> <code>cached</code>","text":"<p>Number of regions</p>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.view.FragmentsView.obs","title":"<code>obs: pd.DataFrame = TSV()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>DataFrame containing information about cells.</p>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.view.FragmentsView.parent","title":"<code>parent: Fragments = Linked()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The parent fragments object from which this view is created</p>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.view.FragmentsView.regions","title":"<code>regions: Regions = Linked()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The regions object</p>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.view.FragmentsView.regionxcell_indptr","title":"<code>regionxcell_indptr: TensorstoreInstance = Tensorstore(dtype='&lt;i8', chunks=[100000], compression='blosc', shape=(0, 2))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Index of fragments in the parent fragments object</p>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.view.FragmentsView.var","title":"<code>var: pd.DataFrame = TSV()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>DataFrame containing information about regions.</p>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.view.FragmentsView.create_regionxcell_indptr","title":"<code>create_regionxcell_indptr(inclusive=(True, False), overwrite=True)</code>","text":"<p>Create index pointers (left and right) for the fragments associated to each regionxcell combination</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <p>Number of regions to wait before saving the intermediate results and freeing up memory. Reduce this number to avoid running out of memory.</p> required <code>inclusive</code> <code>tuple</code> <p>Whether to only include fragments that are only partially overlapping with the region. Must be a tuple indicating left and/or right inclusivity.</p> <code>(True, False)</code> <code>overwrite</code> <p>Whether to overwrite the existing index pointers.</p> <code>True</code> <p>Returns:</p> Type Description <code>FragmentsView</code> <p>Same object but with the <code>regionxcell_indptr</code> populated</p> Source code in <code>src/chromatinhd/data/fragments/view.py</code> <pre><code>def create_regionxcell_indptr(\n    self,\n    inclusive: tuple = (True, False),\n    overwrite=True,\n) -&gt; FragmentsView:\n\"\"\"\n    Create index pointers (left and right) for the fragments associated to each regionxcell combination\n\n    Parameters:\n        batch_size:\n            Number of regions to wait before saving the intermediate results and freeing up memory. Reduce this number to avoid running out of memory.\n        inclusive:\n            Whether to only include fragments that are only partially overlapping with the region. Must be a tuple indicating left and/or right inclusivity.\n        overwrite:\n            Whether to overwrite the existing index pointers.\n\n    Returns:\n        Same object but with the `regionxcell_indptr` populated\n\n    \"\"\"\n    if self.regionxcell_indptr.exists() and not overwrite:\n        return self\n\n    mapping = self.parent.mapping[:]\n    coordinates = self.parent.coordinates[:]\n\n    # convert regions in parent to parent region ixs\n    parentregion_to_parentregion_ix = self.parent.regions.coordinates.index.get_loc\n\n    # reset the tensorstores\n    regionxcell_indptr = np.zeros((len(self.regions.coordinates) * len(self.obs), 2), dtype=np.int64)\n\n    # index pointers from parentregions to fragments\n    parentregion_fragment_indptr = indices_to_indptr(mapping[:, 1], self.parent.n_regions)\n\n    pbar = tqdm.tqdm(\n        total=len(self.regions.coordinates),\n        leave=False,\n        desc=\"Processing regions\",\n    )\n\n    self.regions.coordinates[\"ix\"] = np.arange(len(self.regions))\n\n    for parentregion, subcoordinates in self.regions.coordinates.groupby(self.parentregion_column):\n        # extract in which parent region we need to look\n        parentregion_ix = parentregion_to_parentregion_ix(parentregion)\n\n        parentregion_start_ix, parentregion_end_ix = (\n            parentregion_fragment_indptr[parentregion_ix],\n            parentregion_fragment_indptr[parentregion_ix + 1],\n        )\n\n        # extract parent's mapping and coordinates\n        coordinates_parentregion = coordinates[parentregion_start_ix:parentregion_end_ix]\n        cellmapping_parentregion = mapping[parentregion_start_ix:parentregion_end_ix, 0]\n\n        cell_indptr_parentregion = indices_to_indptr(cellmapping_parentregion, self.n_cells, dtype=np.int64)\n\n        for region_id, region in subcoordinates.iterrows():\n            region_ix = region[\"ix\"]\n\n            if pbar is not None:\n                pbar.update(1)\n                pbar.set_description(f\"Processing region {region_id}\")\n\n            # extract fragments lying within the region\n            # depending on the strand, we either need to include the start or the end\n            region[\"start_inclusive\"] = region[\"start\"] + (region[\"strand\"] == -1)\n            region[\"end_inclusive\"] = region[\"end\"] + (region[\"strand\"] == -1)\n\n            if inclusive == (True, False):\n                fragments_oi = (coordinates_parentregion[:, 0] &gt;= region[\"start_inclusive\"]) &amp; (\n                    coordinates_parentregion[:, 0] &lt; region[\"end_inclusive\"]\n                )\n                fragments_excluded_left = coordinates_parentregion[:, 0] &lt; region[\"start_inclusive\"]\n            else:\n                raise NotImplementedError(\"For now, only left-inclusive fragments are supported\")\n\n            n_excluded_left = np.bincount(cellmapping_parentregion[fragments_excluded_left], minlength=self.n_cells)\n            n_included = np.bincount(cellmapping_parentregion[fragments_oi], minlength=self.n_cells)\n\n            cell_indptr_left = cell_indptr_parentregion[:-1] + n_excluded_left\n            cell_indptr_right = cell_indptr_parentregion[:-1] + n_excluded_left + n_included\n\n            cell_indptr = np.stack([cell_indptr_left, cell_indptr_right], axis=1)\n\n            regionxcell_indptr[region_ix * self.n_cells : (region_ix + 1) * self.n_cells] = (\n                cell_indptr + parentregion_start_ix\n            )\n\n    regionxcell_indptr_writer = self.regionxcell_indptr.open_creator(\n        shape=(len(self.regions.coordinates) * len(self.obs), 2)\n    )\n    regionxcell_indptr_writer[:] = regionxcell_indptr\n\n    return self\n</code></pre>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.view.FragmentsView.create_regionxcell_indptr2","title":"<code>create_regionxcell_indptr2(inclusive=(True, False), overwrite=True)</code>","text":"<p>Create index pointers (left and right) for the fragments associated to each regionxcell combination. This implementation is faster if there are many samples with a lot of fragments (e.g. minibulk data)</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <p>Number of regions to wait before saving the intermediate results and freeing up memory. Reduce this number to avoid running out of memory.</p> required <code>inclusive</code> <code>tuple</code> <p>Whether to only include fragments that are only partially overlapping with the region. Must be a tuple indicating left and/or right inclusivity.</p> <code>(True, False)</code> <code>overwrite</code> <p>Whether to overwrite the existing index pointers.</p> <code>True</code> <p>Returns:</p> Type Description <code>FragmentsView</code> <p>Same object but with the <code>regionxcell_indptr</code> populated</p> Source code in <code>src/chromatinhd/data/fragments/view.py</code> <pre><code>def create_regionxcell_indptr2(\n    self,\n    inclusive: tuple = (True, False),\n    overwrite=True,\n) -&gt; FragmentsView:\n\"\"\"\n    Create index pointers (left and right) for the fragments associated to each regionxcell combination. This implementation is faster if there are many samples with a lot of fragments (e.g. minibulk data)\n\n    Parameters:\n        batch_size:\n            Number of regions to wait before saving the intermediate results and freeing up memory. Reduce this number to avoid running out of memory.\n        inclusive:\n            Whether to only include fragments that are only partially overlapping with the region. Must be a tuple indicating left and/or right inclusivity.\n        overwrite:\n            Whether to overwrite the existing index pointers.\n\n    Returns:\n        Same object but with the `regionxcell_indptr` populated\n\n    \"\"\"\n    if self.regionxcell_indptr.exists() and not overwrite:\n        return self\n\n    mapping = self.parent.mapping[:]\n    coordinates = self.parent.coordinates[:]\n\n    # convert regions in parent to parent region ixs\n    parentregion_to_parentregion_ix = self.parent.regions.coordinates.index.get_loc\n\n    # reset the tensorstores\n    regionxcell_indptr = np.zeros((len(self.regions.coordinates) * len(self.obs), 2), dtype=np.int64)\n\n    # index pointers from parentregions to fragments\n    parent_regionxcell_indptr = indices_to_indptr(\n        mapping[:, 1] * self.parent.n_cells + mapping[:, 0], self.parent.n_regions * self.parent.n_cells\n    )\n\n    pbar = tqdm.tqdm(\n        total=len(self.regions.coordinates) * len(self.obs),\n        leave=False,\n        desc=\"Processing regions\",\n    )\n\n    self.regions.coordinates[\"ix\"] = np.arange(len(self.regions))\n    self.regions.coordinates[\"start_inclusive\"] = self.regions.coordinates[\"start\"] + (\n        self.regions.coordinates[\"strand\"] == -1\n    )\n    self.regions.coordinates[\"end_inclusive\"] = self.regions.coordinates[\"end\"] + (\n        self.regions.coordinates[\"strand\"] == -1\n    )\n\n    lastupdate = time.time()\n    i = 0\n\n    for parentregion, subcoordinates in self.regions.coordinates.groupby(self.parentregion_column):\n        # extract in which parent region we need to look\n        parentregion_ix = parentregion_to_parentregion_ix(parentregion)\n\n        for cell_ix in range(self.n_cells):\n            # extract the parent region coordinates per cell\n            parentregionxcell_ix = parentregion_ix * self.parent.n_cells + cell_ix\n            parentregion_start_ix, parentregion_end_ix = parent_regionxcell_indptr[\n                parentregionxcell_ix : parentregionxcell_ix + 2\n            ]\n\n            coordinates_parentregion = coordinates[parentregion_start_ix:parentregion_end_ix]\n\n            for region_id, region in subcoordinates.iterrows():\n                # extract fragments lying within the region\n                region_ix = region[\"ix\"]\n                regionxcell_ix = region_ix * self.n_cells + cell_ix\n\n                if (pbar is not None) and (time.time() - lastupdate &gt; 1):\n                    pbar.update(i + 1)\n                    i = 0\n                    pbar.set_description(f\"Processing region {region_id} {cell_ix}\")\n                    lastupdate = time.time()\n                else:\n                    i += 1\n\n                # extract fragments lying within the region\n                if inclusive == (True, False):\n                    n_excluded_left = np.searchsorted(coordinates_parentregion[:, 0], region[\"start_inclusive\"])\n                    n_included = (\n                        np.searchsorted(coordinates_parentregion[:, 0], region[\"end_inclusive\"]) - n_excluded_left\n                    )\n                else:\n                    raise NotImplementedError(\"For now, only left-inclusive fragments are supported\")\n\n                regionxcell_indptr[regionxcell_ix] = [\n                    parentregion_start_ix + n_excluded_left,\n                    parentregion_start_ix + n_excluded_left + n_included,\n                ]\n\n    regionxcell_indptr_writer = self.regionxcell_indptr.open_creator(\n        shape=(len(self.regions.coordinates) * len(self.obs), 2)\n    )\n    regionxcell_indptr_writer[:] = regionxcell_indptr\n\n    return self\n</code></pre>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.view.FragmentsView.estimate_fragment_per_cellxregion","title":"<code>estimate_fragment_per_cellxregion()</code>","text":"<p>Estimate the expected number of fragments per regionxcell combination. This is used to estimate the buffer size for loading fragments.</p> Source code in <code>src/chromatinhd/data/fragments/view.py</code> <pre><code>def estimate_fragment_per_cellxregion(self) -&gt; int:\n\"\"\"\n    Estimate the expected number of fragments per regionxcell combination. This is used to estimate the buffer size for loading fragments.\n    \"\"\"\n    return math.ceil((self.regionxcell_indptr[:, 1] - self.regionxcell_indptr[:, 0]).astype(float).mean())\n</code></pre>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.view.FragmentsView.filter_regions","title":"<code>filter_regions(regions, path=None, overwrite=True)</code>","text":"<p>Filter based on new regions</p> <p>Parameters:</p> Name Type Description Default <code>regions</code> <code>Regions</code> <p>Regions to filter.</p> required <p>Returns:</p> Type Description <code>Fragments</code> <p>A new Fragments object</p> Source code in <code>src/chromatinhd/data/fragments/view.py</code> <pre><code>def filter_regions(self, regions: Regions, path: PathLike = None, overwrite=True) -&gt; Fragments:\n\"\"\"\n    Filter based on new regions\n\n    Parameters:\n        regions:\n            Regions to filter.\n    Returns:\n        A new Fragments object\n    \"\"\"\n\n    # check if new regions are a subset of the existing ones\n    if not regions.coordinates.index.isin(self.regions.coordinates.index).all():\n        raise ValueError(\"New regions should be a subset of the existing ones\")\n\n    # create new fragments\n    fragments = FragmentsView.create(\n        parent=self.parent,\n        regions=regions,\n        path=path,\n        parentregion_column=self.parentregion_column,\n        obs=self.obs,\n        var=regions.coordinates,\n        reset=overwrite,\n    )\n\n    return fragments\n</code></pre>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.view.FragmentsView.from_fragments","title":"<code>from_fragments(parent, regions, parentregion_column='chrom', obs=None, var=None, path=None, overwrite=False)</code>  <code>classmethod</code>","text":"<p>Creates a fragments view from a parent fragments object and a regions object</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>Fragments</code> <p>Parent fragments object. If a fragments view is provided, the parent of the parent will be used.</p> required <code>regions</code> <code>Regions</code> <p>Regions object</p> required <code>obs</code> <code>pd.DataFrame</code> <p>DataFrame containing information about cells, will be copied from the fragments object if not provided</p> <code>None</code> <code>parentregion_column</code> <code>str</code> <p>Column in the regions coordinates that links each new region to the regions of the original fragments. This is typically the chromosome column. This column should be present in both <code>parent.regions.coordinates</code> and <code>regions.coordinates</code></p> <code>'chrom'</code> <code>path</code> <code>PathLike</code> <p>Path to store the fragments view</p> <code>None</code> Source code in <code>src/chromatinhd/data/fragments/view.py</code> <pre><code>@classmethod\ndef from_fragments(\n    cls,\n    parent: Fragments,\n    regions: Regions,\n    parentregion_column: str = \"chrom\",\n    obs: pd.DataFrame = None,\n    var: pd.DataFrame = None,\n    path: PathLike = None,\n    overwrite: bool = False,\n):\n\"\"\"\n    Creates a fragments view from a parent fragments object and a regions object\n\n    Parameters:\n        parent:\n            Parent fragments object. If a fragments view is provided, the parent of the parent will be used.\n        regions:\n            Regions object\n        obs:\n            DataFrame containing information about cells, will be copied from the fragments object if not provided\n        parentregion_column:\n            Column in the regions coordinates that links each new region to the regions of the original fragments. This is typically the chromosome column. This column should be present in both `parent.regions.coordinates` and `regions.coordinates`\n        path:\n            Path to store the fragments view\n    \"\"\"\n\n    if isinstance(parent, FragmentsView):\n        while isinstance(parent, FragmentsView):\n            parent = parent.parent\n    if not isinstance(parent, Fragments):\n        raise ValueError(\"parent should be a Fragments object\")\n    if not isinstance(regions, Regions):\n        raise ValueError(\"regions should be a Regions object\", regions.__class__)\n\n    # dummy proofing\n    if parentregion_column not in regions.coordinates.columns:\n        raise ValueError(\n            f\"Column {parentregion_column} not in regions coordinates. Available columns are {regions.coordinates.columns}\"\n        )\n    if parentregion_column not in parent.regions.coordinates.columns:\n        raise ValueError(\n            f\"Column {parentregion_column} not in fragments regions coordinates. Available columns are {parent.regions.coordinates.columns}\"\n        )\n    if not (regions.coordinates[parentregion_column].isin(parent.regions.coordinates[parentregion_column])).all():\n        raise ValueError(\n            f\"Not all regions are present in the parent fragments. Missing regions: {regions.coordinates[parentregion_column][~regions.coordinates[parentregion_column].isin(parent.regions.coordinates[parentregion_column])]}\"\n        )\n\n    self = cls.create(\n        parent=parent,\n        regions=regions,\n        path=path,\n        parentregion_column=parentregion_column,\n        obs=parent.obs if obs is None else obs,\n        var=regions.coordinates if var is None else var,\n        reset=overwrite,\n    )\n\n    return self\n</code></pre>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.view.FragmentsView.get_cache","title":"<code>get_cache(region_oi)</code>","text":"<p>Get the cache for a specific region</p> Source code in <code>src/chromatinhd/data/fragments/view.py</code> <pre><code>def get_cache(self, region_oi):\n\"\"\"\n    Get the cache for a specific region\n    \"\"\"\n\n    if self._cache is None:\n        self._cache = {}\n\n    if region_oi in self._cache:\n        return self._cache[region_oi]\n\n    region_ix = self.regions.coordinates.index.get_loc(region_oi)\n    regionxcell_ixs = region_ix * self.n_cells + np.arange(self.n_cells)\n\n    indptrs = self.regionxcell_indptr[regionxcell_ixs]\n\n    coordinates_reader = self.parent.coordinates.open_reader()\n\n    n = []\n    i = 0\n    coordinates = []\n    for start, end in indptrs:\n        coordinates.append(coordinates_reader[start:end])\n        i += end - start\n        n.append(end - start)\n\n    coordinates = np.concatenate(coordinates)\n    local_cellxregion_ix = np.repeat(np.arange(len(indptrs)), n)\n    regionxcell_indptr = indices_to_indptr(local_cellxregion_ix, len(indptrs), dtype=np.int64)\n\n    self._cache[region_oi] = {\n        \"regionxcell_indptr\": regionxcell_indptr,\n        \"coordinates\": coordinates,\n    }\n\n    return self._cache[region_oi]\n</code></pre>"},{"location":"reference/data/motifscan/","title":"Motifscan","text":""},{"location":"reference/data/motifscan/#chromatinhd.data.motifscan.Motifscan","title":"<code>chromatinhd.data.motifscan.Motifscan</code>","text":"<p>         Bases: <code>Flow</code></p> <p>A sprase representation of locations of different motifs in regions of the genome</p> Source code in <code>src/chromatinhd/data/motifscan/motifscan.py</code> <pre><code>class Motifscan(Flow):\n\"\"\"\n    A sprase representation of locations of different motifs in regions of the genome\n    \"\"\"\n\n    regions = Linked()\n    \"The regions\"\n\n    indptr: TensorstoreInstance = Tensorstore(dtype=\"&lt;i8\", chunks=(10000,))\n    \"The index pointers for each position in the regions\"\n\n    region_indptr: TensorstoreInstance = Tensorstore(dtype=\"&lt;i8\", chunks=(1000,), compression=None)\n    \"The index pointers for region\"\n\n    coordinates: TensorstoreInstance = Tensorstore(dtype=\"&lt;i4\", chunks=(10000,))\n    \"Coordinate associated to each site\"\n\n    region_indices: TensorstoreInstance = Tensorstore(dtype=\"&lt;i4\", chunks=(10000,))\n    \"Region index associated to each site\"\n\n    indices: TensorstoreInstance = Tensorstore(dtype=\"&lt;i4\", chunks=(10000,))\n    \"Motif index associated to each site\"\n\n    # positions: TensorstoreInstance = Tensorstore(dtype=\"&lt;i8\", chunks=(10000,))\n    # \"Cumulative coordinate of each site\"\n\n    scores: TensorstoreInstance = Tensorstore(dtype=\"&lt;f4\", chunks=(10000,))\n    \"Scores associated with each detected site\"\n\n    strands: TensorstoreInstance = Tensorstore(dtype=\"&lt;f4\", chunks=(10000,))\n    \"Strand associated with each detected site\"\n\n    shape = Stored()\n\n    motifs = StoredDataFrame()\n    \"Dataframe storing auxilliary information for each motif\"\n\n    @classmethod\n    def from_pwms(\n        cls,\n        pwms: dict,\n        regions: Regions,\n        fasta_file: Union[str, pathlib.Path] = None,\n        region_onehots: Dict[np.ndarray, torch.Tensor] = None,\n        cutoffs: Union[int, float, pd.Series] = None,\n        cutoff_col: str = None,\n        min_cutoff=3.0,\n        motifs: pd.DataFrame = None,\n        device: str = None,\n        batch_size: int = 50000000,\n        path: Union[str, pathlib.Path] = None,\n        overwrite: bool = True,\n        reuse: bool = False,\n    ):\n\"\"\"\n        Create a motifscan object from a set of pwms and a set of regions\n\n        Parameters:\n            pwms:\n                A dictionary of pwms, where the keys are the motif ids and the values are the pwms\n            regions:\n                A regions object\n            fasta_file:\n                The location of the fasta file containing the genome\n            region_onehots:\n                A dictionary containing the onehot encoding of each region. If not given, the onehot encoding will be extracted from the fasta file\n            motifs:\n                A dataframe containing auxilliary information for each motif\n            cutoffs:\n                A dictionary containing the cutoffs for each motif.\n            cutoff_col:\n                The column in the motifs dataframe containing the cutoffs\n            device:\n                The device to use for the scanning\n            batch_size:\n                The batch size to use for scanning. Decrease this if the GPU runs out of memory\n            path:\n                The folder where the motifscan data will be stored.\n            overwrite:\n                Whether to overwrite existing motifscan data\n            reuse:\n                Whether to reuse existing motifscan data\n        \"\"\"\n\n        if device is None:\n            device = get_default_device()\n\n        self = cls(path)\n\n        if ((reuse) or (not overwrite)) and self.o.coordinates.exists(self):\n            if not reuse:\n                import warnings\n\n                warnings.warn(\n                    \"Motifscan already exists. Use overwrite=True to overwrite, reuse=True to ignore this warning.\"\n                )\n            return self\n\n        if overwrite:\n            self.reset()\n\n        self.motifs = motifs\n        self.regions = regions\n\n        # check or create cutoffs\n        if cutoffs is None:\n            if cutoff_col is None:\n                raise ValueError(\"Either motifs+cutoff_col or cutoffs need to be specified.\")\n            if motifs is None:\n                raise ValueError(\"Either motifs+cutoff_col or cutoffs need to be specified. motifs is not given\")\n\n            cutoffs = motifs[cutoff_col].to_dict()\n        else:\n            if isinstance(cutoffs, (float, int)):\n                cutoffs = {motif: cutoffs for motif in pwms.keys()}\n            elif isinstance(cutoffs, pd.Series):\n                cutoffs = cutoffs.to_dict()\n            else:\n                raise ValueError(\"cutoffs should be a float, int, dict or pd.Series\")\n            assert set(cutoffs.keys()) == set(pwms.keys())\n\n        # check or create motifs\n        if motifs is None:\n            motifs = pd.DataFrame(\n                {\n                    \"motif\": list(pwms.keys()),\n                }\n            ).set_index(\"motif\")\n\n        # divide regions into batches according to batch size\n        region_coordinates = regions.coordinates\n\n        region_coordinates = divide_regions_in_batches(region_coordinates, batch_size=batch_size)\n\n        # load in fasta file\n        if fasta_file is not None:\n            import pysam\n\n            fasta = pysam.FastaFile(fasta_file)\n        else:\n            fasta = None\n            if region_onehots is None:\n                raise ValueError(\"Either fasta_file or region_onehots need to be specified\")\n\n        self.indices.open_creator()\n        self.scores.open_creator()\n        self.strands.open_creator()\n        self.coordinates.open_creator()\n        self.region_indices.open_creator()\n\n        # do the actual counting by looping over the batches, extract the sequences and scanning\n        progress = tqdm.tqdm(region_coordinates.groupby(\"batch\"))\n        for batch, region_coordinates_batch in progress:\n            # extract onehot\n            if fasta is None:\n                sequences = [\n                    fasta.fetch(chrom, start, end + 1)\n                    for chrom, start, end in region_coordinates_batch[[\"chrom\", \"start\", \"end\"]].values\n                ]\n                if not all(len(sequence) == len(sequences[0]) for sequence in sequences):\n                    raise ValueError(\"All regions/sequences should have the same length\")\n                onehot = create_onehots(sequences).permute(0, 2, 1)\n            else:\n                if region_onehots is None:\n                    if fasta_file is None:\n                        raise ValueError(\"fasta_file must be provided if fasta and region_onehots is not provided\")\n                    progress.set_description(\"Extracting sequences\")\n                    region_onehots = create_region_onehots(regions, fasta_file)\n                onehot = torch.stack([region_onehots[region] for region in region_coordinates_batch.index]).permute(\n                    0, 2, 1\n                )\n            onehot = onehot.to(device)\n\n            progress.set_description(\n                f\"Scanning batch {batch} {region_coordinates_batch.index[0]}-{region_coordinates_batch.index[-1]}\"\n            )\n\n            assert onehot.shape[1] == 4\n            assert onehot.shape[2] == region_coordinates_batch[\"len\"].iloc[0], (\n                onehot.shape[2],\n                region_coordinates_batch[\"len\"].iloc[0],\n            )\n\n            scores_raw = []\n            indices_raw = []\n            coordinates_raw = []\n            strands_raw = []\n            region_indices_raw = []\n            for motif_ix, motif in tqdm.tqdm(enumerate(motifs.index)):\n                cutoff = cutoffs[motif]\n\n                if cutoff &lt; min_cutoff:\n                    cutoff = min_cutoff\n\n                # get pwm\n                pwm = pwms[motif]\n                if not torch.is_tensor(pwm):\n                    pwm = torch.from_numpy(pwm)\n                pwm2 = pwm.to(dtype=torch.float32, device=onehot.device).transpose(1, 0)\n\n                (\n                    scores,\n                    positions,\n                    strands,\n                ) = scan(onehot, pwm2, cutoff=cutoff)\n\n                coordinates = positions.astype(np.int32) % onehot.shape[1]\n\n                region_indices = positions // onehot.shape[1]\n\n                coordinates = (\n                    coordinates\n                    + (self.regions.coordinates[\"start\"] - self.regions.coordinates[\"tss\"]).values[region_indices]\n                )\n\n                coordinates_raw.append(coordinates)\n                indices_raw.append(np.full_like(coordinates, motif_ix, dtype=np.int32))\n                strands_raw.append(strands)\n                scores_raw.append(scores)\n                region_indices_raw.append(region_indices)\n\n            # concatenate raw values (sorted by motif)\n            coordinates = np.concatenate(coordinates_raw)\n            indices = np.concatenate(indices_raw)\n            strands = np.concatenate(strands_raw)\n            scores = np.concatenate(scores_raw)\n            region_indices = np.concatenate(region_indices_raw)\n\n            # sort according to position\n            sorted_idx = np.lexsort([coordinates, region_indices])\n            indices = indices[sorted_idx]\n            scores = scores[sorted_idx]\n            strands = strands[sorted_idx]\n            coordinates = coordinates[sorted_idx]\n            region_indices = region_indices[sorted_idx]\n\n            # store batch\n            self.indices.extend(indices)\n            self.scores.extend(scores)\n            self.strands.extend(strands)\n            self.coordinates.extend(coordinates)\n            self.region_indices.extend(region_indices)\n\n        return self\n\n    def create_region_indptr(self, overwrite=False):\n\"\"\"\n        Populate the region_indptr\n        \"\"\"\n\n        if self.o.region_indptr.exists(self) and not overwrite:\n            return\n\n        region_indices_reader = self.region_indices.open_reader()\n        self.region_indptr = indices_to_indptr_chunked(region_indices_reader, self.regions.n_regions, dtype=np.int64)\n\n    def create_indptr(self, overwrite=False):\n\"\"\"\n        Populate the indptr\n        \"\"\"\n\n        if self.o.indptr.exists(self) and not overwrite:\n            return\n\n        if self.regions.width is not None:\n            indptr = self.indptr.open_creator(\n                shape=((self.regions.n_regions * self.regions.width) + 1,), dtype=np.int64\n            )\n            region_width = self.regions.width\n            for region_ix, (region_start, region_end) in tqdm.tqdm(\n                enumerate(zip(self.region_indptr[:-1], self.region_indptr[1:]))\n            ):\n                indptr[region_ix * region_width : (region_ix + 1) * region_width] = (\n                    indices_to_indptr(self.coordinates[region_start:region_end], self.regions.width)[:-1] + region_start\n                )\n            indptr[-1] = region_end\n        else:\n            indptr = self.indptr.open_creator(shape=(self.regions.cumulative_region_lengths[-1] + 1,), dtype=np.int64)\n            for region_ix, (region_start, region_end) in tqdm.tqdm(\n                enumerate(zip(self.region_indptr[:-1], self.region_indptr[1:]))\n            ):\n                region_start_position = self.regions.cumulative_region_lengths[region_ix]\n                region_end_position = self.regions.cumulative_region_lengths[region_ix + 1]\n                indptr[region_start_position:region_end_position] = (\n                    indices_to_indptr_chunked(\n                        self.coordinates[region_start:region_end],\n                        region_end_position - region_start_position,\n                    )[:-1]\n                    + region_start\n                )\n            indptr[-1] = region_end\n\n    @classmethod\n    def from_positions(cls, positions, indices, scores, strands, regions, motifs, path=None):\n\"\"\"\n        Create a motifscan object from positions, indices, scores, strands, regions and motifs\n        \"\"\"\n        self = cls(path=path)\n\n        # sort the positions\n        sorted_idx = np.argsort(positions)\n\n        self.positions = positions[sorted_idx]\n        self.indices = indices[sorted_idx]\n        self.scores = scores[sorted_idx]\n        self.strands = strands[sorted_idx]\n        self.regions = regions\n        self.motifs = motifs\n\n        return self\n\n    def filter(self, motif_ids, path=None):\n\"\"\"\n        Select a subset of motifs\n        \"\"\"\n\n        self.motifs[\"ix\"] = np.arange(len(self.motifs))\n        motif_ixs = self.motifs.loc[motif_ids, \"ix\"]\n\n        selected_sites = np.isin(self.indices, motif_ixs)\n\n        new = self.__class__(path=path).create(\n            regions=self.regions,\n            positions=self.positions[selected_sites],\n            indices=self.indices[selected_sites],\n            scores=self.scores[selected_sites],\n            strands=self.strands[selected_sites],\n            motifs=self.motifs.loc[motif_ids],\n        )\n\n        new.create_indptr()\n        return new\n\n    @property\n    def n_motifs(self):\n        return len(self.motifs)\n\n    @property\n    def scanned(self):\n        return self.o.indices.exists(self)\n\n    def get_slice(\n        self,\n        region_ix=None,\n        region_id=None,\n        start=None,\n        end=None,\n        return_indptr=False,\n        return_scores=True,\n        return_strands=True,\n        motif_ixs=None,\n    ):\n\"\"\"\n        Get a slice of the motifscan\n\n        Parameters:\n            region:\n                Region id\n            start:\n                Start of the slice, in region coordinates\n            end:\n                End of the slice, in region coordinates\n\n        Returns:\n            Motifs positions, indices, scores and strands of the slice\n        \"\"\"\n        if region_id is not None:\n            region = self.regions.coordinates.loc[region_id]\n        elif region_ix is not None:\n            region = self.regions.coordinates.iloc[region_ix]\n        else:\n            raise ValueError(\"Either region or region_ix should be provided\")\n        if region_ix is None:\n            region_ix = self.regions.coordinates.index.get_indexer([region_id])[0]\n\n        if self.regions.width is None:\n            raise NotImplementedError(\"get_slice is only implemented for fixed width regions\")\n        width = self.regions.width\n\n        if start is None:\n            start = self.regions.window[0]\n        if end is None:\n            end = self.regions.window[1]\n\n        if self.o.indptr.exists(self):\n            start = region_ix * width + start\n            end = region_ix * width + end\n            indptr = self.indptr[start : end + 1]\n            indptr_start, indptr_end = indptr[0], indptr[-1]\n            indptr = indptr - indptr[0]\n        else:\n            region_start = self.region_indptr[region_ix]\n            region_end = self.region_indptr[region_ix + 1]\n            coordinates = self.coordinates[region_start:region_end]\n            indptr_start = coordinates.searchsorted(start - 1) + region_start\n            indptr_end = coordinates.searchsorted(end) + region_start\n\n        coordinates = self.coordinates[indptr_start:indptr_end]\n        indices = self.indices[indptr_start:indptr_end]\n\n        out = [coordinates, indices]\n        if return_scores:\n            out.append(self.scores[indptr_start:indptr_end])\n        if return_strands:\n            out.append(self.strands[indptr_start:indptr_end])\n\n        if motif_ixs is not None:\n            selection = np.isin(indices, motif_ixs)\n            out = [x[selection] for x in out]\n\n        if return_indptr:\n            out.append(indptr)\n\n        return out\n\n    def count_slices(self, slices: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Get multiple slices of the motifscan\n\n        Parameters:\n            slices:\n                DataFrame containing the slices to get. Each row should contain a region_ix, start and end column. The region_ix should refer to the index of the regions object. The start and end columns should contain the start and end of the slice, in region coordinates.\n\n        Returns:\n            DataFrame containing the counts of each motif (columns) in each slice (rows)\n        \"\"\"\n\n        if self.regions.window is None:\n            raise NotImplementedError(\"count_slices is only implemented for regions with a window\")\n\n        if \"region_ix\" not in slices:\n            slices[\"region_ix\"] = self.regions.coordinates.index.get_indexer(slices[\"region\"])\n\n        progress = enumerate(zip(slices[\"start\"], slices[\"end\"], slices[\"region_ix\"]))\n        progress = tqdm.tqdm(\n            progress,\n            total=len(slices),\n            leave=False,\n            desc=\"Counting slices\",\n        )\n\n        motif_counts = np.zeros((len(slices), self.n_motifs), dtype=int)\n        for i, (relative_start, relative_end, region_ix) in progress:\n            start = relative_start\n            end = relative_end\n            positions, indices = self.get_slice(\n                region_ix=region_ix,\n                start=start,\n                end=end,\n                return_scores=False,\n                return_strands=False,\n                return_indptr=False,\n            )\n            motif_counts[i] = np.bincount(indices, minlength=self.n_motifs)\n        motif_counts = pd.DataFrame(motif_counts, index=slices.index, columns=self.motifs.index)\n        return motif_counts\n\n    def select_motif(self, x=None, symbol=None):\n        if symbol is not None:\n            return self.motifs.loc[self.motifs[\"symbol\"] == symbol].index[0]\n        # return motifscan.motifs.loc[motifscan.motifs.index.str.contains(str)].sort_values(\"quality\").index[0]\n        return self.motifs.loc[self.motifs.index.str.contains(x)].index[0]\n\n    def select_motifs(self, x=None, symbol=None):\n        if symbol is not None:\n            return self.motifs.loc[self.motifs[\"symbol\"] == symbol].index.tolist()\n        return self.motifs.loc[self.motifs.index.str.contains(x)].index.tolist()\n</code></pre>"},{"location":"reference/data/motifscan/#chromatinhd.data.motifscan.motifscan.Motifscan.coordinates","title":"<code>coordinates: TensorstoreInstance = Tensorstore(dtype='&lt;i4', chunks=(10000))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Coordinate associated to each site</p>"},{"location":"reference/data/motifscan/#chromatinhd.data.motifscan.motifscan.Motifscan.indices","title":"<code>indices: TensorstoreInstance = Tensorstore(dtype='&lt;i4', chunks=(10000))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Motif index associated to each site</p>"},{"location":"reference/data/motifscan/#chromatinhd.data.motifscan.motifscan.Motifscan.indptr","title":"<code>indptr: TensorstoreInstance = Tensorstore(dtype='&lt;i8', chunks=(10000))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The index pointers for each position in the regions</p>"},{"location":"reference/data/motifscan/#chromatinhd.data.motifscan.motifscan.Motifscan.motifs","title":"<code>motifs = StoredDataFrame()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dataframe storing auxilliary information for each motif</p>"},{"location":"reference/data/motifscan/#chromatinhd.data.motifscan.motifscan.Motifscan.region_indices","title":"<code>region_indices: TensorstoreInstance = Tensorstore(dtype='&lt;i4', chunks=(10000))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Region index associated to each site</p>"},{"location":"reference/data/motifscan/#chromatinhd.data.motifscan.motifscan.Motifscan.region_indptr","title":"<code>region_indptr: TensorstoreInstance = Tensorstore(dtype='&lt;i8', chunks=(1000), compression=None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The index pointers for region</p>"},{"location":"reference/data/motifscan/#chromatinhd.data.motifscan.motifscan.Motifscan.regions","title":"<code>regions = Linked()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The regions</p>"},{"location":"reference/data/motifscan/#chromatinhd.data.motifscan.motifscan.Motifscan.scores","title":"<code>scores: TensorstoreInstance = Tensorstore(dtype='&lt;f4', chunks=(10000))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Scores associated with each detected site</p>"},{"location":"reference/data/motifscan/#chromatinhd.data.motifscan.motifscan.Motifscan.strands","title":"<code>strands: TensorstoreInstance = Tensorstore(dtype='&lt;f4', chunks=(10000))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Strand associated with each detected site</p>"},{"location":"reference/data/motifscan/#chromatinhd.data.motifscan.motifscan.Motifscan.count_slices","title":"<code>count_slices(slices)</code>","text":"<p>Get multiple slices of the motifscan</p> <p>Parameters:</p> Name Type Description Default <code>slices</code> <code>pd.DataFrame</code> <p>DataFrame containing the slices to get. Each row should contain a region_ix, start and end column. The region_ix should refer to the index of the regions object. The start and end columns should contain the start and end of the slice, in region coordinates.</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>DataFrame containing the counts of each motif (columns) in each slice (rows)</p> Source code in <code>src/chromatinhd/data/motifscan/motifscan.py</code> <pre><code>def count_slices(self, slices: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n    Get multiple slices of the motifscan\n\n    Parameters:\n        slices:\n            DataFrame containing the slices to get. Each row should contain a region_ix, start and end column. The region_ix should refer to the index of the regions object. The start and end columns should contain the start and end of the slice, in region coordinates.\n\n    Returns:\n        DataFrame containing the counts of each motif (columns) in each slice (rows)\n    \"\"\"\n\n    if self.regions.window is None:\n        raise NotImplementedError(\"count_slices is only implemented for regions with a window\")\n\n    if \"region_ix\" not in slices:\n        slices[\"region_ix\"] = self.regions.coordinates.index.get_indexer(slices[\"region\"])\n\n    progress = enumerate(zip(slices[\"start\"], slices[\"end\"], slices[\"region_ix\"]))\n    progress = tqdm.tqdm(\n        progress,\n        total=len(slices),\n        leave=False,\n        desc=\"Counting slices\",\n    )\n\n    motif_counts = np.zeros((len(slices), self.n_motifs), dtype=int)\n    for i, (relative_start, relative_end, region_ix) in progress:\n        start = relative_start\n        end = relative_end\n        positions, indices = self.get_slice(\n            region_ix=region_ix,\n            start=start,\n            end=end,\n            return_scores=False,\n            return_strands=False,\n            return_indptr=False,\n        )\n        motif_counts[i] = np.bincount(indices, minlength=self.n_motifs)\n    motif_counts = pd.DataFrame(motif_counts, index=slices.index, columns=self.motifs.index)\n    return motif_counts\n</code></pre>"},{"location":"reference/data/motifscan/#chromatinhd.data.motifscan.motifscan.Motifscan.create_indptr","title":"<code>create_indptr(overwrite=False)</code>","text":"<p>Populate the indptr</p> Source code in <code>src/chromatinhd/data/motifscan/motifscan.py</code> <pre><code>def create_indptr(self, overwrite=False):\n\"\"\"\n    Populate the indptr\n    \"\"\"\n\n    if self.o.indptr.exists(self) and not overwrite:\n        return\n\n    if self.regions.width is not None:\n        indptr = self.indptr.open_creator(\n            shape=((self.regions.n_regions * self.regions.width) + 1,), dtype=np.int64\n        )\n        region_width = self.regions.width\n        for region_ix, (region_start, region_end) in tqdm.tqdm(\n            enumerate(zip(self.region_indptr[:-1], self.region_indptr[1:]))\n        ):\n            indptr[region_ix * region_width : (region_ix + 1) * region_width] = (\n                indices_to_indptr(self.coordinates[region_start:region_end], self.regions.width)[:-1] + region_start\n            )\n        indptr[-1] = region_end\n    else:\n        indptr = self.indptr.open_creator(shape=(self.regions.cumulative_region_lengths[-1] + 1,), dtype=np.int64)\n        for region_ix, (region_start, region_end) in tqdm.tqdm(\n            enumerate(zip(self.region_indptr[:-1], self.region_indptr[1:]))\n        ):\n            region_start_position = self.regions.cumulative_region_lengths[region_ix]\n            region_end_position = self.regions.cumulative_region_lengths[region_ix + 1]\n            indptr[region_start_position:region_end_position] = (\n                indices_to_indptr_chunked(\n                    self.coordinates[region_start:region_end],\n                    region_end_position - region_start_position,\n                )[:-1]\n                + region_start\n            )\n        indptr[-1] = region_end\n</code></pre>"},{"location":"reference/data/motifscan/#chromatinhd.data.motifscan.motifscan.Motifscan.create_region_indptr","title":"<code>create_region_indptr(overwrite=False)</code>","text":"<p>Populate the region_indptr</p> Source code in <code>src/chromatinhd/data/motifscan/motifscan.py</code> <pre><code>def create_region_indptr(self, overwrite=False):\n\"\"\"\n    Populate the region_indptr\n    \"\"\"\n\n    if self.o.region_indptr.exists(self) and not overwrite:\n        return\n\n    region_indices_reader = self.region_indices.open_reader()\n    self.region_indptr = indices_to_indptr_chunked(region_indices_reader, self.regions.n_regions, dtype=np.int64)\n</code></pre>"},{"location":"reference/data/motifscan/#chromatinhd.data.motifscan.motifscan.Motifscan.filter","title":"<code>filter(motif_ids, path=None)</code>","text":"<p>Select a subset of motifs</p> Source code in <code>src/chromatinhd/data/motifscan/motifscan.py</code> <pre><code>def filter(self, motif_ids, path=None):\n\"\"\"\n    Select a subset of motifs\n    \"\"\"\n\n    self.motifs[\"ix\"] = np.arange(len(self.motifs))\n    motif_ixs = self.motifs.loc[motif_ids, \"ix\"]\n\n    selected_sites = np.isin(self.indices, motif_ixs)\n\n    new = self.__class__(path=path).create(\n        regions=self.regions,\n        positions=self.positions[selected_sites],\n        indices=self.indices[selected_sites],\n        scores=self.scores[selected_sites],\n        strands=self.strands[selected_sites],\n        motifs=self.motifs.loc[motif_ids],\n    )\n\n    new.create_indptr()\n    return new\n</code></pre>"},{"location":"reference/data/motifscan/#chromatinhd.data.motifscan.motifscan.Motifscan.from_positions","title":"<code>from_positions(positions, indices, scores, strands, regions, motifs, path=None)</code>  <code>classmethod</code>","text":"<p>Create a motifscan object from positions, indices, scores, strands, regions and motifs</p> Source code in <code>src/chromatinhd/data/motifscan/motifscan.py</code> <pre><code>@classmethod\ndef from_positions(cls, positions, indices, scores, strands, regions, motifs, path=None):\n\"\"\"\n    Create a motifscan object from positions, indices, scores, strands, regions and motifs\n    \"\"\"\n    self = cls(path=path)\n\n    # sort the positions\n    sorted_idx = np.argsort(positions)\n\n    self.positions = positions[sorted_idx]\n    self.indices = indices[sorted_idx]\n    self.scores = scores[sorted_idx]\n    self.strands = strands[sorted_idx]\n    self.regions = regions\n    self.motifs = motifs\n\n    return self\n</code></pre>"},{"location":"reference/data/motifscan/#chromatinhd.data.motifscan.motifscan.Motifscan.from_pwms","title":"<code>from_pwms(pwms, regions, fasta_file=None, region_onehots=None, cutoffs=None, cutoff_col=None, min_cutoff=3.0, motifs=None, device=None, batch_size=50000000, path=None, overwrite=True, reuse=False)</code>  <code>classmethod</code>","text":"<p>Create a motifscan object from a set of pwms and a set of regions</p> <p>Parameters:</p> Name Type Description Default <code>pwms</code> <code>dict</code> <p>A dictionary of pwms, where the keys are the motif ids and the values are the pwms</p> required <code>regions</code> <code>Regions</code> <p>A regions object</p> required <code>fasta_file</code> <code>Union[str, pathlib.Path]</code> <p>The location of the fasta file containing the genome</p> <code>None</code> <code>region_onehots</code> <code>Dict[np.ndarray, torch.Tensor]</code> <p>A dictionary containing the onehot encoding of each region. If not given, the onehot encoding will be extracted from the fasta file</p> <code>None</code> <code>motifs</code> <code>pd.DataFrame</code> <p>A dataframe containing auxilliary information for each motif</p> <code>None</code> <code>cutoffs</code> <code>Union[int, float, pd.Series]</code> <p>A dictionary containing the cutoffs for each motif.</p> <code>None</code> <code>cutoff_col</code> <code>str</code> <p>The column in the motifs dataframe containing the cutoffs</p> <code>None</code> <code>device</code> <code>str</code> <p>The device to use for the scanning</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>The batch size to use for scanning. Decrease this if the GPU runs out of memory</p> <code>50000000</code> <code>path</code> <code>Union[str, pathlib.Path]</code> <p>The folder where the motifscan data will be stored.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite existing motifscan data</p> <code>True</code> <code>reuse</code> <code>bool</code> <p>Whether to reuse existing motifscan data</p> <code>False</code> Source code in <code>src/chromatinhd/data/motifscan/motifscan.py</code> <pre><code>@classmethod\ndef from_pwms(\n    cls,\n    pwms: dict,\n    regions: Regions,\n    fasta_file: Union[str, pathlib.Path] = None,\n    region_onehots: Dict[np.ndarray, torch.Tensor] = None,\n    cutoffs: Union[int, float, pd.Series] = None,\n    cutoff_col: str = None,\n    min_cutoff=3.0,\n    motifs: pd.DataFrame = None,\n    device: str = None,\n    batch_size: int = 50000000,\n    path: Union[str, pathlib.Path] = None,\n    overwrite: bool = True,\n    reuse: bool = False,\n):\n\"\"\"\n    Create a motifscan object from a set of pwms and a set of regions\n\n    Parameters:\n        pwms:\n            A dictionary of pwms, where the keys are the motif ids and the values are the pwms\n        regions:\n            A regions object\n        fasta_file:\n            The location of the fasta file containing the genome\n        region_onehots:\n            A dictionary containing the onehot encoding of each region. If not given, the onehot encoding will be extracted from the fasta file\n        motifs:\n            A dataframe containing auxilliary information for each motif\n        cutoffs:\n            A dictionary containing the cutoffs for each motif.\n        cutoff_col:\n            The column in the motifs dataframe containing the cutoffs\n        device:\n            The device to use for the scanning\n        batch_size:\n            The batch size to use for scanning. Decrease this if the GPU runs out of memory\n        path:\n            The folder where the motifscan data will be stored.\n        overwrite:\n            Whether to overwrite existing motifscan data\n        reuse:\n            Whether to reuse existing motifscan data\n    \"\"\"\n\n    if device is None:\n        device = get_default_device()\n\n    self = cls(path)\n\n    if ((reuse) or (not overwrite)) and self.o.coordinates.exists(self):\n        if not reuse:\n            import warnings\n\n            warnings.warn(\n                \"Motifscan already exists. Use overwrite=True to overwrite, reuse=True to ignore this warning.\"\n            )\n        return self\n\n    if overwrite:\n        self.reset()\n\n    self.motifs = motifs\n    self.regions = regions\n\n    # check or create cutoffs\n    if cutoffs is None:\n        if cutoff_col is None:\n            raise ValueError(\"Either motifs+cutoff_col or cutoffs need to be specified.\")\n        if motifs is None:\n            raise ValueError(\"Either motifs+cutoff_col or cutoffs need to be specified. motifs is not given\")\n\n        cutoffs = motifs[cutoff_col].to_dict()\n    else:\n        if isinstance(cutoffs, (float, int)):\n            cutoffs = {motif: cutoffs for motif in pwms.keys()}\n        elif isinstance(cutoffs, pd.Series):\n            cutoffs = cutoffs.to_dict()\n        else:\n            raise ValueError(\"cutoffs should be a float, int, dict or pd.Series\")\n        assert set(cutoffs.keys()) == set(pwms.keys())\n\n    # check or create motifs\n    if motifs is None:\n        motifs = pd.DataFrame(\n            {\n                \"motif\": list(pwms.keys()),\n            }\n        ).set_index(\"motif\")\n\n    # divide regions into batches according to batch size\n    region_coordinates = regions.coordinates\n\n    region_coordinates = divide_regions_in_batches(region_coordinates, batch_size=batch_size)\n\n    # load in fasta file\n    if fasta_file is not None:\n        import pysam\n\n        fasta = pysam.FastaFile(fasta_file)\n    else:\n        fasta = None\n        if region_onehots is None:\n            raise ValueError(\"Either fasta_file or region_onehots need to be specified\")\n\n    self.indices.open_creator()\n    self.scores.open_creator()\n    self.strands.open_creator()\n    self.coordinates.open_creator()\n    self.region_indices.open_creator()\n\n    # do the actual counting by looping over the batches, extract the sequences and scanning\n    progress = tqdm.tqdm(region_coordinates.groupby(\"batch\"))\n    for batch, region_coordinates_batch in progress:\n        # extract onehot\n        if fasta is None:\n            sequences = [\n                fasta.fetch(chrom, start, end + 1)\n                for chrom, start, end in region_coordinates_batch[[\"chrom\", \"start\", \"end\"]].values\n            ]\n            if not all(len(sequence) == len(sequences[0]) for sequence in sequences):\n                raise ValueError(\"All regions/sequences should have the same length\")\n            onehot = create_onehots(sequences).permute(0, 2, 1)\n        else:\n            if region_onehots is None:\n                if fasta_file is None:\n                    raise ValueError(\"fasta_file must be provided if fasta and region_onehots is not provided\")\n                progress.set_description(\"Extracting sequences\")\n                region_onehots = create_region_onehots(regions, fasta_file)\n            onehot = torch.stack([region_onehots[region] for region in region_coordinates_batch.index]).permute(\n                0, 2, 1\n            )\n        onehot = onehot.to(device)\n\n        progress.set_description(\n            f\"Scanning batch {batch} {region_coordinates_batch.index[0]}-{region_coordinates_batch.index[-1]}\"\n        )\n\n        assert onehot.shape[1] == 4\n        assert onehot.shape[2] == region_coordinates_batch[\"len\"].iloc[0], (\n            onehot.shape[2],\n            region_coordinates_batch[\"len\"].iloc[0],\n        )\n\n        scores_raw = []\n        indices_raw = []\n        coordinates_raw = []\n        strands_raw = []\n        region_indices_raw = []\n        for motif_ix, motif in tqdm.tqdm(enumerate(motifs.index)):\n            cutoff = cutoffs[motif]\n\n            if cutoff &lt; min_cutoff:\n                cutoff = min_cutoff\n\n            # get pwm\n            pwm = pwms[motif]\n            if not torch.is_tensor(pwm):\n                pwm = torch.from_numpy(pwm)\n            pwm2 = pwm.to(dtype=torch.float32, device=onehot.device).transpose(1, 0)\n\n            (\n                scores,\n                positions,\n                strands,\n            ) = scan(onehot, pwm2, cutoff=cutoff)\n\n            coordinates = positions.astype(np.int32) % onehot.shape[1]\n\n            region_indices = positions // onehot.shape[1]\n\n            coordinates = (\n                coordinates\n                + (self.regions.coordinates[\"start\"] - self.regions.coordinates[\"tss\"]).values[region_indices]\n            )\n\n            coordinates_raw.append(coordinates)\n            indices_raw.append(np.full_like(coordinates, motif_ix, dtype=np.int32))\n            strands_raw.append(strands)\n            scores_raw.append(scores)\n            region_indices_raw.append(region_indices)\n\n        # concatenate raw values (sorted by motif)\n        coordinates = np.concatenate(coordinates_raw)\n        indices = np.concatenate(indices_raw)\n        strands = np.concatenate(strands_raw)\n        scores = np.concatenate(scores_raw)\n        region_indices = np.concatenate(region_indices_raw)\n\n        # sort according to position\n        sorted_idx = np.lexsort([coordinates, region_indices])\n        indices = indices[sorted_idx]\n        scores = scores[sorted_idx]\n        strands = strands[sorted_idx]\n        coordinates = coordinates[sorted_idx]\n        region_indices = region_indices[sorted_idx]\n\n        # store batch\n        self.indices.extend(indices)\n        self.scores.extend(scores)\n        self.strands.extend(strands)\n        self.coordinates.extend(coordinates)\n        self.region_indices.extend(region_indices)\n\n    return self\n</code></pre>"},{"location":"reference/data/motifscan/#chromatinhd.data.motifscan.motifscan.Motifscan.get_slice","title":"<code>get_slice(region_ix=None, region_id=None, start=None, end=None, return_indptr=False, return_scores=True, return_strands=True, motif_ixs=None)</code>","text":"<p>Get a slice of the motifscan</p> <p>Parameters:</p> Name Type Description Default <code>region</code> <p>Region id</p> required <code>start</code> <p>Start of the slice, in region coordinates</p> <code>None</code> <code>end</code> <p>End of the slice, in region coordinates</p> <code>None</code> <p>Returns:</p> Type Description <p>Motifs positions, indices, scores and strands of the slice</p> Source code in <code>src/chromatinhd/data/motifscan/motifscan.py</code> <pre><code>def get_slice(\n    self,\n    region_ix=None,\n    region_id=None,\n    start=None,\n    end=None,\n    return_indptr=False,\n    return_scores=True,\n    return_strands=True,\n    motif_ixs=None,\n):\n\"\"\"\n    Get a slice of the motifscan\n\n    Parameters:\n        region:\n            Region id\n        start:\n            Start of the slice, in region coordinates\n        end:\n            End of the slice, in region coordinates\n\n    Returns:\n        Motifs positions, indices, scores and strands of the slice\n    \"\"\"\n    if region_id is not None:\n        region = self.regions.coordinates.loc[region_id]\n    elif region_ix is not None:\n        region = self.regions.coordinates.iloc[region_ix]\n    else:\n        raise ValueError(\"Either region or region_ix should be provided\")\n    if region_ix is None:\n        region_ix = self.regions.coordinates.index.get_indexer([region_id])[0]\n\n    if self.regions.width is None:\n        raise NotImplementedError(\"get_slice is only implemented for fixed width regions\")\n    width = self.regions.width\n\n    if start is None:\n        start = self.regions.window[0]\n    if end is None:\n        end = self.regions.window[1]\n\n    if self.o.indptr.exists(self):\n        start = region_ix * width + start\n        end = region_ix * width + end\n        indptr = self.indptr[start : end + 1]\n        indptr_start, indptr_end = indptr[0], indptr[-1]\n        indptr = indptr - indptr[0]\n    else:\n        region_start = self.region_indptr[region_ix]\n        region_end = self.region_indptr[region_ix + 1]\n        coordinates = self.coordinates[region_start:region_end]\n        indptr_start = coordinates.searchsorted(start - 1) + region_start\n        indptr_end = coordinates.searchsorted(end) + region_start\n\n    coordinates = self.coordinates[indptr_start:indptr_end]\n    indices = self.indices[indptr_start:indptr_end]\n\n    out = [coordinates, indices]\n    if return_scores:\n        out.append(self.scores[indptr_start:indptr_end])\n    if return_strands:\n        out.append(self.strands[indptr_start:indptr_end])\n\n    if motif_ixs is not None:\n        selection = np.isin(indices, motif_ixs)\n        out = [x[selection] for x in out]\n\n    if return_indptr:\n        out.append(indptr)\n\n    return out\n</code></pre>"},{"location":"reference/data/regions/","title":"Regions","text":""},{"location":"reference/data/regions/#chromatinhd.data.Regions","title":"<code>chromatinhd.data.Regions</code>","text":"<p>         Bases: <code>Flow</code></p> <p>Regions in the genome</p> Source code in <code>src/chromatinhd/data/regions.py</code> <pre><code>class Regions(Flow):\n\"\"\"\n    Regions in the genome\n    \"\"\"\n\n    coordinates = TSV(columns=[\"chrom\", \"start\", \"end\"])\n    \"Coordinates dataframe of the regions, with columns chrom, start, end\"\n\n    window = Stored()\n\n    @classmethod\n    def from_transcripts(\n        cls,\n        transcripts: pd.DataFrame,\n        window: [list, np.ndarray],\n        path: PathLike = None,\n        max_n_regions: Optional[int] = None,\n        overwrite=True,\n    ) -&gt; Regions:\n\"\"\"\n        Create regions from a dataframe of transcripts,\n        using a specified window around each transcription start site.\n\n        Parameters:\n            transcripts:\n                Dataframe of transcripts, with columns chrom, start, end, strand, ensembl_transcript_id\n            window:\n                Window around each transcription start site. Should be a 2-element array, e.g. [-10000, 10000]\n            path:\n                Folder in which the regions data will be stored\n            max_n_regions:\n                Maximum number of region to use. If None, all regions are used.\n        Returns:\n            Regions\n        \"\"\"\n        transcripts[\"tss\"] = transcripts[\"start\"] * (transcripts[\"strand\"] == 1) + transcripts[\"end\"] * (\n            transcripts[\"strand\"] == -1\n        )\n\n        regions = transcripts[[\"chrom\", \"tss\", \"ensembl_transcript_id\"]].copy()\n\n        regions[\"strand\"] = transcripts[\"strand\"]\n        regions[\"positive_strand\"] = (regions[\"strand\"] == 1).astype(int)\n        regions[\"negative_strand\"] = (regions[\"strand\"] == -1).astype(int)\n        regions[\"chrom\"] = transcripts.loc[regions.index, \"chrom\"]\n\n        regions[\"start\"] = (\n            regions[\"tss\"] + window[0] * (regions[\"strand\"] == 1) - window[1] * (regions[\"strand\"] == -1)\n        ).astype(int)\n        regions[\"end\"] = (\n            regions[\"tss\"] + window[1] * (regions[\"strand\"] == -1) - window[0] * (regions[\"strand\"] == 1)\n        ).astype(int)\n\n        if max_n_regions is not None:\n            regions = regions.iloc[:max_n_regions]\n\n        return cls.create(\n            path=path,\n            coordinates=regions[[\"chrom\", \"start\", \"end\", \"tss\", \"strand\", \"ensembl_transcript_id\"]],\n            window=window,\n            reset=overwrite,\n        )\n\n    def filter(self, region_ids: List[str], path: PathLike = None, overwrite=True) -&gt; Regions:\n\"\"\"\n        Select a subset of regions\n\n        Parameters:\n            region_ids:\n                Genes to filter. Should be a pandas Series with the index being the ensembl transcript ids.\n            path:\n                Path to store the filtered regions\n        Returns:\n            Regions with only the specified region_ids\n        \"\"\"\n\n        return Regions.create(\n            coordinates=self.coordinates.loc[region_ids], window=self.window, path=path, reset=overwrite\n        )\n\n    @property\n    def window_width(self):\n        if self.window is None:\n            return None\n        return self.window[1] - self.window[0]\n\n    region_width = window_width\n    width = window_width\n    \"Width of the regions, None if regions do not have a fixed width\"\n\n    @classmethod\n    def from_chromosomes_file(\n        cls, chromosomes_file: PathLike, path: PathLike = None, filter_chromosomes=True, overwrite: bool = True\n    ) -&gt; Regions:\n\"\"\"\n        Create regions based on a chromosomes file, e.g. hg38.chrom.sizes\n\n        Parameters:\n            chromosomes_file:\n                Path to chromosomes file, tab separated, with columns chrom, size\n            path:\n                Folder in which the regions data will be stored\n        Returns:\n            Regions\n        \"\"\"\n\n        chromosomes = pd.read_csv(chromosomes_file, sep=\"\\t\", names=[\"chrom\", \"size\"])\n        chromosomes[\"start\"] = 0\n        chromosomes[\"end\"] = chromosomes[\"size\"]\n        chromosomes[\"strand\"] = 1\n        chromosomes = chromosomes[[\"chrom\", \"start\", \"end\", \"strand\"]]\n        chromosomes = chromosomes.set_index(\"chrom\", drop=False)\n        chromosomes.index.name = \"region\"\n\n        if filter_chromosomes:\n            chromosomes = chromosomes.loc[~chromosomes[\"chrom\"].isin([\"chrM\", \"chrMT\"])]\n            chromosomes = chromosomes.loc[~chromosomes[\"chrom\"].str.contains(\"_\")]\n            chromosomes = chromosomes.loc[~chromosomes[\"chrom\"].str.contains(\"\\.\")]\n\n        chromosomes = chromosomes.sort_values(\"chrom\")\n\n        return cls.create(\n            path=path,\n            coordinates=chromosomes,\n            window=None,\n            reset=overwrite,\n        )\n\n    @functools.cached_property\n    def n_regions(self):\n        return self.coordinates.shape[0]\n\n    @functools.cached_property\n    def region_lengths(self):\n        return (self.coordinates[\"end\"] - self.coordinates[\"start\"]).values\n\n    @functools.cached_property\n    def region_starts(self):\n        return self.coordinates[\"start\"].values\n\n    @functools.cached_property\n    def cumulative_region_lengths(self):\n        return np.pad(np.cumsum(self.coordinates[\"end\"].values - self.coordinates[\"start\"].values), (1, 0))\n\n    @property\n    def var(self):\n        return self.coordinates\n\n    @var.setter\n    def var(self, value):\n        self.coordinates = value\n\n    def __len__(self):\n        return self.n_regions\n</code></pre>"},{"location":"reference/data/regions/#chromatinhd.data.regions.Regions.coordinates","title":"<code>coordinates = TSV(columns=['chrom', 'start', 'end'])</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Coordinates dataframe of the regions, with columns chrom, start, end</p>"},{"location":"reference/data/regions/#chromatinhd.data.regions.Regions.width","title":"<code>width = window_width</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Width of the regions, None if regions do not have a fixed width</p>"},{"location":"reference/data/regions/#chromatinhd.data.regions.Regions.filter","title":"<code>filter(region_ids, path=None, overwrite=True)</code>","text":"<p>Select a subset of regions</p> <p>Parameters:</p> Name Type Description Default <code>region_ids</code> <code>List[str]</code> <p>Genes to filter. Should be a pandas Series with the index being the ensembl transcript ids.</p> required <code>path</code> <code>PathLike</code> <p>Path to store the filtered regions</p> <code>None</code> <p>Returns:</p> Type Description <code>Regions</code> <p>Regions with only the specified region_ids</p> Source code in <code>src/chromatinhd/data/regions.py</code> <pre><code>def filter(self, region_ids: List[str], path: PathLike = None, overwrite=True) -&gt; Regions:\n\"\"\"\n    Select a subset of regions\n\n    Parameters:\n        region_ids:\n            Genes to filter. Should be a pandas Series with the index being the ensembl transcript ids.\n        path:\n            Path to store the filtered regions\n    Returns:\n        Regions with only the specified region_ids\n    \"\"\"\n\n    return Regions.create(\n        coordinates=self.coordinates.loc[region_ids], window=self.window, path=path, reset=overwrite\n    )\n</code></pre>"},{"location":"reference/data/regions/#chromatinhd.data.regions.Regions.from_chromosomes_file","title":"<code>from_chromosomes_file(chromosomes_file, path=None, filter_chromosomes=True, overwrite=True)</code>  <code>classmethod</code>","text":"<p>Create regions based on a chromosomes file, e.g. hg38.chrom.sizes</p> <p>Parameters:</p> Name Type Description Default <code>chromosomes_file</code> <code>PathLike</code> <p>Path to chromosomes file, tab separated, with columns chrom, size</p> required <code>path</code> <code>PathLike</code> <p>Folder in which the regions data will be stored</p> <code>None</code> <p>Returns:</p> Type Description <code>Regions</code> <p>Regions</p> Source code in <code>src/chromatinhd/data/regions.py</code> <pre><code>@classmethod\ndef from_chromosomes_file(\n    cls, chromosomes_file: PathLike, path: PathLike = None, filter_chromosomes=True, overwrite: bool = True\n) -&gt; Regions:\n\"\"\"\n    Create regions based on a chromosomes file, e.g. hg38.chrom.sizes\n\n    Parameters:\n        chromosomes_file:\n            Path to chromosomes file, tab separated, with columns chrom, size\n        path:\n            Folder in which the regions data will be stored\n    Returns:\n        Regions\n    \"\"\"\n\n    chromosomes = pd.read_csv(chromosomes_file, sep=\"\\t\", names=[\"chrom\", \"size\"])\n    chromosomes[\"start\"] = 0\n    chromosomes[\"end\"] = chromosomes[\"size\"]\n    chromosomes[\"strand\"] = 1\n    chromosomes = chromosomes[[\"chrom\", \"start\", \"end\", \"strand\"]]\n    chromosomes = chromosomes.set_index(\"chrom\", drop=False)\n    chromosomes.index.name = \"region\"\n\n    if filter_chromosomes:\n        chromosomes = chromosomes.loc[~chromosomes[\"chrom\"].isin([\"chrM\", \"chrMT\"])]\n        chromosomes = chromosomes.loc[~chromosomes[\"chrom\"].str.contains(\"_\")]\n        chromosomes = chromosomes.loc[~chromosomes[\"chrom\"].str.contains(\"\\.\")]\n\n    chromosomes = chromosomes.sort_values(\"chrom\")\n\n    return cls.create(\n        path=path,\n        coordinates=chromosomes,\n        window=None,\n        reset=overwrite,\n    )\n</code></pre>"},{"location":"reference/data/regions/#chromatinhd.data.regions.Regions.from_transcripts","title":"<code>from_transcripts(transcripts, window, path=None, max_n_regions=None, overwrite=True)</code>  <code>classmethod</code>","text":"<p>Create regions from a dataframe of transcripts, using a specified window around each transcription start site.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts</code> <code>pd.DataFrame</code> <p>Dataframe of transcripts, with columns chrom, start, end, strand, ensembl_transcript_id</p> required <code>window</code> <code>[list, np.ndarray]</code> <p>Window around each transcription start site. Should be a 2-element array, e.g. [-10000, 10000]</p> required <code>path</code> <code>PathLike</code> <p>Folder in which the regions data will be stored</p> <code>None</code> <code>max_n_regions</code> <code>Optional[int]</code> <p>Maximum number of region to use. If None, all regions are used.</p> <code>None</code> <p>Returns:</p> Type Description <code>Regions</code> <p>Regions</p> Source code in <code>src/chromatinhd/data/regions.py</code> <pre><code>@classmethod\ndef from_transcripts(\n    cls,\n    transcripts: pd.DataFrame,\n    window: [list, np.ndarray],\n    path: PathLike = None,\n    max_n_regions: Optional[int] = None,\n    overwrite=True,\n) -&gt; Regions:\n\"\"\"\n    Create regions from a dataframe of transcripts,\n    using a specified window around each transcription start site.\n\n    Parameters:\n        transcripts:\n            Dataframe of transcripts, with columns chrom, start, end, strand, ensembl_transcript_id\n        window:\n            Window around each transcription start site. Should be a 2-element array, e.g. [-10000, 10000]\n        path:\n            Folder in which the regions data will be stored\n        max_n_regions:\n            Maximum number of region to use. If None, all regions are used.\n    Returns:\n        Regions\n    \"\"\"\n    transcripts[\"tss\"] = transcripts[\"start\"] * (transcripts[\"strand\"] == 1) + transcripts[\"end\"] * (\n        transcripts[\"strand\"] == -1\n    )\n\n    regions = transcripts[[\"chrom\", \"tss\", \"ensembl_transcript_id\"]].copy()\n\n    regions[\"strand\"] = transcripts[\"strand\"]\n    regions[\"positive_strand\"] = (regions[\"strand\"] == 1).astype(int)\n    regions[\"negative_strand\"] = (regions[\"strand\"] == -1).astype(int)\n    regions[\"chrom\"] = transcripts.loc[regions.index, \"chrom\"]\n\n    regions[\"start\"] = (\n        regions[\"tss\"] + window[0] * (regions[\"strand\"] == 1) - window[1] * (regions[\"strand\"] == -1)\n    ).astype(int)\n    regions[\"end\"] = (\n        regions[\"tss\"] + window[1] * (regions[\"strand\"] == -1) - window[0] * (regions[\"strand\"] == 1)\n    ).astype(int)\n\n    if max_n_regions is not None:\n        regions = regions.iloc[:max_n_regions]\n\n    return cls.create(\n        path=path,\n        coordinates=regions[[\"chrom\", \"start\", \"end\", \"tss\", \"strand\", \"ensembl_transcript_id\"]],\n        window=window,\n        reset=overwrite,\n    )\n</code></pre>"},{"location":"reference/data/regions/#chromatinhd.data.regions.select_tss_from_fragments","title":"<code>chromatinhd.data.regions.select_tss_from_fragments(transcripts, fragments_file, window=(-100, 100))</code>","text":"<p>Select the TSS with the most fragments within a window of the TSS</p> <p>Parameters:</p> Name Type Description Default <code>transcripts</code> <code>pd.DataFrame</code> <p>Dataframe of transcripts, with columns chrom, tss, ensembl_gene_id.</p> required <code>fragments_file</code> <code>PathLike</code> <p>Path to fragments file</p> required <code>window</code> <code>[np.ndarray, tuple]</code> <p>Window around the TSS to count fragments</p> <code>(-100, 100)</code> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>Dataframe of transcripts, with columns chrom, tss and n_fragments, with index being the gene id</p> Source code in <code>src/chromatinhd/data/regions.py</code> <pre><code>def select_tss_from_fragments(\n    transcripts: pd.DataFrame, fragments_file: PathLike, window: [np.ndarray, tuple] = (-100, 100)\n) -&gt; pd.DataFrame:\n\"\"\"\n    Select the TSS with the most fragments within a window of the TSS\n\n    Parameters:\n        transcripts:\n            Dataframe of transcripts, with columns chrom, tss, ensembl_gene_id.\n        fragments_file:\n            Path to fragments file\n        window:\n            Window around the TSS to count fragments\n    Returns:\n        Dataframe of transcripts, with columns chrom, tss and n_fragments, with index being the gene id\n    \"\"\"\n    if not ([col in transcripts.columns for col in [\"chrom\", \"tss\", \"ensembl_gene_id\"]]):\n        raise ValueError(\"Transcripts should have columns chrom, tss, ensembl_gene_id. \")\n\n    import pysam\n\n    fragments_tabix = pysam.TabixFile(str(fragments_file))\n\n    nfrags = []\n    for chrom, tss in tqdm.tqdm(zip(transcripts[\"chrom\"], transcripts[\"tss\"]), total=transcripts.shape[0]):\n        frags = list(fragments_tabix.fetch(chrom, tss + window[0], tss + window[1]))\n        nfrags.append(len(frags))\n    transcripts[\"n_fragments\"] = nfrags\n    selected_transcripts = (\n        transcripts.reset_index().sort_values(\"n_fragments\", ascending=False).groupby(\"ensembl_gene_id\").first()\n    )\n    selected_transcripts.index.name = \"gene\"\n    return selected_transcripts\n</code></pre>"},{"location":"reference/data/transcriptome/","title":"Transcriptome","text":""},{"location":"reference/data/transcriptome/#chromatinhd.data.Transcriptome","title":"<code>chromatinhd.data.Transcriptome</code>","text":"<p>         Bases: <code>Flow</code></p> <p>A transcriptome containing counts for each gene in each cell.</p> Source code in <code>src/chromatinhd/data/transcriptome/transcriptome.py</code> <pre><code>class Transcriptome(Flow):\n\"\"\"\n    A transcriptome containing counts for each gene in each cell.\n    \"\"\"\n\n    var: pd.DataFrame = TSV(index_name=\"gene\")\n    obs: pd.DataFrame = TSV(index_name=\"cell\")\n\n    adata = Stored()\n    \"Anndata object containing the transcriptome data.\"\n\n    def gene_id(self, symbol, column=\"symbol\", optional=False, found=False):\n\"\"\"\n        Get the gene id for a given gene symbol.\n        \"\"\"\n        if found:\n            gene_id = self.var.reset_index().groupby(column).first().reindex(symbol)[\"gene\"]\n            return gene_id\n        if optional:\n            symbol = pd.Series(symbol)[pd.Series(symbol).isin(self.var[column])]\n\n        assert all(pd.Series(symbol).isin(self.var[column])), set(\n            pd.Series(symbol)[~pd.Series(symbol).isin(self.var[column])]\n        )\n        return self.var.reset_index(\"gene\").set_index(column).loc[symbol][\"gene\"]\n\n    def symbol(self, gene_id, column=\"symbol\"):\n\"\"\"\n        Get the gene symbol for a given gene ID (e.g. Ensembl ID).\n        \"\"\"\n        assert all(pd.Series(gene_id).isin(self.var.index)), set(\n            pd.Series(gene_id)[~pd.Series(gene_id).isin(self.var.index)]\n        )\n        return self.var.loc[gene_id][column]\n\n    def gene_ix(self, symbol):\n\"\"\"\n        Get the gene index for a given gene symbol.\n        \"\"\"\n        self.var[\"ix\"] = np.arange(self.var.shape[0])\n        assert all(pd.Series(symbol).isin(self.var[\"symbol\"])), set(\n            pd.Series(symbol)[~pd.Series(symbol).isin(self.var[\"symbol\"])]\n        )\n        return self.var.reset_index(\"gene\").set_index(\"symbol\").loc[symbol][\"ix\"]\n\n    @classmethod\n    def from_adata(\n        cls,\n        adata: sc.AnnData,\n        path: Union[pathlib.Path, str] = None,\n        overwrite=False,\n    ):\n\"\"\"\n        Create a Transcriptome object from an AnnData object.\n\n        Parameters:\n            adata:\n                Anndata object containing the transcriptome data.\n            path:\n                Folder in which the transcriptome data will be stored.\n            overwrite:\n                Whether to overwrite the data if it already exists.\n        \"\"\"\n\n        transcriptome = cls(path=path, reset=overwrite)\n        transcriptome.adata = adata\n\n        for k, v in adata.layers.items():\n            if sparse.is_scipysparse(v):\n                v = np.array(v.todense())\n            transcriptome.layers[k] = v.astype(\"&lt;f4\")\n        if sparse.is_scipysparse(adata.X):\n            v = np.array(adata.X.todense()).astype(\"&lt;f4\")\n        else:\n            v = adata.X.astype(\"&lt;f4\")\n        transcriptome.layers[\"X\"] = v\n        transcriptome.var = adata.var\n        transcriptome.obs = adata.obs\n        return transcriptome\n\n    @property\n    def X(self):\n        return self.layers[list(self.layers.keys())[0]]\n\n    @X.setter\n    def X(self, value):\n        self.layers[\"X\"] = value\n\n    layers = StoredDict(Tensorstore, kwargs=dict(dtype=\"&lt;f4\"))\n    \"Dictionary of layers, such as raw, normalized and imputed data.\"\n\n    def filter_genes(self, genes, path=None):\n\"\"\"\n        Filter genes\n\n        Parameters:\n            genes:\n                Genes to filter.\n        \"\"\"\n\n        self.var[\"ix\"] = np.arange(self.var.shape[0])\n        gene_ixs = self.var[\"ix\"].loc[genes]\n\n        layers = {}\n        for k, v in self.layers.items():\n            layers[k] = v[:, gene_ixs]\n        X = self.X[:, gene_ixs]\n\n        return Transcriptome.create(\n            var=self.var.loc[genes],\n            obs=self.obs,\n            X=X,\n            layers=layers,\n            path=path,\n        )\n\n    def filter_cells(self, cells, path=None):\n\"\"\"\n        Filter cells\n\n        Parameters:\n            cells:\n                Cells to filter.\n        \"\"\"\n\n        self.obs[\"ix\"] = np.arange(self.obs.shape[0])\n        cell_ixs = self.obs[\"ix\"].loc[cells]\n\n        layers = {}\n        for k, v in self.layers.items():\n            layers[k] = v[cell_ixs, :]\n        X = self.X[cell_ixs, :]\n\n        if self.o.adata.exists(self):\n            adata = self.adata[cell_ixs, :]\n        else:\n            adata = None\n\n        return Transcriptome.create(var=self.var, obs=self.obs.loc[cells], X=X, layers=layers, path=path, adata=adata)\n\n    def get_X(self, gene_ids, layer=None):\n\"\"\"\n        Get the counts for a given set of genes.\n        \"\"\"\n        gene_ixs = self.var.index.get_loc(gene_ids)\n\n        if layer is None:\n            value = self.X[:, gene_ixs]\n        else:\n            value = self.layers[layer][:, gene_ixs]\n\n        if sparse.is_scipysparse(value):\n            value = np.array(value.todense())\n            if isinstance(gene_ids, str):\n                value = value[:, 0]\n        return value\n</code></pre>"},{"location":"reference/data/transcriptome/#chromatinhd.data.transcriptome.transcriptome.Transcriptome.adata","title":"<code>adata = Stored()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Anndata object containing the transcriptome data.</p>"},{"location":"reference/data/transcriptome/#chromatinhd.data.transcriptome.transcriptome.Transcriptome.layers","title":"<code>layers = StoredDict(Tensorstore, kwargs=dict(dtype='&lt;f4'))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dictionary of layers, such as raw, normalized and imputed data.</p>"},{"location":"reference/data/transcriptome/#chromatinhd.data.transcriptome.transcriptome.Transcriptome.filter_cells","title":"<code>filter_cells(cells, path=None)</code>","text":"<p>Filter cells</p> <p>Parameters:</p> Name Type Description Default <code>cells</code> <p>Cells to filter.</p> required Source code in <code>src/chromatinhd/data/transcriptome/transcriptome.py</code> <pre><code>def filter_cells(self, cells, path=None):\n\"\"\"\n    Filter cells\n\n    Parameters:\n        cells:\n            Cells to filter.\n    \"\"\"\n\n    self.obs[\"ix\"] = np.arange(self.obs.shape[0])\n    cell_ixs = self.obs[\"ix\"].loc[cells]\n\n    layers = {}\n    for k, v in self.layers.items():\n        layers[k] = v[cell_ixs, :]\n    X = self.X[cell_ixs, :]\n\n    if self.o.adata.exists(self):\n        adata = self.adata[cell_ixs, :]\n    else:\n        adata = None\n\n    return Transcriptome.create(var=self.var, obs=self.obs.loc[cells], X=X, layers=layers, path=path, adata=adata)\n</code></pre>"},{"location":"reference/data/transcriptome/#chromatinhd.data.transcriptome.transcriptome.Transcriptome.filter_genes","title":"<code>filter_genes(genes, path=None)</code>","text":"<p>Filter genes</p> <p>Parameters:</p> Name Type Description Default <code>genes</code> <p>Genes to filter.</p> required Source code in <code>src/chromatinhd/data/transcriptome/transcriptome.py</code> <pre><code>def filter_genes(self, genes, path=None):\n\"\"\"\n    Filter genes\n\n    Parameters:\n        genes:\n            Genes to filter.\n    \"\"\"\n\n    self.var[\"ix\"] = np.arange(self.var.shape[0])\n    gene_ixs = self.var[\"ix\"].loc[genes]\n\n    layers = {}\n    for k, v in self.layers.items():\n        layers[k] = v[:, gene_ixs]\n    X = self.X[:, gene_ixs]\n\n    return Transcriptome.create(\n        var=self.var.loc[genes],\n        obs=self.obs,\n        X=X,\n        layers=layers,\n        path=path,\n    )\n</code></pre>"},{"location":"reference/data/transcriptome/#chromatinhd.data.transcriptome.transcriptome.Transcriptome.from_adata","title":"<code>from_adata(adata, path=None, overwrite=False)</code>  <code>classmethod</code>","text":"<p>Create a Transcriptome object from an AnnData object.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>sc.AnnData</code> <p>Anndata object containing the transcriptome data.</p> required <code>path</code> <code>Union[pathlib.Path, str]</code> <p>Folder in which the transcriptome data will be stored.</p> <code>None</code> <code>overwrite</code> <p>Whether to overwrite the data if it already exists.</p> <code>False</code> Source code in <code>src/chromatinhd/data/transcriptome/transcriptome.py</code> <pre><code>@classmethod\ndef from_adata(\n    cls,\n    adata: sc.AnnData,\n    path: Union[pathlib.Path, str] = None,\n    overwrite=False,\n):\n\"\"\"\n    Create a Transcriptome object from an AnnData object.\n\n    Parameters:\n        adata:\n            Anndata object containing the transcriptome data.\n        path:\n            Folder in which the transcriptome data will be stored.\n        overwrite:\n            Whether to overwrite the data if it already exists.\n    \"\"\"\n\n    transcriptome = cls(path=path, reset=overwrite)\n    transcriptome.adata = adata\n\n    for k, v in adata.layers.items():\n        if sparse.is_scipysparse(v):\n            v = np.array(v.todense())\n        transcriptome.layers[k] = v.astype(\"&lt;f4\")\n    if sparse.is_scipysparse(adata.X):\n        v = np.array(adata.X.todense()).astype(\"&lt;f4\")\n    else:\n        v = adata.X.astype(\"&lt;f4\")\n    transcriptome.layers[\"X\"] = v\n    transcriptome.var = adata.var\n    transcriptome.obs = adata.obs\n    return transcriptome\n</code></pre>"},{"location":"reference/data/transcriptome/#chromatinhd.data.transcriptome.transcriptome.Transcriptome.gene_id","title":"<code>gene_id(symbol, column='symbol', optional=False, found=False)</code>","text":"<p>Get the gene id for a given gene symbol.</p> Source code in <code>src/chromatinhd/data/transcriptome/transcriptome.py</code> <pre><code>def gene_id(self, symbol, column=\"symbol\", optional=False, found=False):\n\"\"\"\n    Get the gene id for a given gene symbol.\n    \"\"\"\n    if found:\n        gene_id = self.var.reset_index().groupby(column).first().reindex(symbol)[\"gene\"]\n        return gene_id\n    if optional:\n        symbol = pd.Series(symbol)[pd.Series(symbol).isin(self.var[column])]\n\n    assert all(pd.Series(symbol).isin(self.var[column])), set(\n        pd.Series(symbol)[~pd.Series(symbol).isin(self.var[column])]\n    )\n    return self.var.reset_index(\"gene\").set_index(column).loc[symbol][\"gene\"]\n</code></pre>"},{"location":"reference/data/transcriptome/#chromatinhd.data.transcriptome.transcriptome.Transcriptome.gene_ix","title":"<code>gene_ix(symbol)</code>","text":"<p>Get the gene index for a given gene symbol.</p> Source code in <code>src/chromatinhd/data/transcriptome/transcriptome.py</code> <pre><code>def gene_ix(self, symbol):\n\"\"\"\n    Get the gene index for a given gene symbol.\n    \"\"\"\n    self.var[\"ix\"] = np.arange(self.var.shape[0])\n    assert all(pd.Series(symbol).isin(self.var[\"symbol\"])), set(\n        pd.Series(symbol)[~pd.Series(symbol).isin(self.var[\"symbol\"])]\n    )\n    return self.var.reset_index(\"gene\").set_index(\"symbol\").loc[symbol][\"ix\"]\n</code></pre>"},{"location":"reference/data/transcriptome/#chromatinhd.data.transcriptome.transcriptome.Transcriptome.get_X","title":"<code>get_X(gene_ids, layer=None)</code>","text":"<p>Get the counts for a given set of genes.</p> Source code in <code>src/chromatinhd/data/transcriptome/transcriptome.py</code> <pre><code>def get_X(self, gene_ids, layer=None):\n\"\"\"\n    Get the counts for a given set of genes.\n    \"\"\"\n    gene_ixs = self.var.index.get_loc(gene_ids)\n\n    if layer is None:\n        value = self.X[:, gene_ixs]\n    else:\n        value = self.layers[layer][:, gene_ixs]\n\n    if sparse.is_scipysparse(value):\n        value = np.array(value.todense())\n        if isinstance(gene_ids, str):\n            value = value[:, 0]\n    return value\n</code></pre>"},{"location":"reference/data/transcriptome/#chromatinhd.data.transcriptome.transcriptome.Transcriptome.symbol","title":"<code>symbol(gene_id, column='symbol')</code>","text":"<p>Get the gene symbol for a given gene ID (e.g. Ensembl ID).</p> Source code in <code>src/chromatinhd/data/transcriptome/transcriptome.py</code> <pre><code>def symbol(self, gene_id, column=\"symbol\"):\n\"\"\"\n    Get the gene symbol for a given gene ID (e.g. Ensembl ID).\n    \"\"\"\n    assert all(pd.Series(gene_id).isin(self.var.index)), set(\n        pd.Series(gene_id)[~pd.Series(gene_id).isin(self.var.index)]\n    )\n    return self.var.loc[gene_id][column]\n</code></pre>"},{"location":"reference/loaders/fragments/","title":"Fragments","text":""},{"location":"reference/loaders/fragments/#chromatinhd.loaders.fragments.Fragments","title":"<code>chromatinhd.loaders.fragments.Fragments</code>","text":"<p>Basic loader for fragments. This requires either <code>regionxcell_indptr</code> (for a Fragments) or <code>regionxcell_fragmentixs_indptr</code> (for a FragmentsView) to be present.</p> Example <pre><code>loader = Fragments(fragments, cellxregion_batch_size=1000)\nminibatch = Minibatch(cells_oi=np.arange(100), regions_oi=np.arange(100))\ndata = loader.load(minibatch)\ndata.coordinates\n</code></pre> Source code in <code>src/chromatinhd/loaders/fragments.py</code> <pre><code>class Fragments:\n\"\"\"\n    Basic loader for fragments. This requires either `regionxcell_indptr` (for a Fragments) or `regionxcell_fragmentixs_indptr` (for a FragmentsView) to be present.\n\n    Example:\n        ```\n        loader = Fragments(fragments, cellxregion_batch_size=1000)\n        minibatch = Minibatch(cells_oi=np.arange(100), regions_oi=np.arange(100))\n        data = loader.load(minibatch)\n        data.coordinates\n        ```\n    \"\"\"\n\n    cellxregion_batch_size: int\n\n    preloaded = False\n\n    out_coordinates: torch.Tensor\n    out_regionmapping: torch.Tensor\n    out_local_cellxregion_ix: torch.Tensor\n\n    n_regions: int\n    is_view: bool\n\n    def __init__(\n        self,\n        fragments: Union[chromatinhd.data.fragments.Fragments, chromatinhd.data.fragments.FragmentsView],\n        cellxregion_batch_size: int,\n        n_fragment_per_cellxregion: int = None,\n        buffer_size_multiplier=10,  # increase this if crashing\n        provide_libsize=False,\n        provide_multiplets=True,\n    ):\n\"\"\"\n        Parameters:\n            fragments: Fragments object\n            cellxregion_batch_size: maximum number of cell x region combinations that will be loaded\n            n_fragment_per_cellxregion: estimated number of the number of fragments per cell x region combination, used for preallocation\n        \"\"\"\n        self.cellxregion_batch_size = cellxregion_batch_size\n\n        # store auxilliary information\n        window = fragments.regions.window\n        self.window = window\n\n        # create buffers for coordinates\n        if n_fragment_per_cellxregion is None:\n            n_fragment_per_cellxregion = fragments.estimate_fragment_per_cellxregion()\n        fragment_buffer_size = n_fragment_per_cellxregion * cellxregion_batch_size * buffer_size_multiplier\n        self.fragment_buffer_size = fragment_buffer_size\n\n        self.n_regions = fragments.n_regions\n\n        # set up readers and determine if we are dealing with a view or not\n        if isinstance(fragments, chromatinhd.data.fragments.Fragments):\n            self.regionxcell_indptr_reader = fragments.regionxcell_indptr.open_reader(\n                {\"context\": {\"data_copy_concurrency\": {\"limit\": 1}}}\n            )\n            self.coordinates_reader = fragments.coordinates.open_reader(\n                {\n                    \"context\": {\n                        \"data_copy_concurrency\": {\"limit\": 1},\n                    }\n                }\n            )\n            self.is_view = False\n\n        elif isinstance(fragments, chromatinhd.data.fragments.view.FragmentsView):\n            self.regionxcell_indptr_reader = fragments.regionxcell_indptr.open_reader()\n            self.coordinates_reader = fragments.coordinates.open_reader()\n\n            if \"strand\" in fragments.regions.coordinates.columns:\n                self.region_strands = fragments.regions.coordinates[\"strand\"].values\n            else:\n                self.region_strands = np.ones((len(fragments.regions.coordinates),), dtype=np.int8)\n            self.region_centers = (fragments.regions.coordinates[\"start\"] - fragments.regions.window[0]).values * (\n                self.region_strands == 1\n            ).astype(int) + (fragments.regions.coordinates[\"end\"] + fragments.regions.window[0]).values * (\n                self.region_strands == -1\n            ).astype(\n                int\n            )\n            self.is_view = True\n        else:\n            raise ValueError(\"fragments must be either a Fragments or FragmentsView object\", type(fragments))\n\n        self.n_cells = fragments.n_cells\n\n        self.provide_multiplets = provide_multiplets\n        self.provide_libsize = provide_libsize\n\n        if provide_libsize:\n            library_size = fragments.libsize\n            self.library_size = torch.from_numpy((library_size - library_size.mean()) / library_size.std()).float()\n\n    def preload(self):\n        self.out_fragmentixs = np.zeros((self.fragment_buffer_size,), dtype=np.int64)\n        self.out_local_cellxregion_ix = np.zeros((self.fragment_buffer_size,), dtype=np.int64)\n\n        self.preloaded = True\n\n    def load(self, minibatch: Minibatch) -&gt; FragmentsResult:\n\"\"\"\n        Load a minibatch of fragments.\n\n        Parameters:\n            minibatch: Minibatch object\n\n        Returns:\n            The loaded fragments\n        \"\"\"\n        if not self.preloaded:\n            self.preload()\n\n        if (len(minibatch.cells_oi) * len(minibatch.regions_oi)) &gt; self.cellxregion_batch_size:\n            raise ValueError(\n                \"Too many cell x region requested, increase cellxregion_batch_size at loader initialization\"\n            )\n\n        if self.is_view:\n            # load the fragment indices using pointers to the regionxcell fragment indices\n            regionxcell_ixs = (minibatch.regions_oi * self.n_cells + minibatch.cells_oi[:, None]).flatten()\n            n_fragments = fragments_helpers.multiple_arange(\n                np.array(self.regionxcell_indptr_reader[regionxcell_ixs, 0]),\n                np.array(self.regionxcell_indptr_reader[regionxcell_ixs, 1]),\n                self.out_fragmentixs,\n                self.out_local_cellxregion_ix,\n            )\n\n            assert n_fragments &lt; self.fragment_buffer_size, \"fragment buffer size too small\"\n\n            regionxcell_fragmentixs = self.out_fragmentixs[:n_fragments]\n            local_cellxregion_ix = self.out_local_cellxregion_ix[:n_fragments]\n\n            regionmapping = minibatch.regions_oi[local_cellxregion_ix % minibatch.n_regions]\n            coordinates = self.coordinates_reader[regionxcell_fragmentixs]  # this is typically the slowest part by far\n\n            # center coordinates around region centers, flip based on strandedness\n            coordinates = (coordinates - self.region_centers[regionmapping][:, None]) * self.region_strands[\n                regionmapping\n            ][:, None]\n        else:\n            regionxcell_ixs = (minibatch.regions_oi * self.n_cells + minibatch.cells_oi[:, None]).flatten()\n            n_fragments = fragments_helpers.multiple_arange(\n                self.regionxcell_indptr_reader[regionxcell_ixs],\n                self.regionxcell_indptr_reader[regionxcell_ixs + 1],\n                self.out_fragmentixs,\n                self.out_local_cellxregion_ix,\n            )\n            regionxcell_fragmentixs = np.resize(self.out_fragmentixs, n_fragments)\n            coordinates = self.coordinates_reader[regionxcell_fragmentixs]  # this is typically the slowest part by far\n            local_cellxregion_ix = np.resize(self.out_local_cellxregion_ix, n_fragments)\n            regionmapping = minibatch.regions_oi[local_cellxregion_ix % minibatch.n_regions]\n\n        # multiplets\n        if self.provide_multiplets:\n            from chromatinhd.utils.numpy import indices_to_indptr\n\n            indptr = indices_to_indptr(local_cellxregion_ix, minibatch.n_cells * minibatch.n_regions)\n            indptr_diff = np.diff(indptr)\n\n            doublets = np.where(indptr_diff == 2)[0]\n            doublet_idx = torch.from_numpy(np.stack([indptr[doublets], indptr[doublets] + 1], -1).flatten())\n\n            triplets = np.where(indptr_diff == 2)[0]\n            triplet_idx = np.stack([indptr[triplets], indptr[triplets] + 1, indptr[triplets] + 2], -1).flatten()\n        else:\n            doublet_idx = None\n            triplet_idx = None\n\n        # libsize\n        if self.provide_libsize:\n            libsize = self.library_size[minibatch.cells_oi]\n        else:\n            libsize = None\n\n        return FragmentsResult(\n            coordinates=torch.from_numpy(coordinates),\n            local_cellxregion_ix=torch.from_numpy(local_cellxregion_ix),\n            n_fragments=n_fragments,\n            regionmapping=torch.from_numpy(regionmapping),\n            window=self.window,\n            n_total_regions=self.n_regions,\n            cells_oi=minibatch.cells_oi,\n            regions_oi=minibatch.regions_oi,\n            doublet_idx=doublet_idx,\n            libsize=libsize,\n        )\n</code></pre>"},{"location":"reference/loaders/fragments/#chromatinhd.loaders.fragments.Fragments.__init__","title":"<code>__init__(fragments, cellxregion_batch_size, n_fragment_per_cellxregion=None, buffer_size_multiplier=10, provide_libsize=False, provide_multiplets=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>fragments</code> <code>Union[chromatinhd.data.fragments.Fragments, chromatinhd.data.fragments.FragmentsView]</code> <p>Fragments object</p> required <code>cellxregion_batch_size</code> <code>int</code> <p>maximum number of cell x region combinations that will be loaded</p> required <code>n_fragment_per_cellxregion</code> <code>int</code> <p>estimated number of the number of fragments per cell x region combination, used for preallocation</p> <code>None</code> Source code in <code>src/chromatinhd/loaders/fragments.py</code> <pre><code>def __init__(\n    self,\n    fragments: Union[chromatinhd.data.fragments.Fragments, chromatinhd.data.fragments.FragmentsView],\n    cellxregion_batch_size: int,\n    n_fragment_per_cellxregion: int = None,\n    buffer_size_multiplier=10,  # increase this if crashing\n    provide_libsize=False,\n    provide_multiplets=True,\n):\n\"\"\"\n    Parameters:\n        fragments: Fragments object\n        cellxregion_batch_size: maximum number of cell x region combinations that will be loaded\n        n_fragment_per_cellxregion: estimated number of the number of fragments per cell x region combination, used for preallocation\n    \"\"\"\n    self.cellxregion_batch_size = cellxregion_batch_size\n\n    # store auxilliary information\n    window = fragments.regions.window\n    self.window = window\n\n    # create buffers for coordinates\n    if n_fragment_per_cellxregion is None:\n        n_fragment_per_cellxregion = fragments.estimate_fragment_per_cellxregion()\n    fragment_buffer_size = n_fragment_per_cellxregion * cellxregion_batch_size * buffer_size_multiplier\n    self.fragment_buffer_size = fragment_buffer_size\n\n    self.n_regions = fragments.n_regions\n\n    # set up readers and determine if we are dealing with a view or not\n    if isinstance(fragments, chromatinhd.data.fragments.Fragments):\n        self.regionxcell_indptr_reader = fragments.regionxcell_indptr.open_reader(\n            {\"context\": {\"data_copy_concurrency\": {\"limit\": 1}}}\n        )\n        self.coordinates_reader = fragments.coordinates.open_reader(\n            {\n                \"context\": {\n                    \"data_copy_concurrency\": {\"limit\": 1},\n                }\n            }\n        )\n        self.is_view = False\n\n    elif isinstance(fragments, chromatinhd.data.fragments.view.FragmentsView):\n        self.regionxcell_indptr_reader = fragments.regionxcell_indptr.open_reader()\n        self.coordinates_reader = fragments.coordinates.open_reader()\n\n        if \"strand\" in fragments.regions.coordinates.columns:\n            self.region_strands = fragments.regions.coordinates[\"strand\"].values\n        else:\n            self.region_strands = np.ones((len(fragments.regions.coordinates),), dtype=np.int8)\n        self.region_centers = (fragments.regions.coordinates[\"start\"] - fragments.regions.window[0]).values * (\n            self.region_strands == 1\n        ).astype(int) + (fragments.regions.coordinates[\"end\"] + fragments.regions.window[0]).values * (\n            self.region_strands == -1\n        ).astype(\n            int\n        )\n        self.is_view = True\n    else:\n        raise ValueError(\"fragments must be either a Fragments or FragmentsView object\", type(fragments))\n\n    self.n_cells = fragments.n_cells\n\n    self.provide_multiplets = provide_multiplets\n    self.provide_libsize = provide_libsize\n\n    if provide_libsize:\n        library_size = fragments.libsize\n        self.library_size = torch.from_numpy((library_size - library_size.mean()) / library_size.std()).float()\n</code></pre>"},{"location":"reference/loaders/fragments/#chromatinhd.loaders.fragments.Fragments.load","title":"<code>load(minibatch)</code>","text":"<p>Load a minibatch of fragments.</p> <p>Parameters:</p> Name Type Description Default <code>minibatch</code> <code>Minibatch</code> <p>Minibatch object</p> required <p>Returns:</p> Type Description <code>FragmentsResult</code> <p>The loaded fragments</p> Source code in <code>src/chromatinhd/loaders/fragments.py</code> <pre><code>def load(self, minibatch: Minibatch) -&gt; FragmentsResult:\n\"\"\"\n    Load a minibatch of fragments.\n\n    Parameters:\n        minibatch: Minibatch object\n\n    Returns:\n        The loaded fragments\n    \"\"\"\n    if not self.preloaded:\n        self.preload()\n\n    if (len(minibatch.cells_oi) * len(minibatch.regions_oi)) &gt; self.cellxregion_batch_size:\n        raise ValueError(\n            \"Too many cell x region requested, increase cellxregion_batch_size at loader initialization\"\n        )\n\n    if self.is_view:\n        # load the fragment indices using pointers to the regionxcell fragment indices\n        regionxcell_ixs = (minibatch.regions_oi * self.n_cells + minibatch.cells_oi[:, None]).flatten()\n        n_fragments = fragments_helpers.multiple_arange(\n            np.array(self.regionxcell_indptr_reader[regionxcell_ixs, 0]),\n            np.array(self.regionxcell_indptr_reader[regionxcell_ixs, 1]),\n            self.out_fragmentixs,\n            self.out_local_cellxregion_ix,\n        )\n\n        assert n_fragments &lt; self.fragment_buffer_size, \"fragment buffer size too small\"\n\n        regionxcell_fragmentixs = self.out_fragmentixs[:n_fragments]\n        local_cellxregion_ix = self.out_local_cellxregion_ix[:n_fragments]\n\n        regionmapping = minibatch.regions_oi[local_cellxregion_ix % minibatch.n_regions]\n        coordinates = self.coordinates_reader[regionxcell_fragmentixs]  # this is typically the slowest part by far\n\n        # center coordinates around region centers, flip based on strandedness\n        coordinates = (coordinates - self.region_centers[regionmapping][:, None]) * self.region_strands[\n            regionmapping\n        ][:, None]\n    else:\n        regionxcell_ixs = (minibatch.regions_oi * self.n_cells + minibatch.cells_oi[:, None]).flatten()\n        n_fragments = fragments_helpers.multiple_arange(\n            self.regionxcell_indptr_reader[regionxcell_ixs],\n            self.regionxcell_indptr_reader[regionxcell_ixs + 1],\n            self.out_fragmentixs,\n            self.out_local_cellxregion_ix,\n        )\n        regionxcell_fragmentixs = np.resize(self.out_fragmentixs, n_fragments)\n        coordinates = self.coordinates_reader[regionxcell_fragmentixs]  # this is typically the slowest part by far\n        local_cellxregion_ix = np.resize(self.out_local_cellxregion_ix, n_fragments)\n        regionmapping = minibatch.regions_oi[local_cellxregion_ix % minibatch.n_regions]\n\n    # multiplets\n    if self.provide_multiplets:\n        from chromatinhd.utils.numpy import indices_to_indptr\n\n        indptr = indices_to_indptr(local_cellxregion_ix, minibatch.n_cells * minibatch.n_regions)\n        indptr_diff = np.diff(indptr)\n\n        doublets = np.where(indptr_diff == 2)[0]\n        doublet_idx = torch.from_numpy(np.stack([indptr[doublets], indptr[doublets] + 1], -1).flatten())\n\n        triplets = np.where(indptr_diff == 2)[0]\n        triplet_idx = np.stack([indptr[triplets], indptr[triplets] + 1, indptr[triplets] + 2], -1).flatten()\n    else:\n        doublet_idx = None\n        triplet_idx = None\n\n    # libsize\n    if self.provide_libsize:\n        libsize = self.library_size[minibatch.cells_oi]\n    else:\n        libsize = None\n\n    return FragmentsResult(\n        coordinates=torch.from_numpy(coordinates),\n        local_cellxregion_ix=torch.from_numpy(local_cellxregion_ix),\n        n_fragments=n_fragments,\n        regionmapping=torch.from_numpy(regionmapping),\n        window=self.window,\n        n_total_regions=self.n_regions,\n        cells_oi=minibatch.cells_oi,\n        regions_oi=minibatch.regions_oi,\n        doublet_idx=doublet_idx,\n        libsize=libsize,\n    )\n</code></pre>"},{"location":"reference/loaders/fragments/#chromatinhd.loaders.fragments.Cuts","title":"<code>chromatinhd.loaders.fragments.Cuts</code>","text":"<p>         Bases: <code>Fragments</code></p> Source code in <code>src/chromatinhd/loaders/fragments.py</code> <pre><code>class Cuts(Fragments):\n    def load(self, minibatch: Minibatch) -&gt; CutsResult:\n\"\"\"\n        Load a minibatch of cuts.\n\n        Parameters:\n            minibatch: Minibatch object\n\n        Returns:\n            The loaded cut sites\n        \"\"\"\n        result = super().load(minibatch)\n\n        cut_coordinates = result.coordinates.flatten()\n\n        n_cuts_per_fragment = result.coordinates.shape[1]\n        local_cellxregion_ix = result.local_cellxregion_ix.expand(n_cuts_per_fragment, -1).T.flatten()\n\n        # selected = np.random.rand(len(cut_coordinates)) &lt; 0.2\n        # cut_coordinates = cut_coordinates[selected]\n        # local_cellxregion_ix = local_cellxregion_ix[selected]\n\n        return CutsResult(\n            coordinates=cut_coordinates,\n            local_cellxregion_ix=local_cellxregion_ix,\n            n_regions=len(minibatch.regions_oi),\n            n_fragments=result.n_fragments,\n            n_cuts=result.n_fragments * n_cuts_per_fragment,\n            window=self.window,\n        )\n</code></pre>"},{"location":"reference/loaders/fragments/#chromatinhd.loaders.fragments.Cuts.load","title":"<code>load(minibatch)</code>","text":"<p>Load a minibatch of cuts.</p> <p>Parameters:</p> Name Type Description Default <code>minibatch</code> <code>Minibatch</code> <p>Minibatch object</p> required <p>Returns:</p> Type Description <code>CutsResult</code> <p>The loaded cut sites</p> Source code in <code>src/chromatinhd/loaders/fragments.py</code> <pre><code>def load(self, minibatch: Minibatch) -&gt; CutsResult:\n\"\"\"\n    Load a minibatch of cuts.\n\n    Parameters:\n        minibatch: Minibatch object\n\n    Returns:\n        The loaded cut sites\n    \"\"\"\n    result = super().load(minibatch)\n\n    cut_coordinates = result.coordinates.flatten()\n\n    n_cuts_per_fragment = result.coordinates.shape[1]\n    local_cellxregion_ix = result.local_cellxregion_ix.expand(n_cuts_per_fragment, -1).T.flatten()\n\n    # selected = np.random.rand(len(cut_coordinates)) &lt; 0.2\n    # cut_coordinates = cut_coordinates[selected]\n    # local_cellxregion_ix = local_cellxregion_ix[selected]\n\n    return CutsResult(\n        coordinates=cut_coordinates,\n        local_cellxregion_ix=local_cellxregion_ix,\n        n_regions=len(minibatch.regions_oi),\n        n_fragments=result.n_fragments,\n        n_cuts=result.n_fragments * n_cuts_per_fragment,\n        window=self.window,\n    )\n</code></pre>"},{"location":"reference/loaders/fragments/#chromatinhd.loaders.fragments.FragmentsResult","title":"<code>chromatinhd.loaders.fragments.FragmentsResult</code>  <code>dataclass</code>","text":"Source code in <code>src/chromatinhd/loaders/fragments.py</code> <pre><code>@dataclasses.dataclass\nclass FragmentsResult:\n    coordinates: torch.Tensor\n    \"Coordinates of the left and right cut site for each fragment\"\n\n    local_cellxregion_ix: torch.Tensor\n    \"Local cell x region index\"\n\n    n_fragments: int\n    \"Number of fragments\"\n\n    regionmapping: torch.Tensor = None\n    \"Mapping from local cell x region index to region index\"\n\n    cells_oi: np.ndarray = None\n    \"Cells of interest\"\n\n    regions_oi: np.ndarray = None\n    \"Regions of interest\"\n\n    window: np.ndarray = None\n    \"Window of the region\"\n\n    n_total_regions: int = None\n    \"Total number of regions\"\n\n    localcellxregion_ix: torch.Tensor = None\n    \"Local cell x region index, in the same order as cells_oi and regions_oi\"\n\n    doublet_idx: torch.Tensor = None\n    \"Indices of doublets\"\n\n    libsize: torch.Tensor = None\n    \"Library size for each cell\"\n\n    @property\n    def n_cells(self):\n        return len(self.cells_oi)\n\n    @property\n    def n_regions(self):\n        return len(self.regions_oi)\n\n    def to(self, device):\n        self.coordinates = self.coordinates.to(device)\n        self.local_cellxregion_ix = self.local_cellxregion_ix.to(device)\n        if self.regionmapping is not None:\n            self.regionmapping = self.regionmapping.to(device)\n        if self.libsize is not None:\n            self.libsize = self.libsize.to(device)\n        return self\n\n    @property\n    def local_region_ix(self):\n        return self.local_cellxregion_ix % self.n_regions\n\n    @property\n    def local_cell_ix(self):\n        return torch.div(self.local_cellxregion_ix, self.n_regions, rounding_mode=\"floor\")\n\n    def filter_fragments(self, fragments_oi):\n        assert len(fragments_oi) == self.n_fragments\n        return FragmentsResult(\n            coordinates=self.coordinates[fragments_oi],\n            local_cellxregion_ix=self.local_cellxregion_ix[fragments_oi],\n            regionmapping=self.regionmapping[fragments_oi],\n            n_fragments=fragments_oi.sum(),\n            cells_oi=self.cells_oi,\n            regions_oi=self.regions_oi,\n            window=self.window,\n            n_total_regions=self.n_total_regions,\n        )\n</code></pre>"},{"location":"reference/loaders/fragments/#chromatinhd.loaders.fragments.FragmentsResult.cells_oi","title":"<code>cells_oi: np.ndarray = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Cells of interest</p>"},{"location":"reference/loaders/fragments/#chromatinhd.loaders.fragments.FragmentsResult.coordinates","title":"<code>coordinates: torch.Tensor</code>  <code>instance-attribute</code>","text":"<p>Coordinates of the left and right cut site for each fragment</p>"},{"location":"reference/loaders/fragments/#chromatinhd.loaders.fragments.FragmentsResult.doublet_idx","title":"<code>doublet_idx: torch.Tensor = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Indices of doublets</p>"},{"location":"reference/loaders/fragments/#chromatinhd.loaders.fragments.FragmentsResult.libsize","title":"<code>libsize: torch.Tensor = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Library size for each cell</p>"},{"location":"reference/loaders/fragments/#chromatinhd.loaders.fragments.FragmentsResult.local_cellxregion_ix","title":"<code>local_cellxregion_ix: torch.Tensor</code>  <code>instance-attribute</code>","text":"<p>Local cell x region index</p>"},{"location":"reference/loaders/fragments/#chromatinhd.loaders.fragments.FragmentsResult.localcellxregion_ix","title":"<code>localcellxregion_ix: torch.Tensor = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Local cell x region index, in the same order as cells_oi and regions_oi</p>"},{"location":"reference/loaders/fragments/#chromatinhd.loaders.fragments.FragmentsResult.n_fragments","title":"<code>n_fragments: int</code>  <code>instance-attribute</code>","text":"<p>Number of fragments</p>"},{"location":"reference/loaders/fragments/#chromatinhd.loaders.fragments.FragmentsResult.n_total_regions","title":"<code>n_total_regions: int = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Total number of regions</p>"},{"location":"reference/loaders/fragments/#chromatinhd.loaders.fragments.FragmentsResult.regionmapping","title":"<code>regionmapping: torch.Tensor = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Mapping from local cell x region index to region index</p>"},{"location":"reference/loaders/fragments/#chromatinhd.loaders.fragments.FragmentsResult.regions_oi","title":"<code>regions_oi: np.ndarray = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Regions of interest</p>"},{"location":"reference/loaders/fragments/#chromatinhd.loaders.fragments.FragmentsResult.window","title":"<code>window: np.ndarray = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Window of the region</p>"},{"location":"reference/loaders/fragments/#chromatinhd.loaders.fragments.CutsResult","title":"<code>chromatinhd.loaders.fragments.CutsResult</code>  <code>dataclass</code>","text":"Source code in <code>src/chromatinhd/loaders/fragments.py</code> <pre><code>@dataclasses.dataclass\nclass CutsResult:\n    coordinates: torch.Tensor\n    local_cellxregion_ix: torch.Tensor\n    n_regions: int\n    n_fragments: int\n    n_cuts: int\n    window: np.ndarray\n\n    @property\n    def local_region_ix(self):\n        return self.local_cellxregion_ix % self.n_regions\n\n    @property\n    def local_cell_ix(self):\n        return torch.div(self.local_cellxregion_ix, self.n_regions, rounding_mode=\"floor\")\n\n    def to(self, device):\n        self.coordinates = self.coordinates.to(device)\n        self.local_cellxregion_ix = self.local_cellxregion_ix.to(device)\n        return self\n</code></pre>"},{"location":"reference/models/diff/interpret/","title":"Interpret","text":""},{"location":"reference/models/diff/interpret/#chromatinhd.models.diff.interpret.RegionPositional","title":"<code>chromatinhd.models.diff.interpret.RegionPositional</code>","text":"<p>         Bases: <code>chd.flow.Flow</code></p> <p>Positional interpretation of diff models</p> Source code in <code>src/chromatinhd/models/diff/interpret/regionpositional.py</code> <pre><code>class RegionPositional(chd.flow.Flow):\n\"\"\"\n    Positional interpretation of *diff* models\n    \"\"\"\n\n    regions = Linked()\n    clustering = Linked()\n\n    probs = StoredDict(DataArray)\n\n    def score(\n        self,\n        models: Models,\n        fragments: Fragments = None,\n        clustering: Clustering = None,\n        regions: list = None,\n        force: bool = False,\n        device: str = \"cpu\",\n        step: int = 50,\n        batch_size: int = 5000,\n        normalize_per_cell: int = 100,\n    ):\n\"\"\"\n        Main scoring function\n\n        Parameters:\n            fragments:\n                the fragments\n            clustering:\n                the clustering\n            models:\n                the models\n            regions:\n                the regions to score, if None, all regions are scored\n            force:\n                whether to force rescoring even if the scores already exist\n            device:\n                the device to use\n        \"\"\"\n        force_ = force\n\n        if fragments is None:\n            fragments = models.fragments\n        if clustering is None:\n            clustering = models.clustering\n\n        if regions is None:\n            regions = fragments.var.index\n\n        self.regions = fragments.regions\n\n        pbar = tqdm.tqdm(regions, leave=False)\n\n        window = fragments.regions.window\n\n        if device is None:\n            device = get_default_device()\n\n        for region in pbar:\n            pbar.set_description(region)\n\n            force = force_\n            if region not in self.probs:\n                force = True\n\n            if force:\n                design_region = pd.DataFrame({\"region_ix\": [fragments.var.index.get_loc(region)]}).astype(\"category\")\n                design_region.index = pd.Series([region], name=\"region\")\n                design_clustering = pd.DataFrame({\"active_cluster\": np.arange(clustering.n_clusters)}).astype(\n                    \"category\"\n                )\n                design_clustering.index = clustering.cluster_info.index\n                design_coord = pd.DataFrame({\"coord\": np.arange(window[0], window[1] + 1, step=step)}).astype(\n                    \"category\"\n                )\n                design_coord.index = design_coord[\"coord\"]\n                design = chd.utils.crossing(design_region, design_clustering, design_coord)\n\n                design[\"batch\"] = np.floor(np.arange(design.shape[0]) / batch_size).astype(int)\n\n                probs = []\n\n                if len(models) == 0:\n                    raise ValueError(\"No models to score\")\n                for model in models:\n                    probs_model = []\n                    for _, design_subset in design.groupby(\"batch\"):\n                        pseudocoordinates = torch.from_numpy(design_subset[\"coord\"].values.astype(int))\n                        # pseudocoordinates = (pseudocoordinates - window[0]) / (window[1] - window[0])\n                        pseudocluster = torch.nn.functional.one_hot(\n                            torch.from_numpy(design_subset[\"active_cluster\"].values.astype(int)),\n                            clustering.n_clusters,\n                        ).to(torch.float)\n                        region_ix = torch.from_numpy(design_subset[\"region_ix\"].values.astype(int))\n\n                        prob = model.evaluate_pseudo(\n                            pseudocoordinates,\n                            clustering=pseudocluster,\n                            region_ix=region_ix,\n                            device=device,\n                            normalize_per_cell=normalize_per_cell,\n                        )\n\n                        probs_model.append(prob.numpy())\n                    probs_model = np.hstack(probs_model)\n                    probs.append(probs_model)\n\n                probs = np.vstack(probs)\n                probs = probs.mean(axis=0)\n\n                probs = xr.DataArray(\n                    probs.reshape(  # we have only one region anyway\n                        (\n                            design_clustering.shape[0],\n                            design_coord.shape[0],\n                        )\n                    ),\n                    coords=[\n                        design_clustering.index,\n                        design_coord.index,\n                    ],\n                )\n\n                self.probs[region] = probs\n\n        return self\n\n    def get_plotdata(self, region: str, clusters=None, relative_to=None) -&gt; (pd.DataFrame, pd.DataFrame):\n\"\"\"\n        Returns average and differential probabilities for a particular region.\n\n        Parameters:\n            region:\n                the region\n\n        Returns:\n            Two dataframes, one with the probabilities per cluster, one with the mean\n        \"\"\"\n        probs = self.probs[region]\n\n        if clusters is not None:\n            probs = probs.sel(cluster=clusters)\n\n        plotdata = probs.to_dataframe(\"prob\")\n\n        plotdata[\"prob\"] = plotdata[\"prob\"]\n\n        if relative_to is not None:\n            plotdata_mean = plotdata[[\"prob\"]].query(\"cluster in @relative_to\").groupby(\"coord\").mean()\n        else:\n            plotdata_mean = plotdata[[\"prob\"]].groupby(\"coord\").mean()\n\n        return plotdata, plotdata_mean\n\n    @property\n    def scored(self):\n        return len(self.probs) &gt; 0\n\n    def calculate_slices(self, prob_cutoff=1.5, clusters_oi=None, cluster_grouping=None, step=1):\n        start_position_ixs = []\n        end_position_ixs = []\n        data = []\n        region_ixs = []\n\n        if clusters_oi is not None:\n            if isinstance(clusters_oi, (pd.Series, pd.Index)):\n                clusters_oi = clusters_oi.tolist()\n\n        for region, probs in tqdm.tqdm(self.probs.items(), leave=False, total=len(self.probs)):\n            if clusters_oi is not None:\n                probs = probs.sel(cluster=clusters_oi)\n\n            if cluster_grouping is not None:\n                probs = probs.groupby(cluster_grouping).mean()\n\n            region_ix = self.regions.var.index.get_loc(region)\n            desired_x = np.arange(*self.regions.window, step=step) - self.regions.window[0]\n            x = probs.coords[\"coord\"].values - self.regions.window[0]\n            y = probs.values\n\n            y_interpolated = chd.utils.interpolate_1d(\n                torch.from_numpy(desired_x), torch.from_numpy(x), torch.from_numpy(y)\n            ).numpy()\n\n            # from y_interpolated, determine start and end positions of the relevant slices\n            start_position_ixs_region, end_position_ixs_region, data_region = extract_slices(\n                y_interpolated, prob_cutoff\n            )\n            start_position_ixs.append(start_position_ixs_region)\n            end_position_ixs.append(end_position_ixs_region)\n            data.append(data_region)\n            region_ixs.append(np.ones(len(start_position_ixs_region), dtype=int) * region_ix)\n        data = np.concatenate(data, axis=0)\n        start_position_ixs = np.concatenate(start_position_ixs, axis=0)\n        end_position_ixs = np.concatenate(end_position_ixs, axis=0)\n        region_ixs = np.concatenate(region_ixs, axis=0)\n\n        slices = Slices(\n            region_ixs,\n            start_position_ixs,\n            end_position_ixs,\n            data,\n            self.regions.n_regions,\n            step=step,\n            window=self.regions.window,\n        )\n        return slices\n\n    def calculate_differential_slices(self, slices, fc_cutoff=2.0, score=\"diff\", a=None, b=None, n=None):\n        if score == \"diff\":\n            data_diff = slices.data - slices.data.mean(1, keepdims=True)\n            data_selected = data_diff &gt; np.log(fc_cutoff)\n        elif score == \"diff2\":\n            data_diff = slices.data - slices.data.mean(1, keepdims=True)\n\n            data_selected = (\n                (data_diff &gt; np.log(4.0))\n                | ((data_diff &gt; np.log(3.0)) &amp; (slices.data &gt; 0.0))\n                | ((data_diff &gt; np.log(2.0)) &amp; (slices.data &gt; 1.0))\n            )\n            # data_diff = data_diff * np.exp(slices.data.mean(1, keepdims=True))\n        elif score == \"diff3\":\n            probs_mean = slices.data.mean(1, keepdims=True)\n            actual = slices.data\n            diff = slices.data - probs_mean\n\n            x1, y1 = a\n            x2, y2 = b\n\n            X = diff\n            Y = actual\n\n            data_diff = -((x2 - x1) * (Y - y1) - (y2 - y1) * (X - x1))\n        else:\n            raise ValueError(f\"Unknown score {score}\")\n\n        if n is None:\n            data_selected = data_diff &gt; np.log(fc_cutoff)\n        else:\n            cutoff = np.quantile(data_diff, 1 - n / data_diff.shape[0], axis=0, keepdims=True)\n            data_selected = data_diff &gt; cutoff\n\n        region_indices = np.repeat(slices.region_ixs, slices.end_position_ixs - slices.start_position_ixs)\n        position_indices = np.concatenate(\n            [np.arange(start, end) for start, end in zip(slices.start_position_ixs, slices.end_position_ixs)]\n        )\n\n        positions = []\n        region_ixs = []\n        cluster_ixs = []\n        for ct_ix in range(data_diff.shape[1]):\n            # select which data is relevant\n            oi = data_selected[:, ct_ix]\n            if oi.sum() == 0:\n                continue\n            positions_oi = position_indices[oi]\n            regions_oi = region_indices[oi]\n\n            start = np.where(\n                np.pad(np.diff(positions_oi) != 1, (1, 0), constant_values=True)\n                | np.pad(np.diff(regions_oi) != 0, (1, 0), constant_values=True)\n            )[0]\n            end = np.pad(start[1:], (0, 1), constant_values=len(positions_oi)) - 1\n\n            positions.append(np.stack([positions_oi[start], positions_oi[end]], axis=1))\n            region_ixs.append(regions_oi[start])\n            cluster_ixs.append(np.ones(len(start), dtype=int) * ct_ix)\n        start_position_ixs, end_position_ixs = np.concatenate(positions, axis=0).T\n        region_ixs = np.concatenate(region_ixs, axis=0)\n        cluster_ixs = np.concatenate(cluster_ixs, axis=0)\n\n        differential_slices = DifferentialSlices(\n            region_ixs,\n            cluster_ixs,\n            start_position_ixs,\n            end_position_ixs,\n            data_diff,\n            self.regions.n_regions,\n            step=slices.step,\n            window=slices.window,\n        )\n        return differential_slices\n\n    def calculate_top_slices(self, slices, fc_cutoff=2.0):\n        data_diff = slices.data - slices.data.mean(1, keepdims=True)\n\n        region_indices = np.repeat(slices.region_ixs, slices.end_position_ixs - slices.start_position_ixs)\n        position_indices = np.concatenate(\n            [np.arange(start, end) for start, end in zip(slices.start_position_ixs, slices.end_position_ixs)]\n        )\n\n        # select which data is relevant\n        oi = data_diff[:,].max(1) &gt; np.log(fc_cutoff)\n        positions_oi = position_indices[oi]\n        regions_oi = region_indices[oi]\n\n        start = np.where(\n            np.pad(np.diff(positions_oi) != 1, (1, 0), constant_values=True)\n            | np.pad(np.diff(regions_oi) != 0, (1, 0), constant_values=True)\n        )[0]\n        end = np.pad(start[1:], (0, 1), constant_values=len(positions_oi)) - 1\n\n        region_ixs = regions_oi[start]\n        data = data_diff[oi].max(1)\n\n        start_position_ixs, end_position_ixs = positions_oi[start], positions_oi[end]\n\n        differential_slices = Slices(\n            region_ixs,\n            start_position_ixs,\n            end_position_ixs,\n            data,\n            self.regions.n_regions,\n            step=slices.step,\n            window=slices.window,\n        )\n        return differential_slices\n\n    def select_windows(self, region_id, max_merge_distance=500, min_length=50, padding=500, prob_cutoff=1.5):\n\"\"\"\n        Select windows based on the number of fragments\n\n        Parameters:\n            region_id:\n                the identifier of the region of interest\n            max_merge_distance:\n                the maximum distance between windows before merging\n            min_length:\n                the minimum length of a window\n            padding:\n                the padding to add to each window\n            prob_cutoff:\n                the probability cutoff\n        \"\"\"\n\n        from scipy.ndimage import convolve\n\n        def spread_true(arr, width=5):\n            kernel = np.ones(width, dtype=bool)\n            result = convolve(arr, kernel, mode=\"constant\", cval=False)\n            result = result != 0\n            return result\n\n        plotdata, plotdata_mean = self.get_plotdata(region_id)\n        selection = pd.DataFrame({\"chosen\": (plotdata[\"prob\"].unstack() &gt; prob_cutoff).any()})\n\n        # add padding\n        step = plotdata.index.get_level_values(\"coord\")[1] - plotdata.index.get_level_values(\"coord\")[0]\n        k_padding = padding // step\n        selection[\"chosen\"] = spread_true(selection[\"chosen\"], width=k_padding)\n\n        # select all contiguous regions where chosen is true\n        selection[\"selection\"] = selection[\"chosen\"].cumsum()\n\n        windows = pd.DataFrame(\n            {\n                \"start\": selection.index[\n                    (np.diff(np.pad(selection[\"chosen\"], (1, 1), constant_values=False).astype(int)) == 1)[:-1]\n                ].astype(int),\n                \"end\": selection.index[\n                    (np.diff(np.pad(selection[\"chosen\"], (1, 1), constant_values=False).astype(int)) == -1)[1:]\n                ].astype(int),\n            }\n        )\n\n        # merge windows that are close to each other\n        windows[\"distance_to_next\"] = windows[\"start\"].shift(-1) - windows[\"end\"]\n\n        windows[\"merge\"] = (windows[\"distance_to_next\"] &lt; max_merge_distance).fillna(False)\n        windows[\"group\"] = (~windows[\"merge\"]).cumsum().shift(1).fillna(0).astype(int)\n        windows = (\n            windows.groupby(\"group\")\n            .agg({\"start\": \"min\", \"end\": \"max\", \"distance_to_next\": \"last\"})\n            .reset_index(drop=True)\n        )\n\n        # filter on length\n        windows[\"length\"] = windows[\"end\"] - windows[\"start\"]\n        windows = windows[windows[\"length\"] &gt; min_length]\n        return windows\n\n    def get_interpolated(self, region_id, clusters=None, desired_x=None, step=1):\n        probs = self.probs[region_id]\n\n        x_raw = probs.coords[\"coord\"].values\n        y_raw = probs.values\n\n        if desired_x is None:\n            assert step is not None\n            desired_x = np.arange(*self.regions.window, step=step) - self.regions.window[0]\n\n        y = chd.utils.interpolate_1d(\n            torch.from_numpy(desired_x), torch.from_numpy(x_raw), torch.from_numpy(y_raw)\n        ).numpy()\n\n        return y\n</code></pre>"},{"location":"reference/models/diff/interpret/#chromatinhd.models.diff.interpret.regionpositional.RegionPositional.get_plotdata","title":"<code>get_plotdata(region, clusters=None, relative_to=None)</code>","text":"<p>Returns average and differential probabilities for a particular region.</p> <p>Parameters:</p> Name Type Description Default <code>region</code> <code>str</code> <p>the region</p> required <p>Returns:</p> Type Description <code>pd.DataFrame, pd.DataFrame</code> <p>Two dataframes, one with the probabilities per cluster, one with the mean</p> Source code in <code>src/chromatinhd/models/diff/interpret/regionpositional.py</code> <pre><code>def get_plotdata(self, region: str, clusters=None, relative_to=None) -&gt; (pd.DataFrame, pd.DataFrame):\n\"\"\"\n    Returns average and differential probabilities for a particular region.\n\n    Parameters:\n        region:\n            the region\n\n    Returns:\n        Two dataframes, one with the probabilities per cluster, one with the mean\n    \"\"\"\n    probs = self.probs[region]\n\n    if clusters is not None:\n        probs = probs.sel(cluster=clusters)\n\n    plotdata = probs.to_dataframe(\"prob\")\n\n    plotdata[\"prob\"] = plotdata[\"prob\"]\n\n    if relative_to is not None:\n        plotdata_mean = plotdata[[\"prob\"]].query(\"cluster in @relative_to\").groupby(\"coord\").mean()\n    else:\n        plotdata_mean = plotdata[[\"prob\"]].groupby(\"coord\").mean()\n\n    return plotdata, plotdata_mean\n</code></pre>"},{"location":"reference/models/diff/interpret/#chromatinhd.models.diff.interpret.regionpositional.RegionPositional.score","title":"<code>score(models, fragments=None, clustering=None, regions=None, force=False, device='cpu', step=50, batch_size=5000, normalize_per_cell=100)</code>","text":"<p>Main scoring function</p> <p>Parameters:</p> Name Type Description Default <code>fragments</code> <code>Fragments</code> <p>the fragments</p> <code>None</code> <code>clustering</code> <code>Clustering</code> <p>the clustering</p> <code>None</code> <code>models</code> <code>Models</code> <p>the models</p> required <code>regions</code> <code>list</code> <p>the regions to score, if None, all regions are scored</p> <code>None</code> <code>force</code> <code>bool</code> <p>whether to force rescoring even if the scores already exist</p> <code>False</code> <code>device</code> <code>str</code> <p>the device to use</p> <code>'cpu'</code> Source code in <code>src/chromatinhd/models/diff/interpret/regionpositional.py</code> <pre><code>def score(\n    self,\n    models: Models,\n    fragments: Fragments = None,\n    clustering: Clustering = None,\n    regions: list = None,\n    force: bool = False,\n    device: str = \"cpu\",\n    step: int = 50,\n    batch_size: int = 5000,\n    normalize_per_cell: int = 100,\n):\n\"\"\"\n    Main scoring function\n\n    Parameters:\n        fragments:\n            the fragments\n        clustering:\n            the clustering\n        models:\n            the models\n        regions:\n            the regions to score, if None, all regions are scored\n        force:\n            whether to force rescoring even if the scores already exist\n        device:\n            the device to use\n    \"\"\"\n    force_ = force\n\n    if fragments is None:\n        fragments = models.fragments\n    if clustering is None:\n        clustering = models.clustering\n\n    if regions is None:\n        regions = fragments.var.index\n\n    self.regions = fragments.regions\n\n    pbar = tqdm.tqdm(regions, leave=False)\n\n    window = fragments.regions.window\n\n    if device is None:\n        device = get_default_device()\n\n    for region in pbar:\n        pbar.set_description(region)\n\n        force = force_\n        if region not in self.probs:\n            force = True\n\n        if force:\n            design_region = pd.DataFrame({\"region_ix\": [fragments.var.index.get_loc(region)]}).astype(\"category\")\n            design_region.index = pd.Series([region], name=\"region\")\n            design_clustering = pd.DataFrame({\"active_cluster\": np.arange(clustering.n_clusters)}).astype(\n                \"category\"\n            )\n            design_clustering.index = clustering.cluster_info.index\n            design_coord = pd.DataFrame({\"coord\": np.arange(window[0], window[1] + 1, step=step)}).astype(\n                \"category\"\n            )\n            design_coord.index = design_coord[\"coord\"]\n            design = chd.utils.crossing(design_region, design_clustering, design_coord)\n\n            design[\"batch\"] = np.floor(np.arange(design.shape[0]) / batch_size).astype(int)\n\n            probs = []\n\n            if len(models) == 0:\n                raise ValueError(\"No models to score\")\n            for model in models:\n                probs_model = []\n                for _, design_subset in design.groupby(\"batch\"):\n                    pseudocoordinates = torch.from_numpy(design_subset[\"coord\"].values.astype(int))\n                    # pseudocoordinates = (pseudocoordinates - window[0]) / (window[1] - window[0])\n                    pseudocluster = torch.nn.functional.one_hot(\n                        torch.from_numpy(design_subset[\"active_cluster\"].values.astype(int)),\n                        clustering.n_clusters,\n                    ).to(torch.float)\n                    region_ix = torch.from_numpy(design_subset[\"region_ix\"].values.astype(int))\n\n                    prob = model.evaluate_pseudo(\n                        pseudocoordinates,\n                        clustering=pseudocluster,\n                        region_ix=region_ix,\n                        device=device,\n                        normalize_per_cell=normalize_per_cell,\n                    )\n\n                    probs_model.append(prob.numpy())\n                probs_model = np.hstack(probs_model)\n                probs.append(probs_model)\n\n            probs = np.vstack(probs)\n            probs = probs.mean(axis=0)\n\n            probs = xr.DataArray(\n                probs.reshape(  # we have only one region anyway\n                    (\n                        design_clustering.shape[0],\n                        design_coord.shape[0],\n                    )\n                ),\n                coords=[\n                    design_clustering.index,\n                    design_coord.index,\n                ],\n            )\n\n            self.probs[region] = probs\n\n    return self\n</code></pre>"},{"location":"reference/models/diff/interpret/#chromatinhd.models.diff.interpret.regionpositional.RegionPositional.select_windows","title":"<code>select_windows(region_id, max_merge_distance=500, min_length=50, padding=500, prob_cutoff=1.5)</code>","text":"<p>Select windows based on the number of fragments</p> <p>Parameters:</p> Name Type Description Default <code>region_id</code> <p>the identifier of the region of interest</p> required <code>max_merge_distance</code> <p>the maximum distance between windows before merging</p> <code>500</code> <code>min_length</code> <p>the minimum length of a window</p> <code>50</code> <code>padding</code> <p>the padding to add to each window</p> <code>500</code> <code>prob_cutoff</code> <p>the probability cutoff</p> <code>1.5</code> Source code in <code>src/chromatinhd/models/diff/interpret/regionpositional.py</code> <pre><code>def select_windows(self, region_id, max_merge_distance=500, min_length=50, padding=500, prob_cutoff=1.5):\n\"\"\"\n    Select windows based on the number of fragments\n\n    Parameters:\n        region_id:\n            the identifier of the region of interest\n        max_merge_distance:\n            the maximum distance between windows before merging\n        min_length:\n            the minimum length of a window\n        padding:\n            the padding to add to each window\n        prob_cutoff:\n            the probability cutoff\n    \"\"\"\n\n    from scipy.ndimage import convolve\n\n    def spread_true(arr, width=5):\n        kernel = np.ones(width, dtype=bool)\n        result = convolve(arr, kernel, mode=\"constant\", cval=False)\n        result = result != 0\n        return result\n\n    plotdata, plotdata_mean = self.get_plotdata(region_id)\n    selection = pd.DataFrame({\"chosen\": (plotdata[\"prob\"].unstack() &gt; prob_cutoff).any()})\n\n    # add padding\n    step = plotdata.index.get_level_values(\"coord\")[1] - plotdata.index.get_level_values(\"coord\")[0]\n    k_padding = padding // step\n    selection[\"chosen\"] = spread_true(selection[\"chosen\"], width=k_padding)\n\n    # select all contiguous regions where chosen is true\n    selection[\"selection\"] = selection[\"chosen\"].cumsum()\n\n    windows = pd.DataFrame(\n        {\n            \"start\": selection.index[\n                (np.diff(np.pad(selection[\"chosen\"], (1, 1), constant_values=False).astype(int)) == 1)[:-1]\n            ].astype(int),\n            \"end\": selection.index[\n                (np.diff(np.pad(selection[\"chosen\"], (1, 1), constant_values=False).astype(int)) == -1)[1:]\n            ].astype(int),\n        }\n    )\n\n    # merge windows that are close to each other\n    windows[\"distance_to_next\"] = windows[\"start\"].shift(-1) - windows[\"end\"]\n\n    windows[\"merge\"] = (windows[\"distance_to_next\"] &lt; max_merge_distance).fillna(False)\n    windows[\"group\"] = (~windows[\"merge\"]).cumsum().shift(1).fillna(0).astype(int)\n    windows = (\n        windows.groupby(\"group\")\n        .agg({\"start\": \"min\", \"end\": \"max\", \"distance_to_next\": \"last\"})\n        .reset_index(drop=True)\n    )\n\n    # filter on length\n    windows[\"length\"] = windows[\"end\"] - windows[\"start\"]\n    windows = windows[windows[\"length\"] &gt; min_length]\n    return windows\n</code></pre>"},{"location":"reference/models/diff/model/","title":"Model","text":""},{"location":"reference/models/diff/model/#cutnf","title":"CutNF","text":"<p>The basic differential model that only looks at cut sites individually, regardless of the fragment's and cell's other cut sites</p>"},{"location":"reference/models/diff/model/#chromatinhd.models.diff.model.cutnf.Model","title":"<code>chromatinhd.models.diff.model.cutnf.Model</code>","text":"<p>         Bases: <code>torch.nn.Module</code>, <code>HybridModel</code></p> <p>A ChromatinHD-diff model that models the probability density of observing a cut site between clusterings</p> Source code in <code>src/chromatinhd/models/diff/model/cutnf.py</code> <pre><code>class Model(torch.nn.Module, HybridModel):\n\"\"\"\n    A ChromatinHD-diff model that models the probability density of observing a cut site between clusterings\n    \"\"\"\n\n    def __init__(\n        self,\n        fragments: Fragments,\n        clustering: Clustering,\n        nbins: List[int] = (\n            128,\n            64,\n            32,\n        ),\n        decoder_n_layers=0,\n        baseline=False,\n        rho_delta_regularization=True,\n        rho_delta_p_scale_free=False,\n        mixture_delta_regularization=True,\n        mixture_delta_p_scale_free=False,\n        mixture_delta_p_scale_dist=\"normal\",\n        mixture_delta_p_scale=1.0,\n    ):\n\"\"\"\n        Parameters:\n            fragments:\n                Fragments object\n            clustering:\n                Clustering object\n            nbins:\n                Number of bins for the spline\n            decoder_n_layers:\n                Number of layers in the decoder\n            baseline:\n                Whether to use a baseline model\n        \"\"\"\n        super().__init__()\n\n        self.n_total_regions = fragments.n_regions\n\n        self.n_clusters = clustering.n_clusters\n\n        transform = DifferentialQuadraticSplineStack(\n            nbins=nbins,\n            n_regions=fragments.n_regions,\n        )\n        self.mixture = TransformedDistribution(transform)\n        n_delta_mixture_components = sum(transform.split_deltas)\n\n        if not baseline:\n            self.decoder = Decoder(\n                self.n_clusters,\n                fragments.n_regions,\n                n_delta_mixture_components,\n                n_layers=decoder_n_layers,\n            )\n        else:\n            self.decoder = BaselineDecoder(\n                self.n_clusters,\n                fragments.n_regions,\n                n_delta_mixture_components,\n                n_layers=decoder_n_layers,\n            )\n\n        # calculate libsizes and rho bias\n        libsize = torch.from_numpy(np.bincount(fragments.mapping[:, 0], minlength=fragments.n_cells))\n\n        rho_bias = (\n            torch.from_numpy(np.bincount(fragments.mapping[:, 1], minlength=fragments.n_regions))\n            / fragments.n_cells\n            / libsize.to(torch.float).mean()\n        )\n        min_rho_bias = 1e-5\n        rho_bias = min_rho_bias + (1 - min_rho_bias) * rho_bias\n        self.register_buffer(\"rho_bias\", rho_bias)\n\n        self.track = {}\n\n        self.mixture_delta_regularization = mixture_delta_regularization\n        if self.mixture_delta_regularization:\n            if mixture_delta_p_scale_free:\n                self.mixture_delta_p_scale = torch.nn.Parameter(\n                    torch.tensor(math.log(mixture_delta_p_scale), requires_grad=True)\n                )\n            else:\n                self.register_buffer(\n                    \"mixture_delta_p_scale\",\n                    torch.tensor(math.log(mixture_delta_p_scale)),\n                )\n        self.mixture_delta_p_scale_dist = mixture_delta_p_scale_dist\n\n        self.rho_delta_regularization = rho_delta_regularization\n        if self.rho_delta_regularization:\n            if rho_delta_p_scale_free:\n                self.rho_delta_p_scale = torch.nn.Parameter(torch.log(torch.tensor(0.1, requires_grad=True)))\n            else:\n                self.register_buffer(\"rho_delta_p_scale\", torch.tensor(math.log(1.0)))\n\n    def forward_(\n        self,\n        coordinates,\n        clustering,\n        regions_oi,\n        local_cellxregion_ix,\n        localcellxregion_ix,\n        local_region_ix,\n    ):\n        # decode\n        mixture_delta, rho_delta = self.decoder(clustering, regions_oi)\n\n        # rho\n        rho = torch.nn.functional.softmax(torch.log(self.rho_bias) + rho_delta, -1)\n        rho_cuts = rho.flatten()[localcellxregion_ix]\n\n        # fragment counts\n        mixture_delta_cellxregion = mixture_delta.view(np.prod(mixture_delta.shape[:2]), mixture_delta.shape[-1])\n        mixture_delta = mixture_delta_cellxregion[local_cellxregion_ix]\n\n        self.track[\"likelihood_mixture\"] = likelihood_mixture = self.mixture.log_prob(\n            coordinates, regions_oi=regions_oi, local_region_ix=local_region_ix, delta=mixture_delta\n        )\n\n        self.track[\"likelihood_overall\"] = likelihood_overall = torch.log(rho_cuts) + math.log(self.n_total_regions)\n\n        # likelihood\n        likelihood = self.track[\"likelihood\"] = likelihood_mixture + likelihood_overall\n\n        elbo = -likelihood.sum()\n\n        # regularization\n        # mixture\n        if self.mixture_delta_regularization:\n            mixture_delta_p = torch.distributions.Normal(0.0, torch.exp(self.mixture_delta_p_scale))\n            mixture_delta_kl = mixture_delta_p.log_prob(self.decoder.logit_weight(regions_oi))\n\n            elbo -= mixture_delta_kl.sum()\n\n        # rho delta\n        if self.rho_delta_regularization:\n            rho_delta_p = torch.distributions.Normal(0.0, torch.exp(self.rho_delta_p_scale))\n            rho_delta_kl = rho_delta_p.log_prob(self.decoder.rho_weight(regions_oi))\n\n            elbo -= rho_delta_kl.sum()\n\n        return elbo\n\n    def forward(self, data):\n        return self.forward_(\n            coordinates=(data.cuts.coordinates - data.cuts.window[0]) / (data.cuts.window[1] - data.cuts.window[0]),\n            clustering=data.clustering.onehot,\n            regions_oi=data.minibatch.regions_oi_torch,\n            local_region_ix=data.cuts.local_region_ix,\n            local_cellxregion_ix=data.cuts.local_cellxregion_ix,\n            localcellxregion_ix=data.cuts.localcellxregion_ix,\n        )\n\n    def train_model(self, fragments, clustering, fold, device=None, n_epochs=30, lr=1e-2):\n\"\"\"\n        Trains the model\n        \"\"\"\n\n        if device is None:\n            device = get_default_device()\n\n        # set up minibatchers and loaders\n        minibatcher_train = Minibatcher(\n            fold[\"cells_train\"],\n            range(fragments.n_regions),\n            n_regions_step=500,\n            n_cells_step=200,\n        )\n        minibatcher_validation = Minibatcher(\n            fold[\"cells_validation\"],\n            range(fragments.n_regions),\n            n_regions_step=10,\n            n_cells_step=10000,\n            permute_cells=False,\n            permute_regions=False,\n        )\n\n        loaders_train = LoaderPool(\n            ClusteringCuts,\n            dict(\n                clustering=clustering,\n                fragments=fragments,\n                cellxregion_batch_size=minibatcher_train.cellxregion_batch_size,\n            ),\n            n_workers=10,\n        )\n        loaders_validation = LoaderPool(\n            ClusteringCuts,\n            dict(\n                clustering=clustering,\n                fragments=fragments,\n                cellxregion_batch_size=minibatcher_validation.cellxregion_batch_size,\n            ),\n            n_workers=5,\n        )\n\n        trainer = Trainer(\n            self,\n            loaders_train,\n            loaders_validation,\n            minibatcher_train,\n            minibatcher_validation,\n            SparseDenseAdam(\n                self.parameters_sparse(),\n                self.parameters_dense(),\n                lr=lr,\n                weight_decay=1e-5,\n            ),\n            n_epochs=n_epochs,\n            checkpoint_every_epoch=1,\n            optimize_every_step=1,\n            device=device,\n        )\n        self.trace = trainer.trace\n\n        trainer.train()\n\n    def _get_likelihood_cell_region(self, likelihood, local_cellxregion_ix, n_cells, n_regions):\n        return torch_scatter.segment_sum_coo(likelihood, local_cellxregion_ix, dim_size=n_cells * n_regions).reshape(\n            (n_cells, n_regions)\n        )\n\n    def get_prediction(\n        self,\n        fragments: Fragments,\n        clustering: Clustering,\n        cells: List[str] = None,\n        cell_ixs: List[int] = None,\n        regions: List[str] = None,\n        region_ixs: List[int] = None,\n        device: str = None,\n    ) -&gt; xr.Dataset:\n\"\"\"\n        Returns the likelihoods of the observed cut sites for each cell and region\n\n        Parameters:\n            fragments: Fragments object\n            clustering: Clustering object\n            cells: Cells to predict\n            cell_ixs: Cell indices to predict\n            regions: Genes to predict\n            region_ixs: Gene indices to predict\n            device: Device to use\n\n        Returns:\n            **likelihood_mixture**, likelihood of the observing a cut site at the particular genomic location, conditioned on the region region. **likelihood_overall**, likelihood of observing a cut site in the region region\n        \"\"\"\n\n        if cell_ixs is None:\n            if cells is None:\n                cells = fragments.obs.index\n            fragments.obs[\"ix\"] = np.arange(len(fragments.obs))\n            cell_ixs = fragments.obs.loc[cells][\"ix\"].values\n        if cells is None:\n            cells = fragments.obs.index[cell_ixs]\n\n        if region_ixs is None:\n            if regions is None:\n                regions = fragments.var.index\n            fragments.var[\"ix\"] = np.arange(len(fragments.var))\n            region_ixs = fragments.var.loc[regions][\"ix\"].values\n        if regions is None:\n            regions = fragments.var.index[region_ixs]\n\n        minibatches = Minibatcher(\n            cell_ixs,\n            region_ixs,\n            n_regions_step=500,\n            n_cells_step=200,\n            use_all_cells=True,\n            use_all_regions=True,\n            permute_cells=False,\n            permute_regions=False,\n        )\n        loaders = LoaderPool(\n            ClusteringCuts,\n            dict(\n                clustering=clustering,\n                fragments=fragments,\n                cellxregion_batch_size=minibatches.cellxregion_batch_size,\n            ),\n            n_workers=5,\n        )\n        loaders.initialize(minibatches)\n\n        likelihood_mixture = np.zeros((len(cell_ixs), len(region_ixs)))\n        likelihood_overall = np.zeros((len(cell_ixs), len(region_ixs)))\n\n        cell_mapping = np.zeros(fragments.n_cells, dtype=np.int64)\n        cell_mapping[cell_ixs] = np.arange(len(cell_ixs))\n\n        region_mapping = np.zeros(fragments.n_regions, dtype=np.int64)\n        region_mapping[region_ixs] = np.arange(len(region_ixs))\n\n        self.eval()\n        self = self.to(device)\n\n        for data in loaders:\n            data = data.to(device)\n            with torch.no_grad():\n                self.forward(data)\n\n            likelihood_mixture[\n                np.ix_(\n                    cell_mapping[data.minibatch.cells_oi],\n                    region_mapping[data.minibatch.regions_oi],\n                )\n            ] += (\n                self._get_likelihood_cell_region(\n                    self.track[\"likelihood_mixture\"],\n                    data.cuts.local_cellxregion_ix,\n                    data.minibatch.n_cells,\n                    data.minibatch.n_regions,\n                )\n                .cpu()\n                .numpy()\n            )\n            likelihood_overall[\n                np.ix_(\n                    cell_mapping[data.minibatch.cells_oi],\n                    region_mapping[data.minibatch.regions_oi],\n                )\n            ] += (\n                self._get_likelihood_cell_region(\n                    self.track[\"likelihood_overall\"],\n                    data.cuts.local_cellxregion_ix,\n                    data.minibatch.n_cells,\n                    data.minibatch.n_regions,\n                )\n                .cpu()\n                .numpy()\n            )\n\n        self = self.to(\"cpu\")\n\n        result = xr.Dataset(\n            {\n                \"likelihood_mixture\": xr.DataArray(\n                    likelihood_mixture,\n                    dims=(fragments.obs.index.name, fragments.var.index.name),\n                    coords={fragments.obs.index.name: cells, fragments.var.index.name: fragments.var.index},\n                ),\n                \"likelihood_overall\": xr.DataArray(\n                    likelihood_overall,\n                    dims=(fragments.obs.index.name, fragments.var.index.name),\n                    coords={fragments.obs.index.name: cells, fragments.var.index.name: fragments.var.index},\n                ),\n            }\n        )\n        return result\n\n    def evaluate_pseudo(\n        self,\n        coordinates,\n        clustering=None,\n        region_oi=None,\n        region_ix=None,\n        device=None,\n    ):\n        from chromatinhd.loaders.clustering import Result as ClusteringResult\n        from chromatinhd.models.diff.loader.clustering_cuts import (\n            Result as ClusteringCutsResult,\n        )\n        from chromatinhd.loaders.fragments import CutsResult\n        from chromatinhd.loaders.minibatches import Minibatch\n\n        if not torch.is_tensor(clustering):\n            if clustering is None:\n                clustering = 0.0\n            clustering = torch.ones((1, self.n_clusters)) * clustering\n\n            print(clustering)\n\n        cells_oi = torch.ones((1,), dtype=torch.long)\n\n        local_cellxregion_ix = torch.tensor([], dtype=torch.long)\n        if region_ix is None:\n            if region_oi is None:\n                region_oi = 0\n            regions_oi = torch.tensor([region_oi], dtype=torch.long)\n            local_region_ix = torch.zeros_like(coordinates).to(torch.long)\n            local_cellxregion_ix = torch.zeros_like(coordinates).to(torch.long)\n            localcellxregion_ix = torch.ones_like(coordinates).to(torch.long) * region_oi\n        else:\n            assert len(region_ix) == len(coordinates)\n            regions_oi = torch.unique(region_ix)\n\n            local_region_mapping = torch.zeros(regions_oi.max() + 1, dtype=torch.long)\n            local_region_mapping.index_add_(0, regions_oi, torch.arange(len(regions_oi)))\n\n            local_region_ix = local_region_mapping[region_ix]\n            local_cell_ix = torch.arange(clustering.shape[0])\n            local_cellxregion_ix = local_cell_ix * len(regions_oi) + local_region_ix\n            localcellxregion_ix = local_cell_ix * self.n_total_regions + region_ix\n\n        data = ClusteringCutsResult(\n            cuts=CutsResult(\n                coordinates=coordinates,\n                local_cellxregion_ix=local_cellxregion_ix,\n                localcellxregion_ix=localcellxregion_ix,\n                n_regions=len(regions_oi),\n                n_fragments=len(coordinates),\n                n_cuts=len(coordinates),\n                window=torch.tensor([0, 1]),\n            ),\n            clustering=ClusteringResult(\n                onehot=clustering,\n            ),\n            minibatch=Minibatch(\n                cells_oi=cells_oi.cpu().numpy(),\n                regions_oi=regions_oi.cpu().numpy(),\n            ),\n        ).to(device)\n\n        self = self.to(device).eval()\n\n        with torch.no_grad():\n            self.forward(data)\n\n        self = self.to(\"cpu\")\n\n        prob = self.track[\"likelihood\"].detach().cpu()\n        return prob.detach().cpu()\n</code></pre>"},{"location":"reference/models/diff/model/#chromatinhd.models.diff.model.cutnf.Model.__init__","title":"<code>__init__(fragments, clustering, nbins=(128, 64, 32), decoder_n_layers=0, baseline=False, rho_delta_regularization=True, rho_delta_p_scale_free=False, mixture_delta_regularization=True, mixture_delta_p_scale_free=False, mixture_delta_p_scale_dist='normal', mixture_delta_p_scale=1.0)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>fragments</code> <code>Fragments</code> <p>Fragments object</p> required <code>clustering</code> <code>Clustering</code> <p>Clustering object</p> required <code>nbins</code> <code>List[int]</code> <p>Number of bins for the spline</p> <code>(128, 64, 32)</code> <code>decoder_n_layers</code> <p>Number of layers in the decoder</p> <code>0</code> <code>baseline</code> <p>Whether to use a baseline model</p> <code>False</code> Source code in <code>src/chromatinhd/models/diff/model/cutnf.py</code> <pre><code>def __init__(\n    self,\n    fragments: Fragments,\n    clustering: Clustering,\n    nbins: List[int] = (\n        128,\n        64,\n        32,\n    ),\n    decoder_n_layers=0,\n    baseline=False,\n    rho_delta_regularization=True,\n    rho_delta_p_scale_free=False,\n    mixture_delta_regularization=True,\n    mixture_delta_p_scale_free=False,\n    mixture_delta_p_scale_dist=\"normal\",\n    mixture_delta_p_scale=1.0,\n):\n\"\"\"\n    Parameters:\n        fragments:\n            Fragments object\n        clustering:\n            Clustering object\n        nbins:\n            Number of bins for the spline\n        decoder_n_layers:\n            Number of layers in the decoder\n        baseline:\n            Whether to use a baseline model\n    \"\"\"\n    super().__init__()\n\n    self.n_total_regions = fragments.n_regions\n\n    self.n_clusters = clustering.n_clusters\n\n    transform = DifferentialQuadraticSplineStack(\n        nbins=nbins,\n        n_regions=fragments.n_regions,\n    )\n    self.mixture = TransformedDistribution(transform)\n    n_delta_mixture_components = sum(transform.split_deltas)\n\n    if not baseline:\n        self.decoder = Decoder(\n            self.n_clusters,\n            fragments.n_regions,\n            n_delta_mixture_components,\n            n_layers=decoder_n_layers,\n        )\n    else:\n        self.decoder = BaselineDecoder(\n            self.n_clusters,\n            fragments.n_regions,\n            n_delta_mixture_components,\n            n_layers=decoder_n_layers,\n        )\n\n    # calculate libsizes and rho bias\n    libsize = torch.from_numpy(np.bincount(fragments.mapping[:, 0], minlength=fragments.n_cells))\n\n    rho_bias = (\n        torch.from_numpy(np.bincount(fragments.mapping[:, 1], minlength=fragments.n_regions))\n        / fragments.n_cells\n        / libsize.to(torch.float).mean()\n    )\n    min_rho_bias = 1e-5\n    rho_bias = min_rho_bias + (1 - min_rho_bias) * rho_bias\n    self.register_buffer(\"rho_bias\", rho_bias)\n\n    self.track = {}\n\n    self.mixture_delta_regularization = mixture_delta_regularization\n    if self.mixture_delta_regularization:\n        if mixture_delta_p_scale_free:\n            self.mixture_delta_p_scale = torch.nn.Parameter(\n                torch.tensor(math.log(mixture_delta_p_scale), requires_grad=True)\n            )\n        else:\n            self.register_buffer(\n                \"mixture_delta_p_scale\",\n                torch.tensor(math.log(mixture_delta_p_scale)),\n            )\n    self.mixture_delta_p_scale_dist = mixture_delta_p_scale_dist\n\n    self.rho_delta_regularization = rho_delta_regularization\n    if self.rho_delta_regularization:\n        if rho_delta_p_scale_free:\n            self.rho_delta_p_scale = torch.nn.Parameter(torch.log(torch.tensor(0.1, requires_grad=True)))\n        else:\n            self.register_buffer(\"rho_delta_p_scale\", torch.tensor(math.log(1.0)))\n</code></pre>"},{"location":"reference/models/diff/model/#chromatinhd.models.diff.model.cutnf.Model.get_prediction","title":"<code>get_prediction(fragments, clustering, cells=None, cell_ixs=None, regions=None, region_ixs=None, device=None)</code>","text":"<p>Returns the likelihoods of the observed cut sites for each cell and region</p> <p>Parameters:</p> Name Type Description Default <code>fragments</code> <code>Fragments</code> <p>Fragments object</p> required <code>clustering</code> <code>Clustering</code> <p>Clustering object</p> required <code>cells</code> <code>List[str]</code> <p>Cells to predict</p> <code>None</code> <code>cell_ixs</code> <code>List[int]</code> <p>Cell indices to predict</p> <code>None</code> <code>regions</code> <code>List[str]</code> <p>Genes to predict</p> <code>None</code> <code>region_ixs</code> <code>List[int]</code> <p>Gene indices to predict</p> <code>None</code> <code>device</code> <code>str</code> <p>Device to use</p> <code>None</code> <p>Returns:</p> Type Description <code>xr.Dataset</code> <p>likelihood_mixture, likelihood of the observing a cut site at the particular genomic location, conditioned on the region region. likelihood_overall, likelihood of observing a cut site in the region region</p> Source code in <code>src/chromatinhd/models/diff/model/cutnf.py</code> <pre><code>def get_prediction(\n    self,\n    fragments: Fragments,\n    clustering: Clustering,\n    cells: List[str] = None,\n    cell_ixs: List[int] = None,\n    regions: List[str] = None,\n    region_ixs: List[int] = None,\n    device: str = None,\n) -&gt; xr.Dataset:\n\"\"\"\n    Returns the likelihoods of the observed cut sites for each cell and region\n\n    Parameters:\n        fragments: Fragments object\n        clustering: Clustering object\n        cells: Cells to predict\n        cell_ixs: Cell indices to predict\n        regions: Genes to predict\n        region_ixs: Gene indices to predict\n        device: Device to use\n\n    Returns:\n        **likelihood_mixture**, likelihood of the observing a cut site at the particular genomic location, conditioned on the region region. **likelihood_overall**, likelihood of observing a cut site in the region region\n    \"\"\"\n\n    if cell_ixs is None:\n        if cells is None:\n            cells = fragments.obs.index\n        fragments.obs[\"ix\"] = np.arange(len(fragments.obs))\n        cell_ixs = fragments.obs.loc[cells][\"ix\"].values\n    if cells is None:\n        cells = fragments.obs.index[cell_ixs]\n\n    if region_ixs is None:\n        if regions is None:\n            regions = fragments.var.index\n        fragments.var[\"ix\"] = np.arange(len(fragments.var))\n        region_ixs = fragments.var.loc[regions][\"ix\"].values\n    if regions is None:\n        regions = fragments.var.index[region_ixs]\n\n    minibatches = Minibatcher(\n        cell_ixs,\n        region_ixs,\n        n_regions_step=500,\n        n_cells_step=200,\n        use_all_cells=True,\n        use_all_regions=True,\n        permute_cells=False,\n        permute_regions=False,\n    )\n    loaders = LoaderPool(\n        ClusteringCuts,\n        dict(\n            clustering=clustering,\n            fragments=fragments,\n            cellxregion_batch_size=minibatches.cellxregion_batch_size,\n        ),\n        n_workers=5,\n    )\n    loaders.initialize(minibatches)\n\n    likelihood_mixture = np.zeros((len(cell_ixs), len(region_ixs)))\n    likelihood_overall = np.zeros((len(cell_ixs), len(region_ixs)))\n\n    cell_mapping = np.zeros(fragments.n_cells, dtype=np.int64)\n    cell_mapping[cell_ixs] = np.arange(len(cell_ixs))\n\n    region_mapping = np.zeros(fragments.n_regions, dtype=np.int64)\n    region_mapping[region_ixs] = np.arange(len(region_ixs))\n\n    self.eval()\n    self = self.to(device)\n\n    for data in loaders:\n        data = data.to(device)\n        with torch.no_grad():\n            self.forward(data)\n\n        likelihood_mixture[\n            np.ix_(\n                cell_mapping[data.minibatch.cells_oi],\n                region_mapping[data.minibatch.regions_oi],\n            )\n        ] += (\n            self._get_likelihood_cell_region(\n                self.track[\"likelihood_mixture\"],\n                data.cuts.local_cellxregion_ix,\n                data.minibatch.n_cells,\n                data.minibatch.n_regions,\n            )\n            .cpu()\n            .numpy()\n        )\n        likelihood_overall[\n            np.ix_(\n                cell_mapping[data.minibatch.cells_oi],\n                region_mapping[data.minibatch.regions_oi],\n            )\n        ] += (\n            self._get_likelihood_cell_region(\n                self.track[\"likelihood_overall\"],\n                data.cuts.local_cellxregion_ix,\n                data.minibatch.n_cells,\n                data.minibatch.n_regions,\n            )\n            .cpu()\n            .numpy()\n        )\n\n    self = self.to(\"cpu\")\n\n    result = xr.Dataset(\n        {\n            \"likelihood_mixture\": xr.DataArray(\n                likelihood_mixture,\n                dims=(fragments.obs.index.name, fragments.var.index.name),\n                coords={fragments.obs.index.name: cells, fragments.var.index.name: fragments.var.index},\n            ),\n            \"likelihood_overall\": xr.DataArray(\n                likelihood_overall,\n                dims=(fragments.obs.index.name, fragments.var.index.name),\n                coords={fragments.obs.index.name: cells, fragments.var.index.name: fragments.var.index},\n            ),\n        }\n    )\n    return result\n</code></pre>"},{"location":"reference/models/diff/model/#chromatinhd.models.diff.model.cutnf.Model.train_model","title":"<code>train_model(fragments, clustering, fold, device=None, n_epochs=30, lr=0.01)</code>","text":"<p>Trains the model</p> Source code in <code>src/chromatinhd/models/diff/model/cutnf.py</code> <pre><code>def train_model(self, fragments, clustering, fold, device=None, n_epochs=30, lr=1e-2):\n\"\"\"\n    Trains the model\n    \"\"\"\n\n    if device is None:\n        device = get_default_device()\n\n    # set up minibatchers and loaders\n    minibatcher_train = Minibatcher(\n        fold[\"cells_train\"],\n        range(fragments.n_regions),\n        n_regions_step=500,\n        n_cells_step=200,\n    )\n    minibatcher_validation = Minibatcher(\n        fold[\"cells_validation\"],\n        range(fragments.n_regions),\n        n_regions_step=10,\n        n_cells_step=10000,\n        permute_cells=False,\n        permute_regions=False,\n    )\n\n    loaders_train = LoaderPool(\n        ClusteringCuts,\n        dict(\n            clustering=clustering,\n            fragments=fragments,\n            cellxregion_batch_size=minibatcher_train.cellxregion_batch_size,\n        ),\n        n_workers=10,\n    )\n    loaders_validation = LoaderPool(\n        ClusteringCuts,\n        dict(\n            clustering=clustering,\n            fragments=fragments,\n            cellxregion_batch_size=minibatcher_validation.cellxregion_batch_size,\n        ),\n        n_workers=5,\n    )\n\n    trainer = Trainer(\n        self,\n        loaders_train,\n        loaders_validation,\n        minibatcher_train,\n        minibatcher_validation,\n        SparseDenseAdam(\n            self.parameters_sparse(),\n            self.parameters_dense(),\n            lr=lr,\n            weight_decay=1e-5,\n        ),\n        n_epochs=n_epochs,\n        checkpoint_every_epoch=1,\n        optimize_every_step=1,\n        device=device,\n    )\n    self.trace = trainer.trace\n\n    trainer.train()\n</code></pre>"},{"location":"reference/models/diff/model/#chromatinhd.models.diff.model.cutnf.Models","title":"<code>chromatinhd.models.diff.model.cutnf.Models</code>","text":"<p>         Bases: <code>Flow</code></p> Source code in <code>src/chromatinhd/models/diff/model/cutnf.py</code> <pre><code>class Models(Flow):\n    n_models = Stored()\n\n    @property\n    def models_path(self):\n        path = self.path / \"models\"\n        path.mkdir(exist_ok=True)\n        return path\n\n    def train_models(self, fragments, clustering, folds, device=None, n_epochs=30, **kwargs):\n\"\"\"\n        Trains the models\n\n        Parameters:\n            fragments:\n                Fragments object\n        \"\"\"\n        self.n_models = len(folds)\n        for fold_ix, fold in [(fold_ix, fold) for fold_ix, fold in enumerate(folds)]:\n            desired_outputs = [self.models_path / (\"model_\" + str(fold_ix) + \".pkl\")]\n            force = False\n            if not all([desired_output.exists() for desired_output in desired_outputs]):\n                force = True\n\n            if force:\n                model = Model(fragments, clustering, **kwargs)\n                model.train_model(fragments, clustering, fold, device=device, n_epochs=n_epochs)\n\n                model = model.to(\"cpu\")\n\n                pickle.dump(\n                    model,\n                    open(self.models_path / (\"model_\" + str(fold_ix) + \".pkl\"), \"wb\"),\n                )\n\n    def __getitem__(self, ix):\n        return pickle.load((self.models_path / (\"model_\" + str(ix) + \".pkl\")).open(\"rb\"))\n\n    def __len__(self):\n        return self.n_models\n\n    def __iter__(self):\n        for ix in range(len(self)):\n            yield self[ix]\n</code></pre>"},{"location":"reference/models/diff/model/#chromatinhd.models.diff.model.cutnf.Models.train_models","title":"<code>train_models(fragments, clustering, folds, device=None, n_epochs=30, **kwargs)</code>","text":"<p>Trains the models</p> <p>Parameters:</p> Name Type Description Default <code>fragments</code> <p>Fragments object</p> required Source code in <code>src/chromatinhd/models/diff/model/cutnf.py</code> <pre><code>def train_models(self, fragments, clustering, folds, device=None, n_epochs=30, **kwargs):\n\"\"\"\n    Trains the models\n\n    Parameters:\n        fragments:\n            Fragments object\n    \"\"\"\n    self.n_models = len(folds)\n    for fold_ix, fold in [(fold_ix, fold) for fold_ix, fold in enumerate(folds)]:\n        desired_outputs = [self.models_path / (\"model_\" + str(fold_ix) + \".pkl\")]\n        force = False\n        if not all([desired_output.exists() for desired_output in desired_outputs]):\n            force = True\n\n        if force:\n            model = Model(fragments, clustering, **kwargs)\n            model.train_model(fragments, clustering, fold, device=device, n_epochs=n_epochs)\n\n            model = model.to(\"cpu\")\n\n            pickle.dump(\n                model,\n                open(self.models_path / (\"model_\" + str(fold_ix) + \".pkl\"), \"wb\"),\n            )\n</code></pre>"},{"location":"reference/models/diff/plot/","title":"Plot","text":""},{"location":"reference/models/diff/plot/#chromatinhd.models.diff.plot","title":"<code>chromatinhd.models.diff.plot</code>","text":""},{"location":"reference/models/pred/interpret/","title":"Interpret","text":""},{"location":"reference/models/pred/interpret/#chromatinhd.models.pred.interpret.RegionMultiWindow","title":"<code>chromatinhd.models.pred.interpret.RegionMultiWindow</code>","text":"<p>         Bases: <code>chd.flow.Flow</code></p> <p>Interpret a pred model positionally by censoring windows of across multiple window sizes.</p> Source code in <code>src/chromatinhd/models/pred/interpret/regionmultiwindow.py</code> <pre><code>class RegionMultiWindow(chd.flow.Flow):\n\"\"\"\n    Interpret a *pred* model positionally by censoring windows of across multiple window sizes.\n    \"\"\"\n\n    design = chd.flow.Stored()\n\"\"\"\n    The design of the censoring windows.\n    \"\"\"\n\n    regions = chd.flow.Stored(default=set)\n\"\"\"\n    The regions that have been scored.\n    \"\"\"\n\n    scores = chd.flow.SparseDataset()\n    interpolation = chd.flow.SparseDataset()\n    censorer = Stored()\n\n    @classmethod\n    def create(\n        cls, folds, transcriptome, fragments, censorer, path=None, phases=None, overwrite=False\n    ) -&gt; RegionMultiWindow:\n        self = super().create(path, reset=overwrite)\n\n        if phases is None:\n            # phases = [\"train\", \"validation\", \"test\"]\n            phases = [\"validation\", \"test\"]\n\n        regions = fragments.regions.var.index\n\n        coords_pointed = {\n            regions.name: regions,\n            \"fold\": pd.Index(range(len(folds)), name=\"fold\"),\n            \"phase\": pd.Index(phases, name=\"phase\"),\n        }\n        coords_fixed = {\n            censorer.design.index.name: censorer.design.index[1:],\n        }\n\n        self.censorer = censorer\n\n        design_censorer = censorer.design\n\n        if not self.o.scores.exists(self):\n            self.scores = chd.sparse.SparseDataset.create(\n                self.path / \"scores\",\n                variables={\n                    \"deltacor\": {\n                        \"dimensions\": (regions.name, \"fold\", \"phase\", design_censorer.index.name),\n                        \"dtype\": np.float32,\n                    },\n                    \"lost\": {\n                        \"dimensions\": (regions.name, \"fold\", \"phase\", design_censorer.index.name),\n                        \"dtype\": np.float32,\n                    },\n                    \"effect\": {\n                        \"dimensions\": (regions.name, \"fold\", \"phase\", design_censorer.index.name),\n                        \"dtype\": np.float32,\n                    },\n                    \"scored\": {\n                        \"dimensions\": (regions.name, \"fold\"),\n                        \"dtype\": bool,\n                        \"sparse\": False,\n                    },\n                },\n                coords_pointed=coords_pointed,\n                coords_fixed=coords_fixed,\n            )\n\n        if not self.o.interpolation.exists(self):\n            positions_oi = np.arange(\n                self.design[\"window_start\"].min(),\n                self.design[\"window_end\"].max() + 1,\n                10,\n            )\n\n            self.design_interpolation = pd.DataFrame(\n                {\n                    \"position\": positions_oi,\n                }\n            ).set_index(\"position\")\n\n            self.interpolation = chd.sparse.SparseDataset.create(\n                self.path / \"interpolation\",\n                variables={\n                    \"deltacor\": {\n                        \"dimensions\": (regions.name, self.design_interpolation.index.name),\n                        \"dtype\": np.float32,\n                    },\n                    \"lost\": {\"dimensions\": (regions.name, self.design_interpolation.index.name), \"dtype\": np.float32},\n                    \"effect\": {\"dimensions\": (regions.name, self.design_interpolation.index.name), \"dtype\": np.float32},\n                    \"interpolated\": {\"dimensions\": (regions.name,), \"dtype\": bool, \"sparse\": False},\n                },\n                coords_pointed={regions.name: regions},\n                coords_fixed={\"position\": self.design_interpolation.index},\n            )\n\n        return self\n\n    def score(\n        self,\n        models,\n        folds=None,\n        fragments=None,\n        transcriptome=None,\n        regions=None,\n        force=False,\n        device=None,\n        min_fragments=3,\n    ):\n        force_ = force\n\n        if regions is None:\n            # get regions from models\n            if models.regions_oi is not None:\n                regions = models.regions_oi\n            else:\n                regions = self.scores.coords_pointed[list(self.scores.coords_pointed)[0]]\n\n        if folds is None:\n            folds = models.folds\n\n        pbar = tqdm.tqdm(regions, leave=False)\n\n        for region in pbar:\n            pbar.set_description(region)\n\n            for fold_ix, fold in enumerate(folds):\n                force = force_\n                if not self.scores[\"scored\"][region, fold_ix]:\n                    force = True\n\n                if force:\n                    model_name = f\"{region}_{fold_ix}\"\n                    if model_name not in models:\n                        continue\n\n                    pbar.set_description(region + \" \" + str(fold_ix))\n\n                    model = models[model_name]\n                    predicted, expected, n_fragments = model.get_prediction_censored(\n                        fragments=fragments,\n                        transcriptome=transcriptome,\n                        censorer=self.censorer,\n                        cell_ixs=np.concatenate([fold[\"cells_validation\"], fold[\"cells_test\"]]),\n                        regions=[region],\n                        device=device,\n                        min_fragments=min_fragments,\n                    )\n\n                    # select 1st region, given that we're working with one region anyway\n                    predicted = predicted[..., 0]\n                    expected = expected[..., 0]\n                    n_fragments = n_fragments[..., 0]\n\n                    cor = chd.utils.paircor(predicted, expected, dim=-1)\n                    deltacor = cor[1:] - cor[0]\n\n                    lost = (n_fragments[0] - n_fragments[1:]).mean(-1)\n\n                    effect = (predicted[0] - predicted[1:]).mean(-1)\n\n                    self.scores[\"deltacor\"][region, fold_ix, \"test\"] = deltacor\n                    self.scores[\"lost\"][region, fold_ix, \"test\"] = lost\n                    self.scores[\"effect\"][region, fold_ix, \"test\"] = effect\n                    self.scores[\"scored\"][region, fold_ix] = True\n\n        return self\n\n    def score2(\n        self,\n        models,\n        folds=None,\n        fragments=None,\n        transcriptome=None,\n        regions=None,\n        force=False,\n        device=None,\n        min_fragments=3,\n    ):\n        force_ = force\n\n        if regions is None:\n            # get regions from models\n            if models.regions_oi is not None:\n                regions = models.regions_oi\n            else:\n                regions = self.scores.coords_pointed[list(self.scores.coords_pointed)[0]]\n\n        if folds is None:\n            folds = models.folds\n\n        pbar = tqdm.tqdm(regions, leave=False)\n\n        for region in pbar:\n            pbar.set_description(region)\n\n            for fold_ix, fold in enumerate(folds):\n                force = force_\n                if not self.scores[\"scored\"][region, fold_ix]:\n                    force = True\n\n                if force:\n                    model_name = f\"{region}_{fold_ix}\"\n                    if model_name not in models:\n                        continue\n\n                    pbar.set_description(region + \" \" + str(fold_ix))\n\n                    model = models[model_name]\n                    deltacor, lost, effect = model.get_performance_censored(\n                        fragments=fragments,\n                        transcriptome=transcriptome,\n                        censorer=self.censorer,\n                        cell_ixs=np.concatenate([fold[\"cells_validation\"], fold[\"cells_test\"]]),\n                        regions=[region],\n                        device=device,\n                        min_fragments=min_fragments,\n                    )\n\n                    self.scores[\"deltacor\"][region, fold_ix, \"test\"] = deltacor\n                    self.scores[\"lost\"][region, fold_ix, \"test\"] = lost\n                    self.scores[\"effect\"][region, fold_ix, \"test\"] = effect\n                    self.scores[\"scored\"][region, fold_ix] = True\n\n        return self\n\n    @property\n    def design(self):\n        return self.censorer.design.iloc[1:]\n\n    def interpolate(self, regions=None, force=False, pbar=True):\n        force_ = force\n\n        if regions is None:\n            regions = self.scores.coords_pointed[list(self.scores.coords_pointed)[0]]\n\n        progress = regions\n        if pbar:\n            progress = tqdm.tqdm(progress, leave=False)\n\n        for region in progress:\n            if pbar:\n                progress.set_description(region)\n\n            if not all([self.scores[\"scored\"][region, fold_ix] for fold_ix in self.scores.coords_pointed[\"fold\"]]):\n                continue\n\n            force = force_\n            if not self.interpolation[\"interpolated\"][region]:\n                force = True\n\n            if force:\n                deltacor, lost, effect = self._interpolate(region)\n                self.interpolation[\"deltacor\"][region] = deltacor\n                self.interpolation[\"effect\"][region] = effect\n                self.interpolation[\"lost\"][region] = lost\n                self.interpolation[\"interpolated\"][region] = True\n\n        return self\n\n    def _interpolate(self, region):\n        deltacors = []\n        effects = []\n        losts = []\n        for fold_ix in self.scores.coords_pointed[\"fold\"]:\n            deltacors.append(self.scores[\"deltacor\"][region, fold_ix, \"test\"])\n            effects.append(self.scores[\"effect\"][region, fold_ix, \"test\"])\n            losts.append(self.scores[\"lost\"][region, fold_ix, \"test\"])\n        deltacors = np.stack(deltacors)\n        effects = np.stack(effects)\n        losts = np.stack(losts)\n\n        scores_statistical = []\n        for i in range(deltacors.shape[1]):\n            if deltacors.shape[0] &gt; 1:\n                scores_statistical.append(scipy.stats.ttest_1samp(deltacors[:, i], 0, alternative=\"less\").pvalue)\n            else:\n                scores_statistical.append(0.0)\n        scores_statistical = pd.DataFrame({\"pvalue\": scores_statistical})\n        scores_statistical[\"qval\"] = fdr_nan(scores_statistical[\"pvalue\"])\n\n        plotdata = pd.DataFrame(\n            {\n                \"deltacor\": deltacors.mean(0),\n                \"effect\": effects.mean(0),\n                \"lost\": losts.mean(0),\n            },\n            index=self.design.index,\n        )\n        plotdata = self.design.join(plotdata)\n\n        plotdata[\"qval\"] = scores_statistical[\"qval\"].values\n\n        window_sizes_info = pd.DataFrame({\"window_size\": self.design[\"window_size\"].unique()}).set_index(\"window_size\")\n        window_sizes_info[\"ix\"] = np.arange(len(window_sizes_info))\n\n        # interpolate\n        positions_oi = np.arange(\n            self.design[\"window_start\"].min(),\n            self.design[\"window_end\"].max() + 1,\n            10,\n        )\n\n        deltacor_interpolated = np.zeros((len(window_sizes_info), len(positions_oi)))\n        lost_interpolated = np.zeros((len(window_sizes_info), len(positions_oi)))\n        effect_interpolated = np.zeros((len(window_sizes_info), len(positions_oi)))\n        for window_size, window_size_info in window_sizes_info.iterrows():\n            plotdata_oi = plotdata.query(\"window_size == @window_size\")\n            x = plotdata_oi[\"window_mid\"].values.copy()\n            y = plotdata_oi[\"deltacor\"].values.copy()\n            # y[(plotdata_oi[\"qval\"] &gt; 0.2) | pd.isnull(plotdata_oi[\"qval\"])] = 0.0\n            deltacor_interpolated_ = np.clip(\n                np.interp(positions_oi, x, y) / window_size * 1000,\n                -np.inf,\n                0,\n            )\n            deltacor_interpolated[window_size_info[\"ix\"], :] = deltacor_interpolated_\n\n            lost_interpolated_ = (\n                np.interp(positions_oi, plotdata_oi[\"window_mid\"], plotdata_oi[\"lost\"]) / window_size * 1000\n            )\n            lost_interpolated[window_size_info[\"ix\"], :] = lost_interpolated_\n\n            effect_interpolated_ = (\n                np.interp(\n                    positions_oi,\n                    plotdata_oi[\"window_mid\"],\n                    plotdata_oi[\"effect\"],\n                )\n                / window_size\n                * 1000\n            )\n            effect_interpolated[window_size_info[\"ix\"], :] = effect_interpolated_\n        return deltacor_interpolated.mean(0), lost_interpolated.mean(0), effect_interpolated.mean(0)\n\n    def get_plotdata(self, region):\n        if not self.interpolation[\"interpolated\"][region]:\n            raise ValueError(f\"Region {region} not interpolated. Run .interpolate() first.\")\n\n        plotdata = self.interpolation.sel_xr(region, variables=[\"deltacor\", \"lost\", \"effect\"]).to_pandas()\n\n        return plotdata\n\n    def get_scoring_path(self, region):\n        path = self.path / f\"{region}\"\n        path.mkdir(parents=True, exist_ok=True)\n        return path\n\n    def select_windows(self, region_id, max_merge_distance=500, min_length=50, padding=500, lost_cutoff=0.5):\n        from scipy.ndimage import convolve\n\n        def spread_true(arr, width=5):\n            kernel = np.ones(width, dtype=bool)\n            result = convolve(arr, kernel, mode=\"constant\", cval=False)\n            result = result != 0\n            return result\n\n        plotdata = self.get_plotdata(region_id)\n        selection = pd.DataFrame({\"chosen\": (plotdata[\"lost\"] &gt; lost_cutoff)})\n\n        # add padding\n        step = plotdata.index.get_level_values(\"position\")[1] - plotdata.index.get_level_values(\"position\")[0]\n        k_padding = int(padding // step)\n        selection[\"chosen\"] = spread_true(selection[\"chosen\"], width=k_padding)\n\n        # select all contiguous regions where chosen is true\n        selection[\"selection\"] = selection[\"chosen\"].cumsum()\n\n        regions = pd.DataFrame(\n            {\n                \"start\": selection.index[\n                    (np.diff(np.pad(selection[\"chosen\"], (1, 1), constant_values=False).astype(int)) == 1)[:-1]\n                ],\n                \"end\": selection.index[\n                    (np.diff(np.pad(selection[\"chosen\"], (1, 1), constant_values=False).astype(int)) == -1)[1:]\n                ],\n            }\n        )\n\n        # merge regions that are close to each other\n        regions[\"distance_to_next\"] = regions[\"start\"].shift(-1) - regions[\"end\"]\n\n        regions[\"merge\"] = (regions[\"distance_to_next\"] &lt; max_merge_distance).fillna(False)\n        regions[\"group\"] = (~regions[\"merge\"]).cumsum().shift(1).fillna(0).astype(int)\n        regions = (\n            regions.groupby(\"group\")\n            .agg({\"start\": \"min\", \"end\": \"max\", \"distance_to_next\": \"last\"})\n            .reset_index(drop=True)\n        )\n\n        # filter on length\n        regions[\"length\"] = regions[\"end\"] - regions[\"start\"]\n        regions = regions[regions[\"length\"] &gt; min_length]\n\n        return regions\n\n    def extract_predictive_windows(self, region_id=None, deltacor_cutoff=-0.001):\n\"\"\"\n        Extract predictive windows for one (or more) regions\n        \"\"\"\n\n        feature_name = list(self.scores.coords_pointed.keys())[0]\n\n        if region_id is None:\n            region_id = self.scores.coords_pointed[feature_name]\n\n        if isinstance(region_id, str):\n            region_id = [region_id]\n\n        extracted = []\n\n        for region_id in region_id:\n            if self.interpolation[\"interpolated\"][region_id]:\n                plotdata = self.get_plotdata(region_id)\n                plotdata[\"chosen\"] = (plotdata[\"deltacor\"] &lt; deltacor_cutoff) &amp; (plotdata[\"effect\"] &gt; 0)\n\n                extracted_region_positive = pd.DataFrame(\n                    {\n                        \"start\": plotdata.index[\n                            (np.diff(np.pad(plotdata[\"chosen\"], (1, 1), constant_values=False).astype(int)) == 1)[:-1]\n                        ].astype(int),\n                        \"end\": plotdata.index[\n                            (np.diff(np.pad(plotdata[\"chosen\"], (1, 1), constant_values=False).astype(int)) == -1)[1:]\n                        ].astype(int),\n                        feature_name: region_id,\n                        \"effect_direction\": +1,\n                    }\n                )\n                extracted.append(extracted_region_positive)\n\n                plotdata[\"chosen\"] = (plotdata[\"deltacor\"] &lt; deltacor_cutoff) &amp; (plotdata[\"effect\"] &lt; 0)\n                extracted_region_negative = pd.DataFrame(\n                    {\n                        \"start\": plotdata.index[\n                            (np.diff(np.pad(plotdata[\"chosen\"], (1, 1), constant_values=False).astype(int)) == 1)[:-1]\n                        ],\n                        \"end\": plotdata.index[\n                            (np.diff(np.pad(plotdata[\"chosen\"], (1, 1), constant_values=False).astype(int)) == -1)[1:]\n                        ],\n                        feature_name: region_id,\n                        \"effect_direction\": -1,\n                    }\n                )\n                extracted.append(extracted_region_negative)\n\n        return pd.concat(extracted)\n</code></pre>"},{"location":"reference/models/pred/interpret/#chromatinhd.models.pred.interpret.regionmultiwindow.RegionMultiWindow.regions","title":"<code>regions = chd.flow.Stored(default=set)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The regions that have been scored.</p>"},{"location":"reference/models/pred/interpret/#chromatinhd.models.pred.interpret.regionmultiwindow.RegionMultiWindow.extract_predictive_windows","title":"<code>extract_predictive_windows(region_id=None, deltacor_cutoff=-0.001)</code>","text":"<p>Extract predictive windows for one (or more) regions</p> Source code in <code>src/chromatinhd/models/pred/interpret/regionmultiwindow.py</code> <pre><code>def extract_predictive_windows(self, region_id=None, deltacor_cutoff=-0.001):\n\"\"\"\n    Extract predictive windows for one (or more) regions\n    \"\"\"\n\n    feature_name = list(self.scores.coords_pointed.keys())[0]\n\n    if region_id is None:\n        region_id = self.scores.coords_pointed[feature_name]\n\n    if isinstance(region_id, str):\n        region_id = [region_id]\n\n    extracted = []\n\n    for region_id in region_id:\n        if self.interpolation[\"interpolated\"][region_id]:\n            plotdata = self.get_plotdata(region_id)\n            plotdata[\"chosen\"] = (plotdata[\"deltacor\"] &lt; deltacor_cutoff) &amp; (plotdata[\"effect\"] &gt; 0)\n\n            extracted_region_positive = pd.DataFrame(\n                {\n                    \"start\": plotdata.index[\n                        (np.diff(np.pad(plotdata[\"chosen\"], (1, 1), constant_values=False).astype(int)) == 1)[:-1]\n                    ].astype(int),\n                    \"end\": plotdata.index[\n                        (np.diff(np.pad(plotdata[\"chosen\"], (1, 1), constant_values=False).astype(int)) == -1)[1:]\n                    ].astype(int),\n                    feature_name: region_id,\n                    \"effect_direction\": +1,\n                }\n            )\n            extracted.append(extracted_region_positive)\n\n            plotdata[\"chosen\"] = (plotdata[\"deltacor\"] &lt; deltacor_cutoff) &amp; (plotdata[\"effect\"] &lt; 0)\n            extracted_region_negative = pd.DataFrame(\n                {\n                    \"start\": plotdata.index[\n                        (np.diff(np.pad(plotdata[\"chosen\"], (1, 1), constant_values=False).astype(int)) == 1)[:-1]\n                    ],\n                    \"end\": plotdata.index[\n                        (np.diff(np.pad(plotdata[\"chosen\"], (1, 1), constant_values=False).astype(int)) == -1)[1:]\n                    ],\n                    feature_name: region_id,\n                    \"effect_direction\": -1,\n                }\n            )\n            extracted.append(extracted_region_negative)\n\n    return pd.concat(extracted)\n</code></pre>"},{"location":"reference/models/pred/interpret/#chromatinhd.models.pred.interpret.RegionPairWindow","title":"<code>chromatinhd.models.pred.interpret.RegionPairWindow</code>","text":"<p>         Bases: <code>chd.flow.Flow</code></p> <p>Interpret a pred model positionally by censoring windows and comparing the decrease in predictivity per cell between pairs of windows</p> Source code in <code>src/chromatinhd/models/pred/interpret/regionpairwindow.py</code> <pre><code>class RegionPairWindow(chd.flow.Flow):\n\"\"\"\n    Interpret a *pred* model positionally by censoring windows and comparing\n    the decrease in predictivity per cell between pairs of windows\n    \"\"\"\n\n    design = chd.flow.Stored()\n\n    scores = StoredDict(Dataset)\n    interaction = StoredDict(DataArray)\n\n    def score(\n        self,\n        models: Models,\n        censorer,\n        regions: Optional[List] = None,\n        folds=None,\n        transcriptome=None,\n        fragments=None,\n        force=False,\n        device=None,\n    ):\n\"\"\"\n        Score the models\n\n        Parameters:\n            fragments:\n                the fragments\n            transcriptome:\n                the transcriptome\n            models:\n                the models\n            folds:\n                the folds\n            regions:\n                which regions to score, defaults to all\n\n        \"\"\"\n        force_ = force\n        design = censorer.design.iloc[1:].copy()\n        self.design = design\n\n        if regions is None:\n            regions = fragments.var.index\n\n        if device is None:\n            device = get_default_device()\n\n        if folds is None:\n            folds = models.folds\n\n        pbar = tqdm.tqdm(regions, leave=False)\n        for region in pbar:\n            pbar.set_description(region)\n\n            force = force_\n\n            if region not in self.scores:\n                force = True\n\n            deltacor_folds = []\n            copredictivity_folds = []\n            lost_folds = []\n\n            if force:\n                for fold_ix, fold in enumerate(folds):\n                    model_name = f\"{region}_{fold_ix}\"\n                    if model_name not in models:\n                        continue\n                        raise ValueError(f\"Model {model_name} not found\")\n\n                    pbar.set_description(region + \" \" + str(fold_ix))\n\n                    model = models[model_name]\n                    predicted, expected, n_fragments = model.get_prediction_censored(\n                        fragments=fragments,\n                        transcriptome=transcriptome,\n                        censorer=censorer,\n                        cell_ixs=np.concatenate([fold[\"cells_validation\"], fold[\"cells_test\"]]),\n                        regions=[region],\n                        device=device,\n                    )\n\n                    # select 1st region, given that we're working with one region anyway\n                    predicted = predicted[..., 0]\n                    expected = expected[..., 0]\n                    n_fragments = n_fragments[..., 0]\n\n                    # calculate delta cor per cell\n                    predicted_censored = predicted[1:]\n                    predicted_full = predicted[0][None, ...]\n                    predicted_full_norm = zscore(predicted_full, 1)\n                    predicted_censored_norm = zscore_relative(predicted_censored, predicted_full, 1)\n\n                    expected_norm = zscore(expected[None, ...], 1)\n\n                    celldeltacor = -np.abs(predicted_censored_norm - expected_norm) - -np.abs(\n                        predicted_full_norm - expected_norm\n                    )\n                    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n                        copredictivity = np.corrcoef(celldeltacor)\n                    copredictivity[np.isnan(copredictivity)] = 0.0\n\n                    copredictivity_folds.append(copredictivity)\n\n                    cor = chd.utils.paircor(predicted, expected, dim=-1)\n                    deltacor = cor[1:] - cor[0]\n\n                    lost = (n_fragments[0] - n_fragments[1:]).mean(-1)\n\n                    deltacor_folds.append(deltacor)\n                    lost_folds.append(lost)\n\n                if len(lost_folds) == 0:\n                    continue\n\n                lost_folds = np.stack(lost_folds, 0)\n                deltacor_folds = np.stack(deltacor_folds, 0)\n                copredictivity_folds = np.stack(copredictivity_folds, 0)\n\n                result = xr.Dataset(\n                    {\n                        \"deltacor\": xr.DataArray(\n                            deltacor_folds,\n                            coords=[\n                                (\"fold\", np.arange(len(folds))),\n                                (\"window\", design.index),\n                            ],\n                        ),\n                        \"lost\": xr.DataArray(\n                            lost_folds,\n                            coords=[\n                                (\"fold\", np.arange(len(folds))),\n                                (\"window\", design.index),\n                            ],\n                        ),\n                    }\n                )\n\n                windows_oi = lost_folds.mean(0) &gt; 1e-3\n                windows_oi = np.ones(len(design), dtype=bool)\n\n                interaction = xr.DataArray(\n                    copredictivity_folds[:, windows_oi][:, :, windows_oi],\n                    coords=[\n                        (\"fold\", np.arange(len(folds))),\n                        (\"window1\", design.index[windows_oi]),\n                        (\"window2\", design.index[windows_oi]),\n                    ],\n                )\n\n                self.scores[region] = result\n                self.interaction[region] = interaction\n\n        return self\n\n    def get_plotdata(self, region, windows=None):\n\"\"\"\n        Get plotdata for a region\n        \"\"\"\n\n        if windows is None:\n            windows = self.design\n        else:\n            x = self.design[[\"window_start\", \"window_end\"]].values\n            y = windows[[\"start\", \"end\"]].values\n\n            windows = self.design.loc[chromatinhd.utils.intervals.interval_contains_inclusive(x, y)]\n\n        plotdata_windows = self.scores[region].mean(\"fold\").to_dataframe()\n        plotdata_interaction = self.interaction[region].mean(\"fold\").to_pandas().unstack().to_frame(\"cor\")\n\n        plotdata_interaction = (\n            plotdata_interaction.copy()\n            .join(plotdata_windows.rename(columns=lambda x: x + \"1\"), on=\"window1\")\n            .join(plotdata_windows.rename(columns=lambda x: x + \"2\"), on=\"window2\")\n        )\n\n        # make plotdata, making sure we have all window combinations, otherwise nan\n        plotdata = (\n            pd.DataFrame(itertools.combinations(windows.index, 2), columns=[\"window1\", \"window2\"])\n            .set_index([\"window1\", \"window2\"])\n            .join(plotdata_interaction)\n        )\n        plotdata.loc[np.isnan(plotdata[\"cor\"]), \"cor\"] = 0.0\n        plotdata[\"dist\"] = (\n            windows.loc[plotdata.index.get_level_values(\"window2\"), \"window_mid\"].values\n            - windows.loc[plotdata.index.get_level_values(\"window1\"), \"window_mid\"].values\n        )\n\n        plotdata.loc[plotdata[\"dist\"] &lt; 1000, \"cor\"] = 0.0\n\n        plotdata = plotdata.query(\"dist &gt; 0\")\n\n        return plotdata\n</code></pre>"},{"location":"reference/models/pred/interpret/#chromatinhd.models.pred.interpret.regionpairwindow.RegionPairWindow.get_plotdata","title":"<code>get_plotdata(region, windows=None)</code>","text":"<p>Get plotdata for a region</p> Source code in <code>src/chromatinhd/models/pred/interpret/regionpairwindow.py</code> <pre><code>def get_plotdata(self, region, windows=None):\n\"\"\"\n    Get plotdata for a region\n    \"\"\"\n\n    if windows is None:\n        windows = self.design\n    else:\n        x = self.design[[\"window_start\", \"window_end\"]].values\n        y = windows[[\"start\", \"end\"]].values\n\n        windows = self.design.loc[chromatinhd.utils.intervals.interval_contains_inclusive(x, y)]\n\n    plotdata_windows = self.scores[region].mean(\"fold\").to_dataframe()\n    plotdata_interaction = self.interaction[region].mean(\"fold\").to_pandas().unstack().to_frame(\"cor\")\n\n    plotdata_interaction = (\n        plotdata_interaction.copy()\n        .join(plotdata_windows.rename(columns=lambda x: x + \"1\"), on=\"window1\")\n        .join(plotdata_windows.rename(columns=lambda x: x + \"2\"), on=\"window2\")\n    )\n\n    # make plotdata, making sure we have all window combinations, otherwise nan\n    plotdata = (\n        pd.DataFrame(itertools.combinations(windows.index, 2), columns=[\"window1\", \"window2\"])\n        .set_index([\"window1\", \"window2\"])\n        .join(plotdata_interaction)\n    )\n    plotdata.loc[np.isnan(plotdata[\"cor\"]), \"cor\"] = 0.0\n    plotdata[\"dist\"] = (\n        windows.loc[plotdata.index.get_level_values(\"window2\"), \"window_mid\"].values\n        - windows.loc[plotdata.index.get_level_values(\"window1\"), \"window_mid\"].values\n    )\n\n    plotdata.loc[plotdata[\"dist\"] &lt; 1000, \"cor\"] = 0.0\n\n    plotdata = plotdata.query(\"dist &gt; 0\")\n\n    return plotdata\n</code></pre>"},{"location":"reference/models/pred/interpret/#chromatinhd.models.pred.interpret.regionpairwindow.RegionPairWindow.score","title":"<code>score(models, censorer, regions=None, folds=None, transcriptome=None, fragments=None, force=False, device=None)</code>","text":"<p>Score the models</p> <p>Parameters:</p> Name Type Description Default <code>fragments</code> <p>the fragments</p> <code>None</code> <code>transcriptome</code> <p>the transcriptome</p> <code>None</code> <code>models</code> <code>Models</code> <p>the models</p> required <code>folds</code> <p>the folds</p> <code>None</code> <code>regions</code> <code>Optional[List]</code> <p>which regions to score, defaults to all</p> <code>None</code> Source code in <code>src/chromatinhd/models/pred/interpret/regionpairwindow.py</code> <pre><code>def score(\n    self,\n    models: Models,\n    censorer,\n    regions: Optional[List] = None,\n    folds=None,\n    transcriptome=None,\n    fragments=None,\n    force=False,\n    device=None,\n):\n\"\"\"\n    Score the models\n\n    Parameters:\n        fragments:\n            the fragments\n        transcriptome:\n            the transcriptome\n        models:\n            the models\n        folds:\n            the folds\n        regions:\n            which regions to score, defaults to all\n\n    \"\"\"\n    force_ = force\n    design = censorer.design.iloc[1:].copy()\n    self.design = design\n\n    if regions is None:\n        regions = fragments.var.index\n\n    if device is None:\n        device = get_default_device()\n\n    if folds is None:\n        folds = models.folds\n\n    pbar = tqdm.tqdm(regions, leave=False)\n    for region in pbar:\n        pbar.set_description(region)\n\n        force = force_\n\n        if region not in self.scores:\n            force = True\n\n        deltacor_folds = []\n        copredictivity_folds = []\n        lost_folds = []\n\n        if force:\n            for fold_ix, fold in enumerate(folds):\n                model_name = f\"{region}_{fold_ix}\"\n                if model_name not in models:\n                    continue\n                    raise ValueError(f\"Model {model_name} not found\")\n\n                pbar.set_description(region + \" \" + str(fold_ix))\n\n                model = models[model_name]\n                predicted, expected, n_fragments = model.get_prediction_censored(\n                    fragments=fragments,\n                    transcriptome=transcriptome,\n                    censorer=censorer,\n                    cell_ixs=np.concatenate([fold[\"cells_validation\"], fold[\"cells_test\"]]),\n                    regions=[region],\n                    device=device,\n                )\n\n                # select 1st region, given that we're working with one region anyway\n                predicted = predicted[..., 0]\n                expected = expected[..., 0]\n                n_fragments = n_fragments[..., 0]\n\n                # calculate delta cor per cell\n                predicted_censored = predicted[1:]\n                predicted_full = predicted[0][None, ...]\n                predicted_full_norm = zscore(predicted_full, 1)\n                predicted_censored_norm = zscore_relative(predicted_censored, predicted_full, 1)\n\n                expected_norm = zscore(expected[None, ...], 1)\n\n                celldeltacor = -np.abs(predicted_censored_norm - expected_norm) - -np.abs(\n                    predicted_full_norm - expected_norm\n                )\n                with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n                    copredictivity = np.corrcoef(celldeltacor)\n                copredictivity[np.isnan(copredictivity)] = 0.0\n\n                copredictivity_folds.append(copredictivity)\n\n                cor = chd.utils.paircor(predicted, expected, dim=-1)\n                deltacor = cor[1:] - cor[0]\n\n                lost = (n_fragments[0] - n_fragments[1:]).mean(-1)\n\n                deltacor_folds.append(deltacor)\n                lost_folds.append(lost)\n\n            if len(lost_folds) == 0:\n                continue\n\n            lost_folds = np.stack(lost_folds, 0)\n            deltacor_folds = np.stack(deltacor_folds, 0)\n            copredictivity_folds = np.stack(copredictivity_folds, 0)\n\n            result = xr.Dataset(\n                {\n                    \"deltacor\": xr.DataArray(\n                        deltacor_folds,\n                        coords=[\n                            (\"fold\", np.arange(len(folds))),\n                            (\"window\", design.index),\n                        ],\n                    ),\n                    \"lost\": xr.DataArray(\n                        lost_folds,\n                        coords=[\n                            (\"fold\", np.arange(len(folds))),\n                            (\"window\", design.index),\n                        ],\n                    ),\n                }\n            )\n\n            windows_oi = lost_folds.mean(0) &gt; 1e-3\n            windows_oi = np.ones(len(design), dtype=bool)\n\n            interaction = xr.DataArray(\n                copredictivity_folds[:, windows_oi][:, :, windows_oi],\n                coords=[\n                    (\"fold\", np.arange(len(folds))),\n                    (\"window1\", design.index[windows_oi]),\n                    (\"window2\", design.index[windows_oi]),\n                ],\n            )\n\n            self.scores[region] = result\n            self.interaction[region] = interaction\n\n    return self\n</code></pre>"},{"location":"reference/models/pred/model/","title":"Model","text":""},{"location":"reference/models/pred/model/#additive","title":"Additive","text":""},{"location":"reference/models/pred/model/#chromatinhd.models.pred.model.additive.Model","title":"<code>chromatinhd.models.pred.model.additive.Model</code>","text":"<p>         Bases: <code>FlowModel</code></p> <p>Predicting region expression from raw fragments using an additive model across fragments from the same cell</p> <p>Parameters:</p> Name Type Description Default <code>n_regions</code> <p>the number of regions</p> required <code>dummy</code> <code>bool</code> <p>whether to use a dummy model that just counts fragments.</p> <code>False</code> <code>n_frequencies</code> <code>int</code> <p>the number of frequencies to use for sine encoding</p> <code>50</code> <code>reduce</code> <code>str</code> <p>the reduction to use for pooling fragments across regions and cells</p> <code>'sum'</code> <code>nonlinear</code> <code>bool</code> <p>whether to use a non-linear activation function</p> <code>True</code> <code>n_embedding_dimensions</code> <code>int</code> <p>the number of embedding dimensions</p> <code>10</code> <code>dropout_rate</code> <code>float</code> <p>the dropout rate</p> <code>0.0</code> Source code in <code>src/chromatinhd/models/pred/model/additive.py</code> <pre><code>class Model(FlowModel):\n\"\"\"\n    Predicting region expression from raw fragments using an additive model across fragments from the same cell\n\n    Parameters:\n        n_regions:\n            the number of regions\n        dummy:\n            whether to use a dummy model that just counts fragments.\n        n_frequencies:\n            the number of frequencies to use for sine encoding\n        reduce:\n            the reduction to use for pooling fragments across regions and cells\n        nonlinear:\n            whether to use a non-linear activation function\n        n_embedding_dimensions:\n            the number of embedding dimensions\n        dropout_rate:\n            the dropout rate\n    \"\"\"\n\n    transcriptome = Linked()\n\"\"\"The transcriptome\"\"\"\n\n    fragments = Linked()\n\"\"\"The fragments\"\"\"\n\n    fold = Stored()\n\"\"\"The cells used for training, test and validation\"\"\"\n\n    layer = Stored()\n\"\"\"The layer of the transcriptome\"\"\"\n\n    def __init__(\n        self,\n        path=None,\n        fragments: Fragments | None = None,\n        transcriptome: Transcriptome | None = None,\n        fold=None,\n        dummy: bool = False,\n        n_frequencies: int = 50,\n        reduce: str = \"sum\",\n        nonlinear: bool = True,\n        n_embedding_dimensions: int = 10,\n        dropout_rate: float = 0.0,\n        embedding_to_expression_initialization: str = \"default\",\n        layer=None,\n        reset=False,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(path=path, reset=reset, **kwargs)\n\n        if fragments is not None:\n            self.fragments = fragments\n        if transcriptome is not None:\n            self.transcriptome = transcriptome\n        if fold is not None:\n            self.fold = fold\n        if layer is not None:\n            self.layer = layer\n        elif not self.o.layer.exists(self):\n            if transcriptome is not None:\n                self.layer = list(self.transcriptome.layers.keys())[0]\n\n        if not self.o.state.exists(self):\n            assert fragments is not None\n            assert transcriptome is not None\n\n            n_regions = fragments.n_regions\n\n            if dummy is True:\n                self.fragment_embedder = FragmentEmbedderCounter()\n            else:\n                self.fragment_embedder = FragmentEmbedder(\n                    n_frequencies=n_frequencies,\n                    n_regions=n_regions,\n                    nonlinear=nonlinear,\n                    n_embedding_dimensions=n_embedding_dimensions,\n                    dropout_rate=dropout_rate,\n                )\n            self.embedding_region_pooler = EmbeddingGenePooler(reduce=reduce)\n            self.embedding_to_expression = EmbeddingToExpression(\n                n_regions=n_regions,\n                n_embedding_dimensions=self.fragment_embedder.n_embedding_dimensions,\n                initialization=embedding_to_expression_initialization,\n            )\n\n            n_regions = fragments.n_regions\n\n            if dummy is True:\n                self.fragment_embedder = FragmentEmbedderCounter()\n            else:\n                self.fragment_embedder = FragmentEmbedder(\n                    n_frequencies=n_frequencies,\n                    n_regions=n_regions,\n                    nonlinear=nonlinear,\n                    n_embedding_dimensions=n_embedding_dimensions,\n                    dropout_rate=dropout_rate,\n                )\n            self.embedding_region_pooler = EmbeddingGenePooler(reduce=reduce)\n            self.embedding_to_expression = EmbeddingToExpression(\n                n_regions=n_regions,\n                n_embedding_dimensions=self.fragment_embedder.n_embedding_dimensions,\n                initialization=embedding_to_expression_initialization,\n            )\n\n    def forward(self, data):\n\"\"\"\n        Make a prediction given a data object\n        \"\"\"\n        fragment_embedding = self.fragment_embedder(data.fragments.coordinates, data.fragments.regionmapping)\n        cell_region_embedding = self.embedding_region_pooler(\n            fragment_embedding,\n            data.fragments.local_cellxregion_ix,\n            data.minibatch.n_cells,\n            data.minibatch.n_regions,\n        )\n        expression_predicted = self.embedding_to_expression(cell_region_embedding, data.minibatch.regions_oi_torch)\n        return expression_predicted\n\n    def forward_loss(self, data):\n\"\"\"\n        Make a prediction and calculate the loss given a data object\n        \"\"\"\n        expression_predicted = self.forward(data)\n        expression_true = data.transcriptome.value\n        return paircor_loss(expression_predicted, expression_true)\n\n    def forward_region_loss(self, data):\n\"\"\"\n        Make a prediction and calculate the loss given a data object\n        \"\"\"\n        expression_predicted = self.forward(data)\n        expression_true = data.transcriptome.value\n        return region_paircor_loss(expression_predicted, expression_true)\n\n    def forward_multiple(self, data, fragments_oi, min_fragments=1):\n        fragment_embedding = self.fragment_embedder(data.fragments.coordinates, data.fragments.regionmapping)\n\n        total_n_fragments = torch.bincount(\n            data.fragments.local_cellxregion_ix,\n            minlength=data.minibatch.n_regions * data.minibatch.n_cells,\n        ).reshape((data.minibatch.n_cells, data.minibatch.n_regions))\n\n        total_cell_region_embedding = self.embedding_region_pooler.forward(\n            fragment_embedding,\n            data.fragments.local_cellxregion_ix,\n            data.minibatch.n_cells,\n            data.minibatch.n_regions,\n        )\n\n        total_expression_predicted = self.embedding_to_expression.forward(\n            total_cell_region_embedding, data.minibatch.regions_oi_torch\n        )\n\n        for fragments_oi_ in fragments_oi:\n            if (fragments_oi_ is not None) and ((~fragments_oi_).sum() &gt; min_fragments):\n                lost_fragments_oi = ~fragments_oi_\n                lost_local_cellxregion_ix = data.fragments.local_cellxregion_ix[lost_fragments_oi]\n                n_fragments = total_n_fragments - torch.bincount(\n                    lost_local_cellxregion_ix,\n                    minlength=data.minibatch.n_regions * data.minibatch.n_cells,\n                ).reshape((data.minibatch.n_cells, data.minibatch.n_regions))\n                cell_region_embedding = total_cell_region_embedding - self.embedding_region_pooler.forward(\n                    fragment_embedding[lost_fragments_oi],\n                    lost_local_cellxregion_ix,\n                    data.minibatch.n_cells,\n                    data.minibatch.n_regions,\n                )\n\n                expression_predicted = self.embedding_to_expression.forward(\n                    cell_region_embedding, data.minibatch.regions_oi_torch\n                )\n            else:\n                n_fragments = total_n_fragments\n                expression_predicted = total_expression_predicted\n\n            yield expression_predicted, n_fragments\n\n    def train_model(\n        self,\n        fold: list = None,\n        fragments: Fragments = None,\n        transcriptome: Transcriptome = None,\n        device=None,\n        lr=1e-3,\n        n_epochs=60,\n        pbar=True,\n        n_regions_step=500,\n        n_cells_step=200,\n        weight_decay=1e-5,\n        checkpoint_every_epoch=1,\n    ):\n\"\"\"\n        Train the model\n        \"\"\"\n        if fold is None:\n            fold = self.fold\n        assert fold is not None\n\n        if fragments is None:\n            fragments = self.fragments\n        if transcriptome is None:\n            transcriptome = self.transcriptome\n\n        # set up minibatchers and loaders\n        minibatcher_train = Minibatcher(\n            fold[\"cells_train\"],\n            range(fragments.n_regions),\n            n_regions_step=n_regions_step,\n            n_cells_step=n_cells_step,\n        )\n        minibatcher_validation = Minibatcher(\n            fold[\"cells_validation\"],\n            range(fragments.n_regions),\n            n_regions_step=10,\n            n_cells_step=10000,\n            permute_cells=False,\n            permute_regions=False,\n        )\n\n        if device is None:\n            device = get_default_device()\n\n        loaders_train = LoaderPool(\n            TranscriptomeFragments,\n            dict(\n                transcriptome=transcriptome,\n                fragments=fragments,\n                cellxregion_batch_size=minibatcher_train.cellxregion_batch_size,\n                layer=self.layer,\n            ),\n            n_workers=10,\n        )\n        loaders_validation = LoaderPool(\n            TranscriptomeFragments,\n            dict(\n                transcriptome=transcriptome,\n                fragments=fragments,\n                cellxregion_batch_size=minibatcher_validation.cellxregion_batch_size,\n                layer=self.layer,\n            ),\n            n_workers=5,\n        )\n\n        trainer = Trainer(\n            self,\n            loaders_train,\n            loaders_validation,\n            minibatcher_train,\n            minibatcher_validation,\n            SparseDenseAdam(\n                self.parameters_sparse(),\n                self.parameters_dense(),\n                lr=lr,\n                weight_decay=weight_decay,\n            ),\n            n_epochs=n_epochs,\n            checkpoint_every_epoch=checkpoint_every_epoch,\n            optimize_every_step=1,\n            device=device,\n            pbar=pbar,\n        )\n\n        self.trace = trainer.trace\n\n        trainer.train()\n        # trainer.trace.plot()\n\n    def get_prediction(\n        self,\n        fragments=None,\n        transcriptome=None,\n        cells=None,\n        cell_ixs=None,\n        regions=None,\n        region_ixs=None,\n        device=None,\n        return_raw=False,\n    ):\n\"\"\"\n        Returns the prediction of a dataset\n        \"\"\"\n\n        if fragments is None:\n            fragments = self.fragments\n        if transcriptome is None:\n            transcriptome = self.transcriptome\n        if cell_ixs is None:\n            if cells is None:\n                cells = fragments.obs.index\n            fragments.obs[\"ix\"] = np.arange(len(fragments.obs))\n            cell_ixs = fragments.obs.loc[cells][\"ix\"].values\n        if cells is None:\n            cells = fragments.obs.index[cell_ixs]\n\n        if region_ixs is None:\n            if regions is None:\n                regions = fragments.var.index\n            fragments.var[\"ix\"] = np.arange(len(fragments.var))\n            region_ixs = fragments.var.loc[regions][\"ix\"].values\n        if regions is None:\n            regions = fragments.var.index[region_ixs]\n\n        if device is None:\n            device = get_default_device()\n\n        minibatches = Minibatcher(\n            cell_ixs,\n            region_ixs,\n            n_regions_step=500,\n            n_cells_step=200,\n            use_all_cells=True,\n            use_all_regions=True,\n            permute_cells=False,\n            permute_regions=False,\n        )\n        loaders = LoaderPool(\n            TranscriptomeFragments,\n            dict(\n                transcriptome=transcriptome,\n                fragments=fragments,\n                cellxregion_batch_size=minibatches.cellxregion_batch_size,\n                layer=self.layer,\n            ),\n            n_workers=5,\n        )\n        loaders.initialize(minibatches)\n\n        predicted = np.zeros((len(cell_ixs), len(region_ixs)))\n        expected = np.zeros((len(cell_ixs), len(region_ixs)))\n        n_fragments = np.zeros((len(cell_ixs), len(region_ixs)))\n\n        cell_mapping = np.zeros(fragments.n_cells, dtype=np.int64)\n        cell_mapping[cell_ixs] = np.arange(len(cell_ixs))\n\n        region_mapping = np.zeros(fragments.n_regions, dtype=np.int64)\n        region_mapping[region_ixs] = np.arange(len(region_ixs))\n\n        self.eval()\n        self = self.to(device)\n\n        for data in loaders:\n            data = data.to(device)\n            with torch.no_grad():\n                pred_mb = self.forward(data)\n            predicted[np.ix_(cell_mapping[data.minibatch.cells_oi], data.minibatch.regions_oi)] = pred_mb.cpu().numpy()\n            expected[\n                np.ix_(cell_mapping[data.minibatch.cells_oi], data.minibatch.regions_oi)\n            ] = data.transcriptome.value.cpu().numpy()\n            n_fragments[np.ix_(cell_mapping[data.minibatch.cells_oi], data.minibatch.regions_oi)] = (\n                torch.bincount(\n                    data.fragments.local_cellxregion_ix,\n                    minlength=len(data.minibatch.cells_oi) * len(data.minibatch.regions_oi),\n                )\n                .reshape(len(data.minibatch.cells_oi), len(data.minibatch.regions_oi))\n                .cpu()\n                .numpy()\n            )\n\n        self = self.to(\"cpu\")\n\n        if return_raw:\n            return predicted, expected, n_fragments\n\n        result = xr.Dataset(\n            {\n                \"predicted\": xr.DataArray(\n                    predicted,\n                    dims=(fragments.obs.index.name, fragments.var.index.name),\n                    coords={fragments.obs.index.name: cells, fragments.var.index.name: regions},\n                ),\n                \"expected\": xr.DataArray(\n                    expected,\n                    dims=(fragments.obs.index.name, fragments.var.index.name),\n                    coords={fragments.obs.index.name: cells, fragments.var.index.name: regions},\n                ),\n                \"n_fragments\": xr.DataArray(\n                    n_fragments,\n                    dims=(fragments.obs.index.name, fragments.var.index.name),\n                    coords={fragments.obs.index.name: cells, fragments.var.index.name: regions},\n                ),\n            }\n        )\n        return result\n\n    def get_prediction_censored(\n        self,\n        censorer,\n        fragments=None,\n        transcriptome=None,\n        cells=None,\n        cell_ixs=None,\n        regions=None,\n        region_ixs=None,\n        device=None,\n    ):\n\"\"\"\n        Returns the prediction of multiple censored dataset\n        \"\"\"\n        if fragments is None:\n            fragments = self.fragments\n        if transcriptome is None:\n            transcriptome = self.transcriptome\n\n        if cell_ixs is None:\n            if cells is None:\n                cells = fragments.obs.index\n            fragments.obs[\"ix\"] = np.arange(len(fragments.obs))\n            cell_ixs = fragments.obs.loc[cells][\"ix\"].values\n        if cells is None:\n            cells = fragments.obs.index[cell_ixs]\n\n        if region_ixs is None:\n            if regions is None:\n                regions = fragments.var.index\n            fragments.var[\"ix\"] = np.arange(len(fragments.var))\n            region_ixs = fragments.var.loc[regions][\"ix\"].values\n        if regions is None:\n            regions = fragments.var.index[region_ixs]\n\n        if device is None:\n            device = get_default_device()\n\n        minibatcher = Minibatcher(\n            cell_ixs,\n            region_ixs,\n            n_regions_step=500,\n            n_cells_step=5000,\n            use_all_cells=True,\n            use_all_regions=True,\n            permute_cells=False,\n            permute_regions=False,\n        )\n        loaders = LoaderPool(\n            TranscriptomeFragments,\n            dict(\n                transcriptome=transcriptome,\n                fragments=fragments,\n                cellxregion_batch_size=minibatcher.cellxregion_batch_size,\n                layer=self.layer,\n            ),\n            n_workers=10,\n        )\n        loaders.initialize(minibatcher)\n\n        predicted = np.zeros((len(censorer), len(cell_ixs), len(region_ixs)), dtype=float)\n        expected = np.zeros((len(cell_ixs), len(region_ixs)), dtype=float)\n        n_fragments = np.zeros((len(censorer), len(cell_ixs), len(region_ixs)), dtype=int)\n\n        cell_mapping = np.zeros(fragments.n_cells, dtype=np.int64)\n        cell_mapping[cell_ixs] = np.arange(len(cell_ixs))\n        region_mapping = np.zeros(fragments.n_regions, dtype=np.int64)\n        region_mapping[region_ixs] = np.arange(len(region_ixs))\n\n        self.eval()\n        self.to(device)\n        for data in loaders:\n            data = data.to(device)\n            fragments_oi = censorer(data)\n\n            with torch.no_grad():\n                for design_ix, (\n                    pred_mb,\n                    n_fragments_oi_mb,\n                ) in enumerate(self.forward_multiple(data, fragments_oi)):\n                    ix = np.ix_(\n                        [design_ix],\n                        cell_mapping[data.minibatch.cells_oi],\n                        region_mapping[data.minibatch.regions_oi],\n                    )\n                    predicted[ix] = pred_mb.cpu().numpy()\n                    n_fragments[ix] = n_fragments_oi_mb.cpu().numpy()\n            expected[\n                np.ix_(\n                    cell_mapping[data.minibatch.cells_oi],\n                    region_mapping[data.minibatch.regions_oi],\n                )\n            ] = data.transcriptome.value.cpu().numpy()\n\n        self.to(\"cpu\")\n\n        return predicted, expected, n_fragments\n</code></pre>"},{"location":"reference/models/pred/model/#chromatinhd.models.pred.model.additive.Model.fold","title":"<code>fold = Stored()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The cells used for training, test and validation</p>"},{"location":"reference/models/pred/model/#chromatinhd.models.pred.model.additive.Model.fragments","title":"<code>fragments = Linked()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The fragments</p>"},{"location":"reference/models/pred/model/#chromatinhd.models.pred.model.additive.Model.layer","title":"<code>layer = Stored()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The layer of the transcriptome</p>"},{"location":"reference/models/pred/model/#chromatinhd.models.pred.model.additive.Model.transcriptome","title":"<code>transcriptome = Linked()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The transcriptome</p>"},{"location":"reference/models/pred/model/#chromatinhd.models.pred.model.additive.Model.forward","title":"<code>forward(data)</code>","text":"<p>Make a prediction given a data object</p> Source code in <code>src/chromatinhd/models/pred/model/additive.py</code> <pre><code>def forward(self, data):\n\"\"\"\n    Make a prediction given a data object\n    \"\"\"\n    fragment_embedding = self.fragment_embedder(data.fragments.coordinates, data.fragments.regionmapping)\n    cell_region_embedding = self.embedding_region_pooler(\n        fragment_embedding,\n        data.fragments.local_cellxregion_ix,\n        data.minibatch.n_cells,\n        data.minibatch.n_regions,\n    )\n    expression_predicted = self.embedding_to_expression(cell_region_embedding, data.minibatch.regions_oi_torch)\n    return expression_predicted\n</code></pre>"},{"location":"reference/models/pred/model/#chromatinhd.models.pred.model.additive.Model.forward_loss","title":"<code>forward_loss(data)</code>","text":"<p>Make a prediction and calculate the loss given a data object</p> Source code in <code>src/chromatinhd/models/pred/model/additive.py</code> <pre><code>def forward_loss(self, data):\n\"\"\"\n    Make a prediction and calculate the loss given a data object\n    \"\"\"\n    expression_predicted = self.forward(data)\n    expression_true = data.transcriptome.value\n    return paircor_loss(expression_predicted, expression_true)\n</code></pre>"},{"location":"reference/models/pred/model/#chromatinhd.models.pred.model.additive.Model.forward_region_loss","title":"<code>forward_region_loss(data)</code>","text":"<p>Make a prediction and calculate the loss given a data object</p> Source code in <code>src/chromatinhd/models/pred/model/additive.py</code> <pre><code>def forward_region_loss(self, data):\n\"\"\"\n    Make a prediction and calculate the loss given a data object\n    \"\"\"\n    expression_predicted = self.forward(data)\n    expression_true = data.transcriptome.value\n    return region_paircor_loss(expression_predicted, expression_true)\n</code></pre>"},{"location":"reference/models/pred/model/#chromatinhd.models.pred.model.additive.Model.get_prediction","title":"<code>get_prediction(fragments=None, transcriptome=None, cells=None, cell_ixs=None, regions=None, region_ixs=None, device=None, return_raw=False)</code>","text":"<p>Returns the prediction of a dataset</p> Source code in <code>src/chromatinhd/models/pred/model/additive.py</code> <pre><code>def get_prediction(\n    self,\n    fragments=None,\n    transcriptome=None,\n    cells=None,\n    cell_ixs=None,\n    regions=None,\n    region_ixs=None,\n    device=None,\n    return_raw=False,\n):\n\"\"\"\n    Returns the prediction of a dataset\n    \"\"\"\n\n    if fragments is None:\n        fragments = self.fragments\n    if transcriptome is None:\n        transcriptome = self.transcriptome\n    if cell_ixs is None:\n        if cells is None:\n            cells = fragments.obs.index\n        fragments.obs[\"ix\"] = np.arange(len(fragments.obs))\n        cell_ixs = fragments.obs.loc[cells][\"ix\"].values\n    if cells is None:\n        cells = fragments.obs.index[cell_ixs]\n\n    if region_ixs is None:\n        if regions is None:\n            regions = fragments.var.index\n        fragments.var[\"ix\"] = np.arange(len(fragments.var))\n        region_ixs = fragments.var.loc[regions][\"ix\"].values\n    if regions is None:\n        regions = fragments.var.index[region_ixs]\n\n    if device is None:\n        device = get_default_device()\n\n    minibatches = Minibatcher(\n        cell_ixs,\n        region_ixs,\n        n_regions_step=500,\n        n_cells_step=200,\n        use_all_cells=True,\n        use_all_regions=True,\n        permute_cells=False,\n        permute_regions=False,\n    )\n    loaders = LoaderPool(\n        TranscriptomeFragments,\n        dict(\n            transcriptome=transcriptome,\n            fragments=fragments,\n            cellxregion_batch_size=minibatches.cellxregion_batch_size,\n            layer=self.layer,\n        ),\n        n_workers=5,\n    )\n    loaders.initialize(minibatches)\n\n    predicted = np.zeros((len(cell_ixs), len(region_ixs)))\n    expected = np.zeros((len(cell_ixs), len(region_ixs)))\n    n_fragments = np.zeros((len(cell_ixs), len(region_ixs)))\n\n    cell_mapping = np.zeros(fragments.n_cells, dtype=np.int64)\n    cell_mapping[cell_ixs] = np.arange(len(cell_ixs))\n\n    region_mapping = np.zeros(fragments.n_regions, dtype=np.int64)\n    region_mapping[region_ixs] = np.arange(len(region_ixs))\n\n    self.eval()\n    self = self.to(device)\n\n    for data in loaders:\n        data = data.to(device)\n        with torch.no_grad():\n            pred_mb = self.forward(data)\n        predicted[np.ix_(cell_mapping[data.minibatch.cells_oi], data.minibatch.regions_oi)] = pred_mb.cpu().numpy()\n        expected[\n            np.ix_(cell_mapping[data.minibatch.cells_oi], data.minibatch.regions_oi)\n        ] = data.transcriptome.value.cpu().numpy()\n        n_fragments[np.ix_(cell_mapping[data.minibatch.cells_oi], data.minibatch.regions_oi)] = (\n            torch.bincount(\n                data.fragments.local_cellxregion_ix,\n                minlength=len(data.minibatch.cells_oi) * len(data.minibatch.regions_oi),\n            )\n            .reshape(len(data.minibatch.cells_oi), len(data.minibatch.regions_oi))\n            .cpu()\n            .numpy()\n        )\n\n    self = self.to(\"cpu\")\n\n    if return_raw:\n        return predicted, expected, n_fragments\n\n    result = xr.Dataset(\n        {\n            \"predicted\": xr.DataArray(\n                predicted,\n                dims=(fragments.obs.index.name, fragments.var.index.name),\n                coords={fragments.obs.index.name: cells, fragments.var.index.name: regions},\n            ),\n            \"expected\": xr.DataArray(\n                expected,\n                dims=(fragments.obs.index.name, fragments.var.index.name),\n                coords={fragments.obs.index.name: cells, fragments.var.index.name: regions},\n            ),\n            \"n_fragments\": xr.DataArray(\n                n_fragments,\n                dims=(fragments.obs.index.name, fragments.var.index.name),\n                coords={fragments.obs.index.name: cells, fragments.var.index.name: regions},\n            ),\n        }\n    )\n    return result\n</code></pre>"},{"location":"reference/models/pred/model/#chromatinhd.models.pred.model.additive.Model.get_prediction_censored","title":"<code>get_prediction_censored(censorer, fragments=None, transcriptome=None, cells=None, cell_ixs=None, regions=None, region_ixs=None, device=None)</code>","text":"<p>Returns the prediction of multiple censored dataset</p> Source code in <code>src/chromatinhd/models/pred/model/additive.py</code> <pre><code>def get_prediction_censored(\n    self,\n    censorer,\n    fragments=None,\n    transcriptome=None,\n    cells=None,\n    cell_ixs=None,\n    regions=None,\n    region_ixs=None,\n    device=None,\n):\n\"\"\"\n    Returns the prediction of multiple censored dataset\n    \"\"\"\n    if fragments is None:\n        fragments = self.fragments\n    if transcriptome is None:\n        transcriptome = self.transcriptome\n\n    if cell_ixs is None:\n        if cells is None:\n            cells = fragments.obs.index\n        fragments.obs[\"ix\"] = np.arange(len(fragments.obs))\n        cell_ixs = fragments.obs.loc[cells][\"ix\"].values\n    if cells is None:\n        cells = fragments.obs.index[cell_ixs]\n\n    if region_ixs is None:\n        if regions is None:\n            regions = fragments.var.index\n        fragments.var[\"ix\"] = np.arange(len(fragments.var))\n        region_ixs = fragments.var.loc[regions][\"ix\"].values\n    if regions is None:\n        regions = fragments.var.index[region_ixs]\n\n    if device is None:\n        device = get_default_device()\n\n    minibatcher = Minibatcher(\n        cell_ixs,\n        region_ixs,\n        n_regions_step=500,\n        n_cells_step=5000,\n        use_all_cells=True,\n        use_all_regions=True,\n        permute_cells=False,\n        permute_regions=False,\n    )\n    loaders = LoaderPool(\n        TranscriptomeFragments,\n        dict(\n            transcriptome=transcriptome,\n            fragments=fragments,\n            cellxregion_batch_size=minibatcher.cellxregion_batch_size,\n            layer=self.layer,\n        ),\n        n_workers=10,\n    )\n    loaders.initialize(minibatcher)\n\n    predicted = np.zeros((len(censorer), len(cell_ixs), len(region_ixs)), dtype=float)\n    expected = np.zeros((len(cell_ixs), len(region_ixs)), dtype=float)\n    n_fragments = np.zeros((len(censorer), len(cell_ixs), len(region_ixs)), dtype=int)\n\n    cell_mapping = np.zeros(fragments.n_cells, dtype=np.int64)\n    cell_mapping[cell_ixs] = np.arange(len(cell_ixs))\n    region_mapping = np.zeros(fragments.n_regions, dtype=np.int64)\n    region_mapping[region_ixs] = np.arange(len(region_ixs))\n\n    self.eval()\n    self.to(device)\n    for data in loaders:\n        data = data.to(device)\n        fragments_oi = censorer(data)\n\n        with torch.no_grad():\n            for design_ix, (\n                pred_mb,\n                n_fragments_oi_mb,\n            ) in enumerate(self.forward_multiple(data, fragments_oi)):\n                ix = np.ix_(\n                    [design_ix],\n                    cell_mapping[data.minibatch.cells_oi],\n                    region_mapping[data.minibatch.regions_oi],\n                )\n                predicted[ix] = pred_mb.cpu().numpy()\n                n_fragments[ix] = n_fragments_oi_mb.cpu().numpy()\n        expected[\n            np.ix_(\n                cell_mapping[data.minibatch.cells_oi],\n                region_mapping[data.minibatch.regions_oi],\n            )\n        ] = data.transcriptome.value.cpu().numpy()\n\n    self.to(\"cpu\")\n\n    return predicted, expected, n_fragments\n</code></pre>"},{"location":"reference/models/pred/model/#chromatinhd.models.pred.model.additive.Model.train_model","title":"<code>train_model(fold=None, fragments=None, transcriptome=None, device=None, lr=0.001, n_epochs=60, pbar=True, n_regions_step=500, n_cells_step=200, weight_decay=1e-05, checkpoint_every_epoch=1)</code>","text":"<p>Train the model</p> Source code in <code>src/chromatinhd/models/pred/model/additive.py</code> <pre><code>def train_model(\n    self,\n    fold: list = None,\n    fragments: Fragments = None,\n    transcriptome: Transcriptome = None,\n    device=None,\n    lr=1e-3,\n    n_epochs=60,\n    pbar=True,\n    n_regions_step=500,\n    n_cells_step=200,\n    weight_decay=1e-5,\n    checkpoint_every_epoch=1,\n):\n\"\"\"\n    Train the model\n    \"\"\"\n    if fold is None:\n        fold = self.fold\n    assert fold is not None\n\n    if fragments is None:\n        fragments = self.fragments\n    if transcriptome is None:\n        transcriptome = self.transcriptome\n\n    # set up minibatchers and loaders\n    minibatcher_train = Minibatcher(\n        fold[\"cells_train\"],\n        range(fragments.n_regions),\n        n_regions_step=n_regions_step,\n        n_cells_step=n_cells_step,\n    )\n    minibatcher_validation = Minibatcher(\n        fold[\"cells_validation\"],\n        range(fragments.n_regions),\n        n_regions_step=10,\n        n_cells_step=10000,\n        permute_cells=False,\n        permute_regions=False,\n    )\n\n    if device is None:\n        device = get_default_device()\n\n    loaders_train = LoaderPool(\n        TranscriptomeFragments,\n        dict(\n            transcriptome=transcriptome,\n            fragments=fragments,\n            cellxregion_batch_size=minibatcher_train.cellxregion_batch_size,\n            layer=self.layer,\n        ),\n        n_workers=10,\n    )\n    loaders_validation = LoaderPool(\n        TranscriptomeFragments,\n        dict(\n            transcriptome=transcriptome,\n            fragments=fragments,\n            cellxregion_batch_size=minibatcher_validation.cellxregion_batch_size,\n            layer=self.layer,\n        ),\n        n_workers=5,\n    )\n\n    trainer = Trainer(\n        self,\n        loaders_train,\n        loaders_validation,\n        minibatcher_train,\n        minibatcher_validation,\n        SparseDenseAdam(\n            self.parameters_sparse(),\n            self.parameters_dense(),\n            lr=lr,\n            weight_decay=weight_decay,\n        ),\n        n_epochs=n_epochs,\n        checkpoint_every_epoch=checkpoint_every_epoch,\n        optimize_every_step=1,\n        device=device,\n        pbar=pbar,\n    )\n\n    self.trace = trainer.trace\n\n    trainer.train()\n</code></pre>"},{"location":"reference/models/pred/model/#chromatinhd.models.pred.model.additive.Models","title":"<code>chromatinhd.models.pred.model.additive.Models</code>","text":"<p>         Bases: <code>Flow</code></p> Source code in <code>src/chromatinhd/models/pred/model/additive.py</code> <pre><code>class Models(Flow):\n    models = StoredDict(Stored)\n    n_models = Stored()\n\n    @property\n    def models_path(self):\n        path = self.path / \"models\"\n        path.mkdir(exist_ok=True)\n        return path\n\n    def train_models(self, fragments, transcriptome, folds, device=None):\n        self.n_models = len(folds)\n        for fold_ix, fold in [(fold_ix, fold) for fold_ix, fold in enumerate(folds)]:\n            desired_outputs = [self.models_path / (\"model_\" + str(fold_ix) + \".pkl\")]\n            force = False\n            if not all([desired_output.exists() for desired_output in desired_outputs]):\n                force = True\n\n            if force:\n                model = Model(\n                    n_regions=fragments.n_regions,\n                )\n                model.train_model(fragments, transcriptome, fold, device=device)\n\n                model = model.to(\"cpu\")\n\n                self.models[fold_ix] = model\n\n    def __getitem__(self, ix):\n        return self.models[ix]\n\n    def __len__(self):\n        return len(self.models)\n\n    def __iter__(self):\n        for ix in range(len(self)):\n            yield self[ix]\n\n    def get_region_cors(self, fragments, transcriptome, folds, device=None):\n        cor_predicted = np.zeros((len(fragments.var.index), len(folds)))\n        cor_n_fragments = np.zeros((len(fragments.var.index), len(folds)))\n        n_fragments = np.zeros((len(fragments.var.index), len(folds)))\n\n        if device is None:\n            device = get_default_device()\n        for model_ix, (model, fold) in enumerate(zip(self, folds)):\n            prediction = model.get_prediction(fragments, transcriptome, cell_ixs=fold[\"cells_test\"], device=device)\n\n            cor_predicted[:, model_ix] = paircor(prediction[\"predicted\"].values, prediction[\"expected\"].values)\n            cor_n_fragments[:, model_ix] = paircor(prediction[\"n_fragments\"].values, prediction[\"expected\"].values)\n\n            n_fragments[:, model_ix] = prediction[\"n_fragments\"].values.sum(0)\n        cor_predicted = pd.Series(cor_predicted.mean(1), index=fragments.var.index, name=\"cor_predicted\")\n        cor_n_fragments = pd.Series(cor_n_fragments.mean(1), index=fragments.var.index, name=\"cor_n_fragments\")\n        n_fragments = pd.Series(n_fragments.mean(1), index=fragments.var.index, name=\"n_fragments\")\n        result = pd.concat([cor_predicted, cor_n_fragments, n_fragments], axis=1)\n        result[\"deltacor\"] = result[\"cor_predicted\"] - result[\"cor_n_fragments\"]\n\n        return result\n</code></pre>"},{"location":"reference/models/pred/plot/","title":"Plot","text":""},{"location":"reference/models/pred/plot/#chromatinhd.models.pred.plot","title":"<code>chromatinhd.models.pred.plot</code>","text":""},{"location":"reference/models/pred/plot/#chromatinhd.models.pred.plot.Copredictivity","title":"<code>Copredictivity</code>","text":"<p>         Bases: <code>chromatinhd.grid.Panel</code></p> <p>Plot co-predictivity of a gene.</p> Source code in <code>src/chromatinhd/models/pred/plot/copredictivity.py</code> <pre><code>class Copredictivity(chromatinhd.grid.Panel):\n\"\"\"\n    Plot co-predictivity of a gene.\n    \"\"\"\n\n    def __init__(self, plotdata, width):\n        super().__init__((width, width / 2))\n\n        norm = mpl.colors.CenteredNorm(0, np.abs(plotdata[\"cor\"]).max())\n        cmap = mpl.cm.RdBu_r\n\n        chromatinhd.plot.matshow45(\n            self.ax,\n            plotdata.set_index([\"window_mid1\", \"window_mid2\"])[\"cor\"],\n            cmap=cmap,\n            norm=norm,\n            radius=50,\n        )\n        self.ax.invert_yaxis()\n\n        panel_copredictivity_legend = self.add_inset(\n            chromatinhd.grid.Panel((0.05, 0.8)), pos=(0.0, 0.0), offset=(0.0, 0.2)\n        )\n        plt.colorbar(\n            mpl.cm.ScalarMappable(norm=norm, cmap=cmap),\n            cax=panel_copredictivity_legend.ax,\n            orientation=\"vertical\",\n        )\n        panel_copredictivity_legend.ax.set_ylabel(\n            \"Co-predictivity\\n(cor $\\\\Delta$cor)\",\n            rotation=0,\n            ha=\"right\",\n            va=\"center\",\n        )\n        panel_copredictivity_legend.ax.yaxis.set_ticks_position(\"left\")\n        panel_copredictivity_legend.ax.yaxis.set_label_position(\"left\")\n\n    @classmethod\n    def from_regionpairwindow(cls, regionpairwindow, gene, width):\n\"\"\"\n        Plot co-predictivity of a gene using a RegionPairWindow object.\n        \"\"\"\n        plotdata = regionpairwindow.get_plotdata(gene).reset_index()\n        return cls(plotdata, width)\n</code></pre>"},{"location":"reference/models/pred/plot/#chromatinhd.models.pred.plot.copredictivity.Copredictivity.from_regionpairwindow","title":"<code>from_regionpairwindow(regionpairwindow, gene, width)</code>  <code>classmethod</code>","text":"<p>Plot co-predictivity of a gene using a RegionPairWindow object.</p> Source code in <code>src/chromatinhd/models/pred/plot/copredictivity.py</code> <pre><code>@classmethod\ndef from_regionpairwindow(cls, regionpairwindow, gene, width):\n\"\"\"\n    Plot co-predictivity of a gene using a RegionPairWindow object.\n    \"\"\"\n    plotdata = regionpairwindow.get_plotdata(gene).reset_index()\n    return cls(plotdata, width)\n</code></pre>"},{"location":"reference/models/pred/plot/#chromatinhd.models.pred.plot.Pileup","title":"<code>Pileup</code>","text":"<p>         Bases: <code>chromatinhd.grid.Panel</code></p> Source code in <code>src/chromatinhd/models/pred/plot/predictivity.py</code> <pre><code>class Pileup(chromatinhd.grid.Panel):\n    def __init__(\n        self,\n        plotdata,\n        window,\n        width,\n        ymax=2.0,\n    ):\n        super().__init__((width, 0.5))\n        if \"position\" not in plotdata.columns:\n            plotdata = plotdata.reset_index()\n\n        ax = self.ax\n        ax.set_xlim(*window)\n        ax.plot(\n            plotdata[\"position\"],\n            plotdata[\"lost\"],\n            color=\"#333\",\n            lw=1,\n        )\n        ax.fill_between(\n            plotdata[\"position\"],\n            plotdata[\"lost\"],\n            0,\n            color=\"#333\",\n            alpha=0.2,\n            lw=0,\n        )\n        ax.set_xlim(ax.get_xlim())\n        ax.set_ylabel(\n            \"# fragments\\nper 1kb\\nper 1k cells\",\n            rotation=0,\n            ha=\"right\",\n            va=\"center\",\n        )\n\n        # change vertical alignment of last y tick to bottom\n        ax.set_yticks([0, ymax])\n        ax.get_yticklabels()[-1].set_verticalalignment(\"top\")\n        ax.get_yticklabels()[0].set_verticalalignment(\"bottom\")\n\n        # vline at tss\n        ax.axvline(0, color=\"#888888\", lw=0.5, zorder=-1, dashes=(2, 2))\n\n        ax.set_xticks([])\n        ax.set_ylim(0, ymax)\n\n    @classmethod\n    def from_regionmultiwindow(cls, regionmultiwindow, gene, width, window=None):\n\"\"\"\n        Plot pileup of a specific gene using a regionmultiwindow object\n        \"\"\"\n        plotdata = regionmultiwindow.get_plotdata(gene).reset_index()\n        if window is None:\n            window = np.array([plotdata[\"position\"].min(), plotdata[\"position\"].max()])\n        return cls(plotdata, width=width, window=window)\n</code></pre>"},{"location":"reference/models/pred/plot/#chromatinhd.models.pred.plot.predictivity.Pileup.from_regionmultiwindow","title":"<code>from_regionmultiwindow(regionmultiwindow, gene, width, window=None)</code>  <code>classmethod</code>","text":"<p>Plot pileup of a specific gene using a regionmultiwindow object</p> Source code in <code>src/chromatinhd/models/pred/plot/predictivity.py</code> <pre><code>@classmethod\ndef from_regionmultiwindow(cls, regionmultiwindow, gene, width, window=None):\n\"\"\"\n    Plot pileup of a specific gene using a regionmultiwindow object\n    \"\"\"\n    plotdata = regionmultiwindow.get_plotdata(gene).reset_index()\n    if window is None:\n        window = np.array([plotdata[\"position\"].min(), plotdata[\"position\"].max()])\n    return cls(plotdata, width=width, window=window)\n</code></pre>"},{"location":"reference/models/pred/plot/#chromatinhd.models.pred.plot.Predictivity","title":"<code>Predictivity</code>","text":"<p>         Bases: <code>chromatinhd.grid.Panel</code></p> <p>Plot predictivity of a gene.</p> Source code in <code>src/chromatinhd/models/pred/plot/predictivity.py</code> <pre><code>class Predictivity(chromatinhd.grid.Panel):\n\"\"\"\n    Plot predictivity of a gene.\n    \"\"\"\n\n    def __init__(\n        self,\n        plotdata,\n        window,\n        width,\n        show_accessibility=False,\n        color_by_effect=True,\n        limit=-0.05,\n        label_y=True,\n        height=0.5,\n    ):\n        super().__init__((width, height))\n\n        if \"position\" not in plotdata.columns:\n            plotdata = plotdata.reset_index()\n\n        plotdata[\"effect_sign\"] = np.sign(plotdata[\"effect\"])\n        plotdata[\"segment\"] = plotdata[\"effect_sign\"].diff().ne(0).cumsum()\n\n        ax = self.ax\n        ax.set_xlim(*window)\n\n        for segment, segment_data in plotdata.groupby(\"segment\"):\n            if color_by_effect:\n                color = \"tomato\" if segment_data[\"effect\"].iloc[0] &gt; 0 else \"#0074D9\"\n            else:\n                color = \"#333\"\n            ax.plot(\n                segment_data[\"position\"],\n                segment_data[\"deltacor\"],\n                lw=1,\n                color=color,\n            )\n            ax.fill_between(\n                segment_data[\"position\"],\n                segment_data[\"deltacor\"],\n                0,\n                alpha=0.2,\n                lw=0,\n                color=color,\n            )\n\n        # ax.plot(\n        #     plotdata[\"position\"],\n        #     plotdata[\"deltacor\"],\n        #     color=\"#333\",\n        #     lw=1,\n        # )\n        # ax.fill_between(\n        #     plotdata[\"position\"],\n        #     plotdata[\"deltacor\"],\n        #     0,\n        #     color=\"#333\",\n        #     alpha=0.2,\n        #     lw=0,\n        # )\n\n        if label_y is True:\n            label_y = \"Predictivity\\n($\\\\Delta$ cor)\"\n        ax.set_ylabel(\n            label_y,\n            rotation=0,\n            ha=\"right\",\n            va=\"center\",\n        )\n\n        ax.set_xticks([])\n        ax.invert_yaxis()\n        ax.set_ylim(0, max(limit, ax.get_ylim()[1]))\n\n        if show_accessibility:\n            ax2 = self.add_twinx()\n            ax2.plot(\n                plotdata[\"position\"],\n                plotdata[\"lost\"],\n                color=\"tomato\",\n                # color=\"#333\",\n                lw=1,\n            )\n            ax2.fill_between(\n                plotdata[\"position\"],\n                plotdata[\"lost\"],\n                0,\n                color=\"tomato\",\n                alpha=0.2,\n                lw=0,\n            )\n            ax2.set_xlim(ax.get_xlim())\n            ax2.set_ylabel(\n                \"# fragments\\nper 1kb\\nper 1k cells\",\n                rotation=0,\n                ha=\"left\",\n                va=\"center\",\n                color=\"tomato\",\n            )\n            ax2.tick_params(axis=\"y\", colors=\"tomato\")\n            ax2.set_ylim(\n                0,\n                plotdata[\"lost\"].max() / (plotdata[\"deltacor\"].min() / ax.get_ylim()[1]),\n            )\n\n        # change vertical alignment of last y tick to bottom\n        ax.set_yticks([0, ax.get_ylim()[1]])\n        ax.get_yticklabels()[-1].set_verticalalignment(\"top\")\n        ax.get_yticklabels()[0].set_verticalalignment(\"bottom\")\n\n        # vline at tss\n        ax.axvline(0, color=\"#888888\", lw=0.5, zorder=-1, dashes=(2, 2))\n\n    @classmethod\n    def from_regionmultiwindow(cls, regionmultiwindow, gene, width, show_accessibility=False, window=None):\n\"\"\"\n        Plot predictivity of a specific gene using a RegionMultiWindow object\n        \"\"\"\n        plotdata = regionmultiwindow.get_plotdata(gene).reset_index()\n        if window is None:\n            window = np.array([plotdata[\"position\"].min(), plotdata[\"position\"].max()])\n        return cls(plotdata, width=width, show_accessibility=show_accessibility, window=window)\n\n    def add_arrow(self, position, y=0.5, orientation=\"left\"):\n        ax = self.ax\n        trans = mpl.transforms.blended_transform_factory(x_transform=ax.transData, y_transform=ax.transAxes)\n        if orientation == \"left\":\n            xytext = (15, 15)\n        elif orientation == \"right\":\n            xytext = (-15, 15)\n        ax.annotate(\n            text=\"\",\n            xy=(position, y),\n            xytext=xytext,\n            textcoords=\"offset points\",\n            xycoords=trans,\n            arrowprops=dict(arrowstyle=\"-&gt;\", color=\"black\", lw=1, connectionstyle=\"arc3\"),\n        )\n</code></pre>"},{"location":"reference/models/pred/plot/#chromatinhd.models.pred.plot.predictivity.Predictivity.from_regionmultiwindow","title":"<code>from_regionmultiwindow(regionmultiwindow, gene, width, show_accessibility=False, window=None)</code>  <code>classmethod</code>","text":"<p>Plot predictivity of a specific gene using a RegionMultiWindow object</p> Source code in <code>src/chromatinhd/models/pred/plot/predictivity.py</code> <pre><code>@classmethod\ndef from_regionmultiwindow(cls, regionmultiwindow, gene, width, show_accessibility=False, window=None):\n\"\"\"\n    Plot predictivity of a specific gene using a RegionMultiWindow object\n    \"\"\"\n    plotdata = regionmultiwindow.get_plotdata(gene).reset_index()\n    if window is None:\n        window = np.array([plotdata[\"position\"].min(), plotdata[\"position\"].max()])\n    return cls(plotdata, width=width, show_accessibility=show_accessibility, window=window)\n</code></pre>"}]}