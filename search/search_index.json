{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>ChromatinHD analyzes single-cell ATAC+RNA data using the raw fragments as input, by automatically adapting the scale at which relevant chromatin changes on a per-position, per-cell, and per-gene basis. This enables identification of functional chromatin changes regardless of whether they occur in a narrow or broad region.</p> <p>ChromatinHD models can capture long-range interactions by considering fragments co-occuring within the same cell, as we highlight in Figure 5 of our paper,</p> <p>ChromatinHD models can also capture changes in fragment size that are related to gene expression changes, likely driven by dense direct and indirect binding of transcription factors, as we highlight in Figure 6 of our paper.</p> <p>Currently, the following models are supported:</p> Pred <p> To learn where and how accessibility is predictive for gene expression </p> Diff <p>To understand the differences in accessibilty between cell types/states</p> Time <p>To learn where and how accessibility is predictive over (pseudo)time</p> Dime <p>To learn the differences in accessibility over (pseudo)time</p>"},{"location":"benchmark/","title":"Benchmark","text":"<p>Lightweight benchmarking of the various models in terms of function and scalability. More comprehensive benchmarking with the state-of-the-art was performed in our paper.</p>"},{"location":"benchmark/diff/models/","title":"Models","text":"In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nimport seaborn as sns\n\nsns.set_style(\"ticks\")\n%config InlineBackend.figure_format='retina'\n\nimport tqdm.auto as tqdm\n</pre> %load_ext autoreload %autoreload 2  import numpy as np import pandas as pd  import matplotlib.pyplot as plt import matplotlib as mpl  import seaborn as sns  sns.set_style(\"ticks\") %config InlineBackend.figure_format='retina'  import tqdm.auto as tqdm In\u00a0[2]: Copied! <pre>import chromatinhd as chd\n\nchd.set_default_device(\"cuda:1\")\nchd.get_default_device()\n</pre> import chromatinhd as chd  chd.set_default_device(\"cuda:1\") chd.get_default_device() Out[2]: <pre>'cuda:1'</pre> In\u00a0[3]: Copied! <pre>dataset_folder_original = chd.get_output() / \"datasets\" / \"pbmc10k\"\ntranscriptome_original = chd.data.Transcriptome(dataset_folder_original / \"transcriptome\")\nfragments_original = chd.data.Fragments(dataset_folder_original / \"fragments\" / \"10k10k\")\n</pre> dataset_folder_original = chd.get_output() / \"datasets\" / \"pbmc10k\" transcriptome_original = chd.data.Transcriptome(dataset_folder_original / \"transcriptome\") fragments_original = chd.data.Fragments(dataset_folder_original / \"fragments\" / \"10k10k\") In\u00a0[4]: Copied! <pre>genes_oi = transcriptome_original.var.sort_values(\"dispersions_norm\", ascending=False).head(50).index\nregions = fragments_original.regions.filter_genes(genes_oi)\nfragments = fragments_original.filter_genes(regions)\nfragments.create_cellxgene_indptr()\ntranscriptome = transcriptome_original.filter_genes(regions.coordinates.index)\n</pre> genes_oi = transcriptome_original.var.sort_values(\"dispersions_norm\", ascending=False).head(50).index regions = fragments_original.regions.filter_genes(genes_oi) fragments = fragments_original.filter_genes(regions) fragments.create_cellxgene_indptr() transcriptome = transcriptome_original.filter_genes(regions.coordinates.index) In\u00a0[5]: Copied! <pre>folds = chd.data.folds.Folds()\nfolds.sample_cells(fragments, 5)\n</pre> folds = chd.data.folds.Folds() folds.sample_cells(fragments, 5) In\u00a0[6]: Copied! <pre>clustering = chd.data.Clustering.from_labels(transcriptome.obs[\"celltype\"])\n</pre> clustering = chd.data.Clustering.from_labels(transcriptome.obs[\"celltype\"]) In\u00a0[7]: Copied! <pre>fold = folds[0]\n</pre> fold = folds[0] In\u00a0[8]: Copied! <pre>models = {}\n</pre> models = {} In\u00a0[9]: Copied! <pre>import logging\n\nlogger = chd.models.diff.trainer.trainer.logger\nlogger.setLevel(logging.DEBUG)\nlogger.handlers = []\n# logger.handlers = [logging.StreamHandler()]\n</pre> import logging  logger = chd.models.diff.trainer.trainer.logger logger.setLevel(logging.DEBUG) logger.handlers = [] # logger.handlers = [logging.StreamHandler()] In\u00a0[10]: Copied! <pre>model = chd.models.diff.model.cutnf.Model(\n    fragments,\n    clustering,\n)\nmodel.train_model(fragments, clustering, fold, n_epochs=30)\nmodels[\"original\"] = model\n</pre> model = chd.models.diff.model.cutnf.Model(     fragments,     clustering, ) model.train_model(fragments, clustering, fold, n_epochs=30) models[\"original\"] = model <pre>  0%|          | 0/1230 [00:00&lt;?, ?it/s]</pre> In\u00a0[11]: Copied! <pre>model = chd.models.diff.model.cutnf.Model(\n    fragments,\n    clustering,\n    nbins = (32, 64, 128)\n)\nmodel.train_model(fragments, clustering, fold, n_epochs=30)\nmodels[\"original_rev\"] = model\n</pre> model = chd.models.diff.model.cutnf.Model(     fragments,     clustering,     nbins = (32, 64, 128) ) model.train_model(fragments, clustering, fold, n_epochs=30) models[\"original_rev\"] = model <pre>  0%|          | 0/1230 [00:00&lt;?, ?it/s]</pre> In\u00a0[12]: Copied! <pre>model = chd.models.diff.model.cutnf.Model(\n    fragments,\n    clustering,\n)\nmodel.train_model(fragments, clustering, fold, n_epochs=100)\nmodels[\"original_100epoch\"] = model\n</pre> model = chd.models.diff.model.cutnf.Model(     fragments,     clustering, ) model.train_model(fragments, clustering, fold, n_epochs=100) models[\"original_100epoch\"] = model <pre>  0%|          | 0/4100 [00:00&lt;?, ?it/s]</pre> In\u00a0[13]: Copied! <pre>model = chd.models.diff.model.cutnf.Model(\n    fragments,\n    clustering,\n    mixture_delta_p_scale=5.0,\n)\nmodel.train_model(fragments, clustering, fold, n_epochs=30)\nmodels[\"original_mixture-delta-p=5\"] = model\n</pre> model = chd.models.diff.model.cutnf.Model(     fragments,     clustering,     mixture_delta_p_scale=5.0, ) model.train_model(fragments, clustering, fold, n_epochs=30) models[\"original_mixture-delta-p=5\"] = model <pre>  0%|          | 0/1230 [00:00&lt;?, ?it/s]</pre> In\u00a0[14]: Copied! <pre>model = chd.models.diff.model.cutnf.Model(\n    fragments,\n    clustering,\n    mixture_delta_regularization=False,\n    rho_delta_regularization=False,\n)\nmodel.train_model(fragments, clustering, fold, n_epochs=30)\nmodels[\"original_noreg\"] = model\n</pre> model = chd.models.diff.model.cutnf.Model(     fragments,     clustering,     mixture_delta_regularization=False,     rho_delta_regularization=False, ) model.train_model(fragments, clustering, fold, n_epochs=30) models[\"original_noreg\"] = model <pre>  0%|          | 0/1230 [00:00&lt;?, ?it/s]</pre> In\u00a0[15]: Copied! <pre>model = chd.models.diff.model.cutnf.Model(\n    fragments,\n    clustering,\n    mixture_delta_p_scale=0.001,\n)\nmodel.train_model(fragments, clustering, fold, n_epochs=30)\nmodels[\"baseline_orig\"] = model\n</pre> model = chd.models.diff.model.cutnf.Model(     fragments,     clustering,     mixture_delta_p_scale=0.001, ) model.train_model(fragments, clustering, fold, n_epochs=30) models[\"baseline_orig\"] = model <pre>  0%|          | 0/1230 [00:00&lt;?, ?it/s]</pre> In\u00a0[16]: Copied! <pre>model = chd.models.diff.model.cutnf.Model(\n    fragments, clustering, mixture_delta_regularization=False, rho_delta_regularization=False, nbins=(128,)\n)\nmodel.train_model(fragments, clustering, fold, n_epochs=30)\nmodels[\"original_noreg_128\"] = model\n</pre> model = chd.models.diff.model.cutnf.Model(     fragments, clustering, mixture_delta_regularization=False, rho_delta_regularization=False, nbins=(128,) ) model.train_model(fragments, clustering, fold, n_epochs=30) models[\"original_noreg_128\"] = model <pre>  0%|          | 0/1230 [00:00&lt;?, ?it/s]</pre> In\u00a0[17]: Copied! <pre>model = chd.models.diff.model.cutnf.Model(\n    fragments,\n    clustering,\n    mixture_delta_regularization=False,\n    rho_delta_regularization=False,\n    nbins=(256,),\n)\nmodel.train_model(fragments, clustering, fold, n_epochs=30)\nmodels[\"original_noreg_256\"] = model\n</pre> model = chd.models.diff.model.cutnf.Model(     fragments,     clustering,     mixture_delta_regularization=False,     rho_delta_regularization=False,     nbins=(256,), ) model.train_model(fragments, clustering, fold, n_epochs=30) models[\"original_noreg_256\"] = model <pre>  0%|          | 0/1230 [00:00&lt;?, ?it/s]</pre> In\u00a0[18]: Copied! <pre>model = chd.models.diff.model.cutnf.Model(\n    fragments,\n    clustering,\n    mixture_delta_regularization=False,\n    rho_delta_regularization=False,\n    nbins=(256, 128, 64),\n)\nmodel.train_model(fragments, clustering, fold, n_epochs=30)\nmodels[\"original_noreg_256,128,64\"] = model\n</pre> model = chd.models.diff.model.cutnf.Model(     fragments,     clustering,     mixture_delta_regularization=False,     rho_delta_regularization=False,     nbins=(256, 128, 64), ) model.train_model(fragments, clustering, fold, n_epochs=30) models[\"original_noreg_256,128,64\"] = model <pre>  0%|          | 0/1230 [00:00&lt;?, ?it/s]</pre> In\u00a0[19]: Copied! <pre>model = chd.models.diff.model.cutnf.Model(\n    fragments,\n    clustering,\n    nbins=(256, 128, 64),\n)\nmodel.train_model(fragments, clustering, fold, n_epochs=30)\nmodels[\"original_256,128,64\"] = model\n</pre> model = chd.models.diff.model.cutnf.Model(     fragments,     clustering,     nbins=(256, 128, 64), ) model.train_model(fragments, clustering, fold, n_epochs=30) models[\"original_256,128,64\"] = model <pre>  0%|          | 0/1230 [00:00&lt;?, ?it/s]</pre> In\u00a0[20]: Copied! <pre>model = chd.models.diff.model.cutnf.Model(\n    fragments,\n    clustering,\n    nbins=(512, 256, 128, 64),\n)\nmodel.train_model(fragments, clustering, fold, n_epochs=30)\nmodels[\"original_512,256,128,64\"] = model\n</pre> model = chd.models.diff.model.cutnf.Model(     fragments,     clustering,     nbins=(512, 256, 128, 64), ) model.train_model(fragments, clustering, fold, n_epochs=30) models[\"original_512,256,128,64\"] = model <pre>  0%|          | 0/1230 [00:00&lt;?, ?it/s]</pre> In\u00a0[21]: Copied! <pre>model = chd.models.diff.model.cutnf.Model(fragments, clustering, mixture_delta_p_scale=0.001, nbins=(128,))\nmodel.train_model(fragments, clustering, fold, n_epochs=30)\nmodels[\"baseline_128\"] = model\n</pre> model = chd.models.diff.model.cutnf.Model(fragments, clustering, mixture_delta_p_scale=0.001, nbins=(128,)) model.train_model(fragments, clustering, fold, n_epochs=30) models[\"baseline_128\"] = model <pre>  0%|          | 0/1230 [00:00&lt;?, ?it/s]</pre> In\u00a0[22]: Copied! <pre>model = chd.models.diff.model.cutnf.Model(fragments, clustering, mixture_delta_p_scale=0.001, nbins=(256,))\nmodel.train_model(fragments, clustering, fold, n_epochs=30)\nmodels[\"baseline_256\"] = model\n</pre> model = chd.models.diff.model.cutnf.Model(fragments, clustering, mixture_delta_p_scale=0.001, nbins=(256,)) model.train_model(fragments, clustering, fold, n_epochs=30) models[\"baseline_256\"] = model <pre>  0%|          | 0/1230 [00:00&lt;?, ?it/s]</pre> In\u00a0[23]: Copied! <pre>model = chd.models.diff.model.cutnf.Model(fragments, clustering, mixture_delta_p_scale=0.001, nbins=(256, 128, 64))\nmodel.train_model(fragments, clustering, fold, n_epochs=30)\nmodels[\"baseline_256,128,64\"] = model\n</pre> model = chd.models.diff.model.cutnf.Model(fragments, clustering, mixture_delta_p_scale=0.001, nbins=(256, 128, 64)) model.train_model(fragments, clustering, fold, n_epochs=30) models[\"baseline_256,128,64\"] = model <pre>  0%|          | 0/1230 [00:00&lt;?, ?it/s]</pre> In\u00a0[29]: Copied! <pre>scores = []\ngenescores = []\nfor model_id, model in models.items():\n    prediction_test = model.get_prediction(fragments, clustering, cell_ixs=fold[\"cells_test\"])\n    prediction_validation = model.get_prediction(fragments, clustering, cell_ixs=fold[\"cells_validation\"])\n    prediction_train = model.get_prediction(fragments, clustering, cell_ixs=fold[\"cells_train\"])\n    scores.append(\n        {\n            \"model_id\": model_id,\n            \"lik_test\": (prediction_test[\"likelihood_mixture\"]).sum().item(),\n            \"n_test\": len(fold[\"cells_test\"]),\n            \"lik_validation\": (prediction_validation[\"likelihood_mixture\"]).sum().item(),\n            \"n_validation\": len(fold[\"cells_validation\"]),\n            \"lik_train\": (prediction_train[\"likelihood_mixture\"]).sum().item(),\n            \"n_train\": len(fold[\"cells_train\"]),\n        }\n    )\n    genescores.append(\n        pd.DataFrame(\n            {\n                \"model_id\": model_id,\n                \"lik_test\": (prediction_test[\"likelihood_mixture\"]).sum(\"cell\").to_pandas(),\n                \"n_test\": len(fold[\"cells_test\"]),\n                \"lik_validation\": (prediction_validation[\"likelihood_mixture\"]).sum(\"cell\").to_pandas(),\n                \"n_validation\": len(fold[\"cells_validation\"]),\n                \"lik_train\": (prediction_train[\"likelihood_mixture\"]).sum(\"cell\").to_pandas(),\n                \"n_train\": len(fold[\"cells_train\"]),\n            }\n        )\n    )\nscores = pd.DataFrame(scores).set_index(\"model_id\")\ngenescores = (\n    pd.concat([genescores[i] for i in range(len(genescores))], axis=0).reset_index().set_index([\"model_id\", \"gene\"])\n)\n</pre> scores = [] genescores = [] for model_id, model in models.items():     prediction_test = model.get_prediction(fragments, clustering, cell_ixs=fold[\"cells_test\"])     prediction_validation = model.get_prediction(fragments, clustering, cell_ixs=fold[\"cells_validation\"])     prediction_train = model.get_prediction(fragments, clustering, cell_ixs=fold[\"cells_train\"])     scores.append(         {             \"model_id\": model_id,             \"lik_test\": (prediction_test[\"likelihood_mixture\"]).sum().item(),             \"n_test\": len(fold[\"cells_test\"]),             \"lik_validation\": (prediction_validation[\"likelihood_mixture\"]).sum().item(),             \"n_validation\": len(fold[\"cells_validation\"]),             \"lik_train\": (prediction_train[\"likelihood_mixture\"]).sum().item(),             \"n_train\": len(fold[\"cells_train\"]),         }     )     genescores.append(         pd.DataFrame(             {                 \"model_id\": model_id,                 \"lik_test\": (prediction_test[\"likelihood_mixture\"]).sum(\"cell\").to_pandas(),                 \"n_test\": len(fold[\"cells_test\"]),                 \"lik_validation\": (prediction_validation[\"likelihood_mixture\"]).sum(\"cell\").to_pandas(),                 \"n_validation\": len(fold[\"cells_validation\"]),                 \"lik_train\": (prediction_train[\"likelihood_mixture\"]).sum(\"cell\").to_pandas(),                 \"n_train\": len(fold[\"cells_train\"]),             }         )     ) scores = pd.DataFrame(scores).set_index(\"model_id\") genescores = (     pd.concat([genescores[i] for i in range(len(genescores))], axis=0).reset_index().set_index([\"model_id\", \"gene\"]) ) In\u00a0[30]: Copied! <pre>baseline_id = \"baseline_orig\"\nscores[[\"lr_test\", \"lr_validation\", \"lr_train\"]] = (\n    scores[[\"lik_test\", \"lik_validation\", \"lik_train\"]]\n    - scores[[\"lik_test\", \"lik_validation\", \"lik_train\"]].loc[baseline_id]\n)\nscores[[\"nlr_test\", \"nlr_validation\", \"nlr_train\"]] = (\n    scores[[\"lr_test\", \"lr_validation\", \"lr_train\"]].values / scores[[\"n_test\", \"n_validation\", \"n_train\"]].values\n)\ngenescores[[\"lr_test\", \"lr_validation\", \"lr_train\"]] = (\n    genescores[[\"lik_test\", \"lik_validation\", \"lik_train\"]]\n    - genescores[[\"lik_test\", \"lik_validation\", \"lik_train\"]].loc[baseline_id]\n)\ngenescores[[\"nlr_test\", \"nlr_validation\", \"nlr_train\"]] = (\n    genescores[[\"lr_test\", \"lr_validation\", \"lr_train\"]].values\n    / genescores[[\"n_test\", \"n_validation\", \"n_train\"]].values\n)\n</pre> baseline_id = \"baseline_orig\" scores[[\"lr_test\", \"lr_validation\", \"lr_train\"]] = (     scores[[\"lik_test\", \"lik_validation\", \"lik_train\"]]     - scores[[\"lik_test\", \"lik_validation\", \"lik_train\"]].loc[baseline_id] ) scores[[\"nlr_test\", \"nlr_validation\", \"nlr_train\"]] = (     scores[[\"lr_test\", \"lr_validation\", \"lr_train\"]].values / scores[[\"n_test\", \"n_validation\", \"n_train\"]].values ) genescores[[\"lr_test\", \"lr_validation\", \"lr_train\"]] = (     genescores[[\"lik_test\", \"lik_validation\", \"lik_train\"]]     - genescores[[\"lik_test\", \"lik_validation\", \"lik_train\"]].loc[baseline_id] ) genescores[[\"nlr_test\", \"nlr_validation\", \"nlr_train\"]] = (     genescores[[\"lr_test\", \"lr_validation\", \"lr_train\"]].values     / genescores[[\"n_test\", \"n_validation\", \"n_train\"]].values ) In\u00a0[31]: Copied! <pre>model_info = pd.DataFrame({\"model\": models.keys()}).set_index(\"model\")\nmodel_info[\"model_type\"] = model_info.index.map(lambda x: \"_\".join(x.split(\"_\")[:-1]))\nmodel_info = model_info.sort_values([\"model_type\"])\nmodel_info[\"ix\"] = np.arange(model_info.shape[0])\n</pre> model_info = pd.DataFrame({\"model\": models.keys()}).set_index(\"model\") model_info[\"model_type\"] = model_info.index.map(lambda x: \"_\".join(x.split(\"_\")[:-1])) model_info = model_info.sort_values([\"model_type\"]) model_info[\"ix\"] = np.arange(model_info.shape[0]) In\u00a0[32]: Copied! <pre>fig = chd.grid.Figure(chd.grid.Wrap(padding_width=0.1))\nheight = len(scores) * 0.2\n\nplotdata = scores.copy().loc[model_info.index]\n\npanel, ax = fig.main.add(chd.grid.Ax((1, height)))\nax.barh(plotdata.index, plotdata[\"lr_test\"])\nax.axvline(0, color=\"black\", linestyle=\"--\", lw=1)\nax.set_title(\"Test\")\nax.set_xlabel(\"Log-likehood ratio\")\n\npanel, ax = fig.main.add(chd.grid.Ax((1, height)))\nax.set_yticks([])\nax.barh(plotdata.index, plotdata[\"lr_validation\"])\nax.axvline(0, color=\"black\", linestyle=\"--\", lw=1)\nax.set_title(\"Validation\")\n\npanel, ax = fig.main.add(chd.grid.Ax((1, height)))\nax.set_yticks([])\nax.barh(plotdata.index, plotdata[\"lr_train\"])\nax.axvline(0, color=\"black\", linestyle=\"--\", lw=1)\nax.set_title(\"Train\")\nfig.plot()\n</pre> fig = chd.grid.Figure(chd.grid.Wrap(padding_width=0.1)) height = len(scores) * 0.2  plotdata = scores.copy().loc[model_info.index]  panel, ax = fig.main.add(chd.grid.Ax((1, height))) ax.barh(plotdata.index, plotdata[\"lr_test\"]) ax.axvline(0, color=\"black\", linestyle=\"--\", lw=1) ax.set_title(\"Test\") ax.set_xlabel(\"Log-likehood ratio\")  panel, ax = fig.main.add(chd.grid.Ax((1, height))) ax.set_yticks([]) ax.barh(plotdata.index, plotdata[\"lr_validation\"]) ax.axvline(0, color=\"black\", linestyle=\"--\", lw=1) ax.set_title(\"Validation\")  panel, ax = fig.main.add(chd.grid.Ax((1, height))) ax.set_yticks([]) ax.barh(plotdata.index, plotdata[\"lr_train\"]) ax.axvline(0, color=\"black\", linestyle=\"--\", lw=1) ax.set_title(\"Train\") fig.plot() In\u00a0[33]: Copied! <pre>plotdata = genescores[\"lr_test\"].unstack()\nplotdata.columns = transcriptome.symbol(plotdata.columns)\nplotdata = plotdata.loc[model_info.index].T\n\nfig, ax = plt.subplots(figsize=(plotdata.shape[1] * 0.2, plotdata.shape[0] * 0.2))\nsns.heatmap(plotdata, vmax=100, vmin=-100, cmap=\"RdBu_r\", center=0, cbar_kws={\"shrink\": 0.5}, yticklabels=True)\n\n# add dot for highest\nfor i, gene in enumerate(plotdata.index):\n    j = plotdata.loc[gene].argmax()\n    plt.plot(j + 0.5, i + 0.5, \"o\", color=\"black\", markersize=4, markeredgewidth=1.0, markeredgecolor=\"white\")\n</pre> plotdata = genescores[\"lr_test\"].unstack() plotdata.columns = transcriptome.symbol(plotdata.columns) plotdata = plotdata.loc[model_info.index].T  fig, ax = plt.subplots(figsize=(plotdata.shape[1] * 0.2, plotdata.shape[0] * 0.2)) sns.heatmap(plotdata, vmax=100, vmin=-100, cmap=\"RdBu_r\", center=0, cbar_kws={\"shrink\": 0.5}, yticklabels=True)  # add dot for highest for i, gene in enumerate(plotdata.index):     j = plotdata.loc[gene].argmax()     plt.plot(j + 0.5, i + 0.5, \"o\", color=\"black\", markersize=4, markeredgewidth=1.0, markeredgecolor=\"white\") In\u00a0[34]: Copied! <pre>genepositional = chd.models.diff.interpret.genepositional.GenePositional(\n    path=chd.get_output() / \"interpret\" / \"genepositional\"\n)\n</pre> genepositional = chd.models.diff.interpret.genepositional.GenePositional(     path=chd.get_output() / \"interpret\" / \"genepositional\" ) In\u00a0[35]: Copied! <pre>symbol = \"EBF1\"\nmodel_id = \"original\"\n# model_id = \"original_512,256,128,64\"\n\ngenepositional.score(fragments, clustering, [models[model_id]], force=True, genes=transcriptome.gene_id([symbol]))\n</pre> symbol = \"EBF1\" model_id = \"original\" # model_id = \"original_512,256,128,64\"  genepositional.score(fragments, clustering, [models[model_id]], force=True, genes=transcriptome.gene_id([symbol])) <pre>  0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> In\u00a0[165]: Copied! <pre>fig = chd.grid.Figure(chd.grid.Grid(padding_height=0.05, padding_width=0.05))\nwidth = 10\n\nregion = fragments.regions.coordinates.loc[transcriptome.gene_id(symbol)]\npanel_genes = chd.plot.genome.genes.Genes.from_region(region, width=width)\nfig.main.add_under(panel_genes)\n\nplotdata, plotdata_mean = genepositional.get_plotdata(transcriptome.gene_id(symbol))\npanel_differential = chd.models.diff.plot.Differential(\n    plotdata, plotdata_mean, cluster_info=clustering.cluster_info, panel_height=0.5, width=width\n)\nfig.main.add_under(panel_differential)\n\npanel_expression = chd.models.diff.plot.DifferentialExpression.from_transcriptome(\n    transcriptome=transcriptome, clustering=clustering, gene=transcriptome.gene_id(symbol), panel_height=0.5\n)\nfig.main.add_right(panel_expression, row=panel_differential)\n\nfig.plot()\n</pre> fig = chd.grid.Figure(chd.grid.Grid(padding_height=0.05, padding_width=0.05)) width = 10  region = fragments.regions.coordinates.loc[transcriptome.gene_id(symbol)] panel_genes = chd.plot.genome.genes.Genes.from_region(region, width=width) fig.main.add_under(panel_genes)  plotdata, plotdata_mean = genepositional.get_plotdata(transcriptome.gene_id(symbol)) panel_differential = chd.models.diff.plot.Differential(     plotdata, plotdata_mean, cluster_info=clustering.cluster_info, panel_height=0.5, width=width ) fig.main.add_under(panel_differential)  panel_expression = chd.models.diff.plot.DifferentialExpression.from_transcriptome(     transcriptome=transcriptome, clustering=clustering, gene=transcriptome.gene_id(symbol), panel_height=0.5 ) fig.main.add_right(panel_expression, row=panel_differential)  fig.plot()"},{"location":"benchmark/diff/models/#train","title":"Train\u00b6","text":""},{"location":"benchmark/diff/models/#score","title":"Score\u00b6","text":""},{"location":"benchmark/diff/models/#interpret","title":"Interpret\u00b6","text":""},{"location":"benchmark/diff/simulated/","title":"Simulated","text":"In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nimport seaborn as sns\n\nsns.set_style(\"ticks\")\n%config InlineBackend.figure_format='retina'\n\nimport tqdm.auto as tqdm\n</pre> %load_ext autoreload %autoreload 2  import numpy as np import pandas as pd  import matplotlib.pyplot as plt import matplotlib as mpl  import seaborn as sns  sns.set_style(\"ticks\") %config InlineBackend.figure_format='retina'  import tqdm.auto as tqdm In\u00a0[2]: Copied! <pre>import chromatinhd as chd\n\nchd.set_default_device(\"cuda:1\")\nchd.get_default_device()\n</pre> import chromatinhd as chd  chd.set_default_device(\"cuda:1\") chd.get_default_device() Out[2]: <pre>'cuda:1'</pre> In\u00a0[3]: Copied! <pre>import chromatinhd.simulation.simulate\n</pre> import chromatinhd.simulation.simulate In\u00a0[4]: Copied! <pre>simulation = chd.simulation.simulate.Simulation(n_genes=10, window=[-10000, 10000])\nsimulation.create_regions()\nsimulation.create_obs()\n\nsimulation.create_fragments(n_peaks_per_gene = 20)\n</pre> simulation = chd.simulation.simulate.Simulation(n_genes=10, window=[-10000, 10000]) simulation.create_regions() simulation.create_obs()  simulation.create_fragments(n_peaks_per_gene = 20) <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> In\u00a0[5]: Copied! <pre>fragments = simulation.fragments\nfragments.create_cellxgene_indptr()\n</pre> fragments = simulation.fragments fragments.create_cellxgene_indptr() In\u00a0[6]: Copied! <pre>sns.histplot((fragments.coordinates[:, 1] - fragments.coordinates[:, 0]))\n</pre> sns.histplot((fragments.coordinates[:, 1] - fragments.coordinates[:, 0])) Out[6]: <pre>&lt;Axes: ylabel='Count'&gt;</pre> In\u00a0[7]: Copied! <pre>clustering = simulation.clustering\n</pre> clustering = simulation.clustering In\u00a0[8]: Copied! <pre>folds = chd.data.folds.Folds()\nfolds.sample_cells(fragments, 5)\nfold = folds[0]\n</pre> folds = chd.data.folds.Folds() folds.sample_cells(fragments, 5) fold = folds[0] In\u00a0[9]: Copied! <pre>minibatcher = chd.models.diff.loader.Minibatcher(np.arange(len(fragments.obs)), np.arange(len(fragments.var)), 100, 100)\n</pre> minibatcher = chd.models.diff.loader.Minibatcher(np.arange(len(fragments.obs)), np.arange(len(fragments.var)), 100, 100) In\u00a0[10]: Copied! <pre>import chromatinhd.models.diff.loader.clustering_fragments\nloader = chromatinhd.models.diff.loader.clustering_fragments.ClusteringFragments(clustering, fragments, minibatcher.cellxgene_batch_size)\ndata = loader.load(next(iter(minibatcher)))\n</pre> import chromatinhd.models.diff.loader.clustering_fragments loader = chromatinhd.models.diff.loader.clustering_fragments.ClusteringFragments(clustering, fragments, minibatcher.cellxgene_batch_size) data = loader.load(next(iter(minibatcher))) In\u00a0[11]: Copied! <pre>import chromatinhd.models.diff.model.playground\nmodel = chd.models.diff.model.playground.Model(\n    fragments, clustering\n)\n</pre> import chromatinhd.models.diff.model.playground model = chd.models.diff.model.playground.Model(     fragments, clustering ) In\u00a0[12]: Copied! <pre>transform = chromatinhd.models.diff.model.spline.DifferentialQuadraticSplineStack(\n    nbins = (128, ), n_genes = 1\n)\n</pre> transform = chromatinhd.models.diff.model.spline.DifferentialQuadraticSplineStack(     nbins = (128, ), n_genes = 1 ) In\u00a0[13]: Copied! <pre>import truncated_normal\n</pre> import truncated_normal In\u00a0[14]: Copied! <pre>import math\ndef log_prob_normal(value, loc, scale):\n    var = scale**2\n    log_scale = (\n        math.log(scale)\n    )\n    return (\n        -((value - loc) ** 2) / (2 * var)\n        - log_scale\n        - math.log(math.sqrt(2 * math.pi))\n    )\n\ndef log_prob_trun_normal(value, loc, scale, a=0, b=1):\n    return truncated_normal.TruncatedNormal(loc, scale, a, b).log_prob(value)\n\n    \ndef apply_trunc_normal(value, loc, scale, a=0, b=1):\n    dist = truncated_normal.TruncatedNormal(loc, scale, a, b)\n    return dist.cdf(value), dist.log_prob(value)\n</pre> import math def log_prob_normal(value, loc, scale):     var = scale**2     log_scale = (         math.log(scale)     )     return (         -((value - loc) ** 2) / (2 * var)         - log_scale         - math.log(math.sqrt(2 * math.pi))     )  def log_prob_trun_normal(value, loc, scale, a=0, b=1):     return truncated_normal.TruncatedNormal(loc, scale, a, b).log_prob(value)       def apply_trunc_normal(value, loc, scale, a=0, b=1):     dist = truncated_normal.TruncatedNormal(loc, scale, a, b)     return dist.cdf(value), dist.log_prob(value) In\u00a0[15]: Copied! <pre>import torch\nx = torch.linspace(0.001, 1, 100)\ngenes_oi = torch.tensor([0])\nlocal_gene_ix = torch.zeros(len(x), dtype = torch.int)\ndelta = torch.zeros((len(x), np.sum(transform.split_deltas)))\ndelta[:, :30] = 1\ndelta[:, 30:40] = -1\n\nlog_prob = torch.zeros_like(x)\n\noutput, logabsdet = transform.transform_forward(x, genes_oi, local_gene_ix, delta)\nlog_prob += logabsdet\n\nfig, (ax_pdf, ax_cdf) = plt.subplots(1, 2, figsize=(8, 4))\nax_pdf.plot(x, torch.exp(log_prob).detach().numpy())\nax_cdf.plot(\n    x.numpy(),\n    output.detach().numpy()\n)\nassert np.isclose(np.trapz(torch.exp(log_prob).detach().numpy(), x), 1, atol=1e-2)\n\nloc = 0.9\nscale = 0.3\nloc2 = output[torch.argmin((x - loc).abs())]\n\noutput, logabsdet = apply_trunc_normal(output, loc2, scale)\nlog_prob += logabsdet\n\nax_pdf.plot(x, torch.exp(log_prob).detach().numpy())\nax_cdf.plot(\n    x.numpy(),\n    output.detach().numpy()\n)\nassert np.isclose(np.trapz(torch.exp(log_prob).detach().numpy(), x), 1, atol=1e-2)\n\nloc = 0.1\nscale = 0.3\nloc2 = output[torch.argmin((x - loc).abs())]\n\noutput, logabsdet = apply_trunc_normal(output, loc2, scale)\nlog_prob += logabsdet\n\nax_pdf.plot(x, torch.exp(log_prob).detach().numpy())\nax_cdf.plot(\n    x.numpy(),\n    output.detach().numpy()\n)\nassert np.isclose(np.trapz(torch.exp(log_prob).detach().numpy(), x), 1, atol=1e-2)\n</pre> import torch x = torch.linspace(0.001, 1, 100) genes_oi = torch.tensor([0]) local_gene_ix = torch.zeros(len(x), dtype = torch.int) delta = torch.zeros((len(x), np.sum(transform.split_deltas))) delta[:, :30] = 1 delta[:, 30:40] = -1  log_prob = torch.zeros_like(x)  output, logabsdet = transform.transform_forward(x, genes_oi, local_gene_ix, delta) log_prob += logabsdet  fig, (ax_pdf, ax_cdf) = plt.subplots(1, 2, figsize=(8, 4)) ax_pdf.plot(x, torch.exp(log_prob).detach().numpy()) ax_cdf.plot(     x.numpy(),     output.detach().numpy() ) assert np.isclose(np.trapz(torch.exp(log_prob).detach().numpy(), x), 1, atol=1e-2)  loc = 0.9 scale = 0.3 loc2 = output[torch.argmin((x - loc).abs())]  output, logabsdet = apply_trunc_normal(output, loc2, scale) log_prob += logabsdet  ax_pdf.plot(x, torch.exp(log_prob).detach().numpy()) ax_cdf.plot(     x.numpy(),     output.detach().numpy() ) assert np.isclose(np.trapz(torch.exp(log_prob).detach().numpy(), x), 1, atol=1e-2)  loc = 0.1 scale = 0.3 loc2 = output[torch.argmin((x - loc).abs())]  output, logabsdet = apply_trunc_normal(output, loc2, scale) log_prob += logabsdet  ax_pdf.plot(x, torch.exp(log_prob).detach().numpy()) ax_cdf.plot(     x.numpy(),     output.detach().numpy() ) assert np.isclose(np.trapz(torch.exp(log_prob).detach().numpy(), x), 1, atol=1e-2) In\u00a0[16]: Copied! <pre>models = {}\n</pre> models = {} In\u00a0[37]: Copied! <pre>model = chd.models.diff.model.playground.Model(\n    fragments,\n    clustering,\n    cut_embedder = \"dummy\"\n)\nmodel.train_model(fragments, clustering, fold, n_epochs=30)\nmodels[\"original_cutdummy\"] = model\n</pre> model = chd.models.diff.model.playground.Model(     fragments,     clustering,     cut_embedder = \"dummy\" ) model.train_model(fragments, clustering, fold, n_epochs=30) models[\"original_cutdummy\"] = model <pre>  0%|          | 0/120 [00:00&lt;?, ?it/s]</pre> In\u00a0[21]: Copied! <pre>model = chd.models.diff.model.playground.Model(\n    fragments,\n    clustering,\n)\nmodel.train_model(fragments, clustering, fold, n_epochs=100)\nmodels[\"original\"] = model\n</pre> model = chd.models.diff.model.playground.Model(     fragments,     clustering, ) model.train_model(fragments, clustering, fold, n_epochs=100) models[\"original\"] = model <pre>  0%|          | 0/400 [00:00&lt;?, ?it/s]</pre> In\u00a0[22]: Copied! <pre>model = chd.models.diff.model.playground.Model(\n    fragments,\n    clustering,\n    cut_embedder = \"direct\"\n)\nmodel.train_model(fragments, clustering, fold, n_epochs=100)\nmodels[\"original_cutdirect\"] = model\n</pre> model = chd.models.diff.model.playground.Model(     fragments,     clustering,     cut_embedder = \"direct\" ) model.train_model(fragments, clustering, fold, n_epochs=100) models[\"original_cutdirect\"] = model <pre>  0%|          | 0/400 [00:00&lt;?, ?it/s]</pre> In\u00a0[38]: Copied! <pre>gene_ix = 1\ncluster_ixs = np.arange(clustering.n_clusters)\n\ncoordinates = torch.linspace(*fragments.regions.window, 1000)\nsizes = torch.linspace(0, 500, 50)\n\ndesign = chd.utils.crossing(\n    coordinate=coordinates,\n    size=sizes,\n    gene_ix = torch.tensor([gene_ix]),\n    cluster_ix = torch.tensor(cluster_ixs),\n)\ndesign[\"coordinate2\"] = (design[\"coordinate\"] + design[\"size\"])\ndesign = design.loc[design[\"coordinate2\"] &lt;= fragments.regions.window[1]]\ndesign = design.loc[design[\"coordinate2\"] &gt;= fragments.regions.window[0]]\n</pre> gene_ix = 1 cluster_ixs = np.arange(clustering.n_clusters)  coordinates = torch.linspace(*fragments.regions.window, 1000) sizes = torch.linspace(0, 500, 50)  design = chd.utils.crossing(     coordinate=coordinates,     size=sizes,     gene_ix = torch.tensor([gene_ix]),     cluster_ix = torch.tensor(cluster_ixs), ) design[\"coordinate2\"] = (design[\"coordinate\"] + design[\"size\"]) design = design.loc[design[\"coordinate2\"] &lt;= fragments.regions.window[1]] design = design.loc[design[\"coordinate2\"] &gt;= fragments.regions.window[0]] In\u00a0[39]: Copied! <pre>design[\"prob_left\"], design[\"prob_right\"] = model.evaluate_right(torch.from_numpy(design[\"coordinate\"].values), torch.from_numpy(design[\"coordinate2\"].values), gene_ix = torch.from_numpy(design[\"gene_ix\"].values), window = fragments.regions.window, cluster_ix = torch.from_numpy(design[\"cluster_ix\"].values))\n</pre> design[\"prob_left\"], design[\"prob_right\"] = model.evaluate_right(torch.from_numpy(design[\"coordinate\"].values), torch.from_numpy(design[\"coordinate2\"].values), gene_ix = torch.from_numpy(design[\"gene_ix\"].values), window = fragments.regions.window, cluster_ix = torch.from_numpy(design[\"cluster_ix\"].values)) In\u00a0[40]: Copied! <pre>fig = chd.grid.Figure(chd.grid.Grid())\nwidth = 10\npanel, ax = fig.main.add_under(chd.grid.Panel((width, 0.5)))\nplotdata = design.loc[design[\"size\"] == 0].set_index([\"cluster_ix\", \"coordinate\"])[[\"prob_left\"]]\nfor cluster_ix, plotdata_cluster in plotdata.groupby(\"cluster_ix\"):\n    plotdata_cluster = plotdata_cluster.droplevel(\"cluster_ix\").sort_index()\n    ax.plot(plotdata_cluster.index, np.exp(plotdata_cluster[\"prob_left\"]), label = cluster_ix)\nax.set_xlim(*fragments.regions.window)\n\nplotdata = simulation.peaks.query(\"gene == @design.gene_ix.iloc[0]\").sort_values(\"center\")\n\nax2 = panel.add_twinx()\nax2.scatter(plotdata[\"center\"], [0] * len(plotdata), c = plotdata[\"size_scale\"])\nax2.set_xlim(*fragments.regions.window)\n\npanel, ax = fig.main.add_under(chd.grid.Panel((width, 2)))\nplotdata = np.exp(design.groupby([\"size\", \"coordinate\"]).mean()[\"prob_right\"].unstack())\nax.matshow(plotdata, aspect = \"auto\")\nax.set_yticks(np.arange(plotdata.shape[0]))\nax.set_yticklabels(plotdata.index)\n\nfig.plot()\n</pre> fig = chd.grid.Figure(chd.grid.Grid()) width = 10 panel, ax = fig.main.add_under(chd.grid.Panel((width, 0.5))) plotdata = design.loc[design[\"size\"] == 0].set_index([\"cluster_ix\", \"coordinate\"])[[\"prob_left\"]] for cluster_ix, plotdata_cluster in plotdata.groupby(\"cluster_ix\"):     plotdata_cluster = plotdata_cluster.droplevel(\"cluster_ix\").sort_index()     ax.plot(plotdata_cluster.index, np.exp(plotdata_cluster[\"prob_left\"]), label = cluster_ix) ax.set_xlim(*fragments.regions.window)  plotdata = simulation.peaks.query(\"gene == @design.gene_ix.iloc[0]\").sort_values(\"center\")  ax2 = panel.add_twinx() ax2.scatter(plotdata[\"center\"], [0] * len(plotdata), c = plotdata[\"size_scale\"]) ax2.set_xlim(*fragments.regions.window)  panel, ax = fig.main.add_under(chd.grid.Panel((width, 2))) plotdata = np.exp(design.groupby([\"size\", \"coordinate\"]).mean()[\"prob_right\"].unstack()) ax.matshow(plotdata, aspect = \"auto\") ax.set_yticks(np.arange(plotdata.shape[0])) ax.set_yticklabels(plotdata.index)  fig.plot() In\u00a0[41]: Copied! <pre>main = chd.grid.Grid(padding_height=0.1)\nfig = chd.grid.Figure(main)\n\nnbins = np.array(model.mixture.transform.nbins)\nbincuts = np.concatenate([[0], np.cumsum(nbins)])\nbinmids = bincuts[:-1] + nbins / 2\n\nax = main[0, 0] = chd.grid.Ax((10, 0.25))\nax = ax.ax\nplotdata = (model.mixture.transform.unnormalized_heights.data.cpu().numpy())[[gene_ix]]\nax.imshow(plotdata, aspect=\"auto\")\nax.set_yticks([])\nfor b in bincuts:\n    ax.axvline(b - 0.5, color=\"black\", lw=0.5)\nax.set_xlim(0 - 0.5, plotdata.shape[1] - 0.5)\nax.set_xticks([])\nax.set_ylabel(\"$h_0$\", rotation=0, ha=\"right\", va=\"center\")\n\nax = main[1, 0] = chd.grid.Ax(dim=(10, model.n_clusters * 0.25))\nax = ax.ax\nplotdata = model.decoder.delta_height_weight.data[gene_ix].cpu().numpy()\nax.imshow(\n    plotdata, aspect=\"auto\", cmap=mpl.cm.RdBu_r, vmax=np.log(2), vmin=np.log(1 / 2)\n)\nax.set_yticks(range(len(clustering.cluster_info)))\nax.set_yticklabels(clustering.cluster_info.index, rotation=0, ha=\"right\")\nfor b in bincuts:\n    ax.axvline(b - 0.5, color=\"black\", lw=0.5)\nax.set_xlim(-0.5, plotdata.shape[1] - 0.5)\n\nax.set_xticks(bincuts - 0.5, minor=True)\nax.set_xticks(binmids - 0.5)\nax.set_xticklabels(nbins)\nax.xaxis.set_tick_params(length=0)\nax.xaxis.set_tick_params(length=5, which=\"minor\")\nax.set_ylabel(\"$\\Delta h$\", rotation=0, ha=\"right\", va=\"center\")\n\nax.set_xlabel(\"Resolution\")\n\nfig.plot()\n</pre> main = chd.grid.Grid(padding_height=0.1) fig = chd.grid.Figure(main)  nbins = np.array(model.mixture.transform.nbins) bincuts = np.concatenate([[0], np.cumsum(nbins)]) binmids = bincuts[:-1] + nbins / 2  ax = main[0, 0] = chd.grid.Ax((10, 0.25)) ax = ax.ax plotdata = (model.mixture.transform.unnormalized_heights.data.cpu().numpy())[[gene_ix]] ax.imshow(plotdata, aspect=\"auto\") ax.set_yticks([]) for b in bincuts:     ax.axvline(b - 0.5, color=\"black\", lw=0.5) ax.set_xlim(0 - 0.5, plotdata.shape[1] - 0.5) ax.set_xticks([]) ax.set_ylabel(\"$h_0$\", rotation=0, ha=\"right\", va=\"center\")  ax = main[1, 0] = chd.grid.Ax(dim=(10, model.n_clusters * 0.25)) ax = ax.ax plotdata = model.decoder.delta_height_weight.data[gene_ix].cpu().numpy() ax.imshow(     plotdata, aspect=\"auto\", cmap=mpl.cm.RdBu_r, vmax=np.log(2), vmin=np.log(1 / 2) ) ax.set_yticks(range(len(clustering.cluster_info))) ax.set_yticklabels(clustering.cluster_info.index, rotation=0, ha=\"right\") for b in bincuts:     ax.axvline(b - 0.5, color=\"black\", lw=0.5) ax.set_xlim(-0.5, plotdata.shape[1] - 0.5)  ax.set_xticks(bincuts - 0.5, minor=True) ax.set_xticks(binmids - 0.5) ax.set_xticklabels(nbins) ax.xaxis.set_tick_params(length=0) ax.xaxis.set_tick_params(length=5, which=\"minor\") ax.set_ylabel(\"$\\Delta h$\", rotation=0, ha=\"right\", va=\"center\")  ax.set_xlabel(\"Resolution\")  fig.plot() In\u00a0[42]: Copied! <pre>scores = []\ngenescores = []\nfor model_id, model in models.items():\n    prediction_test = model.get_prediction(fragments, clustering, cell_ixs=fold[\"cells_test\"])\n    prediction_validation = model.get_prediction(fragments, clustering, cell_ixs=fold[\"cells_validation\"])\n    prediction_train = model.get_prediction(fragments, clustering, cell_ixs=fold[\"cells_train\"])\n    scores.append(\n        {\n            \"model_id\": model_id,\n            \"lik_test\": (prediction_test[\"likelihood_position\"]).sum().item(),\n            \"n_test\": len(fold[\"cells_test\"]),\n            \"lik_validation\": (prediction_validation[\"likelihood_position\"]).sum().item(),\n            \"n_validation\": len(fold[\"cells_validation\"]),\n            \"lik_train\": (prediction_train[\"likelihood_position\"]).sum().item(),\n            \"n_train\": len(fold[\"cells_train\"]),\n        }\n    )\n    genescores.append(\n        pd.DataFrame(\n            {\n                \"model_id\": model_id,\n                \"lik_test\": (prediction_test[\"likelihood_position\"]).sum(\"cell\").to_pandas(),\n                \"n_test\": len(fold[\"cells_test\"]),\n                \"lik_validation\": (prediction_validation[\"likelihood_position\"]).sum(\"cell\").to_pandas(),\n                \"n_validation\": len(fold[\"cells_validation\"]),\n                \"lik_train\": (prediction_train[\"likelihood_position\"]).sum(\"cell\").to_pandas(),\n                \"n_train\": len(fold[\"cells_train\"]),\n            }\n        )\n    )\nscores = pd.DataFrame(scores).set_index(\"model_id\")\ngenescores = (\n    pd.concat([genescores[i] for i in range(len(genescores))], axis=0).reset_index().set_index([\"model_id\", \"gene\"])\n)\n</pre> scores = [] genescores = [] for model_id, model in models.items():     prediction_test = model.get_prediction(fragments, clustering, cell_ixs=fold[\"cells_test\"])     prediction_validation = model.get_prediction(fragments, clustering, cell_ixs=fold[\"cells_validation\"])     prediction_train = model.get_prediction(fragments, clustering, cell_ixs=fold[\"cells_train\"])     scores.append(         {             \"model_id\": model_id,             \"lik_test\": (prediction_test[\"likelihood_position\"]).sum().item(),             \"n_test\": len(fold[\"cells_test\"]),             \"lik_validation\": (prediction_validation[\"likelihood_position\"]).sum().item(),             \"n_validation\": len(fold[\"cells_validation\"]),             \"lik_train\": (prediction_train[\"likelihood_position\"]).sum().item(),             \"n_train\": len(fold[\"cells_train\"]),         }     )     genescores.append(         pd.DataFrame(             {                 \"model_id\": model_id,                 \"lik_test\": (prediction_test[\"likelihood_position\"]).sum(\"cell\").to_pandas(),                 \"n_test\": len(fold[\"cells_test\"]),                 \"lik_validation\": (prediction_validation[\"likelihood_position\"]).sum(\"cell\").to_pandas(),                 \"n_validation\": len(fold[\"cells_validation\"]),                 \"lik_train\": (prediction_train[\"likelihood_position\"]).sum(\"cell\").to_pandas(),                 \"n_train\": len(fold[\"cells_train\"]),             }         )     ) scores = pd.DataFrame(scores).set_index(\"model_id\") genescores = (     pd.concat([genescores[i] for i in range(len(genescores))], axis=0).reset_index().set_index([\"model_id\", \"gene\"]) ) In\u00a0[43]: Copied! <pre>baseline_id = list(models.keys())[0]\nscores[[\"lr_test\", \"lr_validation\", \"lr_train\"]] = (\n    scores[[\"lik_test\", \"lik_validation\", \"lik_train\"]]\n    - scores[[\"lik_test\", \"lik_validation\", \"lik_train\"]].loc[baseline_id]\n)\nscores[[\"nlr_test\", \"nlr_validation\", \"nlr_train\"]] = (\n    scores[[\"lr_test\", \"lr_validation\", \"lr_train\"]].values / scores[[\"n_test\", \"n_validation\", \"n_train\"]].values\n)\ngenescores[[\"lr_test\", \"lr_validation\", \"lr_train\"]] = (\n    genescores[[\"lik_test\", \"lik_validation\", \"lik_train\"]]\n    - genescores[[\"lik_test\", \"lik_validation\", \"lik_train\"]].loc[baseline_id]\n)\ngenescores[[\"nlr_test\", \"nlr_validation\", \"nlr_train\"]] = (\n    genescores[[\"lr_test\", \"lr_validation\", \"lr_train\"]].values\n    / genescores[[\"n_test\", \"n_validation\", \"n_train\"]].values\n)\n</pre> baseline_id = list(models.keys())[0] scores[[\"lr_test\", \"lr_validation\", \"lr_train\"]] = (     scores[[\"lik_test\", \"lik_validation\", \"lik_train\"]]     - scores[[\"lik_test\", \"lik_validation\", \"lik_train\"]].loc[baseline_id] ) scores[[\"nlr_test\", \"nlr_validation\", \"nlr_train\"]] = (     scores[[\"lr_test\", \"lr_validation\", \"lr_train\"]].values / scores[[\"n_test\", \"n_validation\", \"n_train\"]].values ) genescores[[\"lr_test\", \"lr_validation\", \"lr_train\"]] = (     genescores[[\"lik_test\", \"lik_validation\", \"lik_train\"]]     - genescores[[\"lik_test\", \"lik_validation\", \"lik_train\"]].loc[baseline_id] ) genescores[[\"nlr_test\", \"nlr_validation\", \"nlr_train\"]] = (     genescores[[\"lr_test\", \"lr_validation\", \"lr_train\"]].values     / genescores[[\"n_test\", \"n_validation\", \"n_train\"]].values ) In\u00a0[44]: Copied! <pre>model_info = pd.DataFrame({\"model\": models.keys()}).set_index(\"model\")\nmodel_info[\"model_type\"] = model_info.index.map(lambda x: \"_\".join(x.split(\"_\")[:-1]))\nmodel_info = model_info.sort_values([\"model_type\"])\nmodel_info[\"ix\"] = np.arange(model_info.shape[0])\n</pre> model_info = pd.DataFrame({\"model\": models.keys()}).set_index(\"model\") model_info[\"model_type\"] = model_info.index.map(lambda x: \"_\".join(x.split(\"_\")[:-1])) model_info = model_info.sort_values([\"model_type\"]) model_info[\"ix\"] = np.arange(model_info.shape[0]) In\u00a0[45]: Copied! <pre>fig = chd.grid.Figure(chd.grid.Wrap(padding_width=0.1))\nheight = len(scores) * 0.2\n\nplotdata = scores.copy().loc[model_info.index]\n\npanel, ax = fig.main.add(chd.grid.Ax((1, height)))\nax.barh(plotdata.index, plotdata[\"lr_test\"])\nax.axvline(0, color=\"black\", linestyle=\"--\", lw=1)\nax.set_title(\"Test\")\nax.set_xlabel(\"Log-likehood ratio\")\n\npanel, ax = fig.main.add(chd.grid.Ax((1, height)))\nax.set_yticks([])\nax.barh(plotdata.index, plotdata[\"lr_validation\"])\nax.axvline(0, color=\"black\", linestyle=\"--\", lw=1)\nax.set_title(\"Validation\")\n\npanel, ax = fig.main.add(chd.grid.Ax((1, height)))\nax.set_yticks([])\nax.barh(plotdata.index, plotdata[\"lr_train\"])\nax.axvline(0, color=\"black\", linestyle=\"--\", lw=1)\nax.set_title(\"Train\")\nfig.plot()\n</pre> fig = chd.grid.Figure(chd.grid.Wrap(padding_width=0.1)) height = len(scores) * 0.2  plotdata = scores.copy().loc[model_info.index]  panel, ax = fig.main.add(chd.grid.Ax((1, height))) ax.barh(plotdata.index, plotdata[\"lr_test\"]) ax.axvline(0, color=\"black\", linestyle=\"--\", lw=1) ax.set_title(\"Test\") ax.set_xlabel(\"Log-likehood ratio\")  panel, ax = fig.main.add(chd.grid.Ax((1, height))) ax.set_yticks([]) ax.barh(plotdata.index, plotdata[\"lr_validation\"]) ax.axvline(0, color=\"black\", linestyle=\"--\", lw=1) ax.set_title(\"Validation\")  panel, ax = fig.main.add(chd.grid.Ax((1, height))) ax.set_yticks([]) ax.barh(plotdata.index, plotdata[\"lr_train\"]) ax.axvline(0, color=\"black\", linestyle=\"--\", lw=1) ax.set_title(\"Train\") fig.plot() In\u00a0[17]: Copied! <pre>genepositional = chd.models.diff.interpret.genepositional.GenePositional(\n    path=chd.get_output() / \"interpret\" / \"genepositional\"\n)\n</pre> genepositional = chd.models.diff.interpret.genepositional.GenePositional(     path=chd.get_output() / \"interpret\" / \"genepositional\" ) In\u00a0[22]: Copied! <pre>gene = \"G3\"\nmodel_id = \"original\"\n# model_id = \"original_128\"\n# model_id = \"original_256,128,64,32\"\n\ngenepositional.score(\n    fragments,\n    clustering,\n    [models[model_id]],\n    force=True,\n    genes=[gene],\n    # genes = transcriptome.gene_id([symbol])\n)\n</pre> gene = \"G3\" model_id = \"original\" # model_id = \"original_128\" # model_id = \"original_256,128,64,32\"  genepositional.score(     fragments,     clustering,     [models[model_id]],     force=True,     genes=[gene],     # genes = transcriptome.gene_id([symbol]) ) <pre>  0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> In\u00a0[23]: Copied! <pre>fig = chd.grid.Figure(chd.grid.Grid(padding_height=0.05, padding_width=0.05))\nwidth = 10\n\nplotdata, plotdata_mean = genepositional.get_plotdata(gene)\npanel_differential = chd.models.diff.plot.Differential(\n    plotdata, plotdata_mean, cluster_info=clustering.cluster_info, panel_height=0.5, width=width\n)\nfig.main.add_under(panel_differential)\n\n# panel_expression = chd.models.diff.plot.DifferentialExpression.from_transcriptome(\n#     transcriptome = transcriptome, clustering = clustering, gene = transcriptome.gene_id(symbol), panel_height = 0.5\n# )\n# fig.main.add_right(panel_expression, row = panel_differential)\n\nfig.plot()\n</pre> fig = chd.grid.Figure(chd.grid.Grid(padding_height=0.05, padding_width=0.05)) width = 10  plotdata, plotdata_mean = genepositional.get_plotdata(gene) panel_differential = chd.models.diff.plot.Differential(     plotdata, plotdata_mean, cluster_info=clustering.cluster_info, panel_height=0.5, width=width ) fig.main.add_under(panel_differential)  # panel_expression = chd.models.diff.plot.DifferentialExpression.from_transcriptome( #     transcriptome = transcriptome, clustering = clustering, gene = transcriptome.gene_id(symbol), panel_height = 0.5 # ) # fig.main.add_right(panel_expression, row = panel_differential)  fig.plot() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"benchmark/diff/simulated/#train","title":"Train\u00b6","text":""},{"location":"benchmark/diff/simulated/#score","title":"Score\u00b6","text":""},{"location":"benchmark/diff/simulated/#interpret","title":"Interpret\u00b6","text":""},{"location":"benchmark/diff/timing/","title":"Timing","text":"In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nimport seaborn as sns\n\nsns.set_style(\"ticks\")\n%config InlineBackend.figure_format='retina'\n\nimport tqdm.auto as tqdm\nimport torch\nimport os\nimport time\n</pre> %load_ext autoreload %autoreload 2  import numpy as np import pandas as pd  import matplotlib.pyplot as plt import matplotlib as mpl  import seaborn as sns  sns.set_style(\"ticks\") %config InlineBackend.figure_format='retina'  import tqdm.auto as tqdm import torch import os import time In\u00a0[2]: Copied! <pre>import chromatinhd as chd\n\nchd.set_default_device(\"cuda:1\")\n</pre> import chromatinhd as chd  chd.set_default_device(\"cuda:1\") In\u00a0[3]: Copied! <pre>dataset_folder_original = chd.get_output() / \"datasets\" / \"pbmc10k\"\ntranscriptome_original = chd.data.Transcriptome(dataset_folder_original / \"transcriptome\")\nfragments_original = chd.data.Fragments(dataset_folder_original / \"fragments\" / \"10k10k\")\n</pre> dataset_folder_original = chd.get_output() / \"datasets\" / \"pbmc10k\" transcriptome_original = chd.data.Transcriptome(dataset_folder_original / \"transcriptome\") fragments_original = chd.data.Fragments(dataset_folder_original / \"fragments\" / \"10k10k\") In\u00a0[4]: Copied! <pre>genes_oi = transcriptome_original.var.sort_values(\"dispersions_norm\", ascending=False).head(30).index\nregions = fragments_original.regions.filter_genes(genes_oi)\nfragments = fragments_original.filter_genes(regions)\nfragments.create_cellxgene_indptr()\ntranscriptome = transcriptome_original.filter_genes(regions.coordinates.index)\n</pre> genes_oi = transcriptome_original.var.sort_values(\"dispersions_norm\", ascending=False).head(30).index regions = fragments_original.regions.filter_genes(genes_oi) fragments = fragments_original.filter_genes(regions) fragments.create_cellxgene_indptr() transcriptome = transcriptome_original.filter_genes(regions.coordinates.index) In\u00a0[5]: Copied! <pre>folds = chd.data.folds.Folds()\nfolds.sample_cells(fragments, 5)\n</pre> folds = chd.data.folds.Folds() folds.sample_cells(fragments, 5) In\u00a0[6]: Copied! <pre>clustering = chd.data.Clustering.from_labels(transcriptome_original.obs[\"celltype\"])\n</pre> clustering = chd.data.Clustering.from_labels(transcriptome_original.obs[\"celltype\"]) In\u00a0[7]: Copied! <pre>fold = folds[0]\n</pre> fold = folds[0] In\u00a0[8]: Copied! <pre>models = {}\nscores = []\n</pre> models = {} scores = [] In\u00a0[10]: Copied! <pre>import logging\n\nlogger = chd.models.diff.trainer.trainer.logger\nlogger.setLevel(logging.DEBUG)\nlogger.handlers = []\n# logger.handlers = [logging.StreamHandler()]\n</pre> import logging  logger = chd.models.diff.trainer.trainer.logger logger.setLevel(logging.DEBUG) logger.handlers = [] # logger.handlers = [logging.StreamHandler()] In\u00a0[11]: Copied! <pre>devices = pd.DataFrame({\"device\": [\"cuda:0\", \"cuda:1\", \"cpu\"]}).set_index(\"device\")\nfor device in devices.index:\n    if device != \"cpu\":\n        devices.loc[device, \"label\"] = torch.cuda.get_device_properties(device).name\n    else:\n        devices.loc[device, \"label\"] = os.popen(\"lscpu\").read().split(\"\\n\")[13].split(\": \")[-1].lstrip()\n</pre> devices = pd.DataFrame({\"device\": [\"cuda:0\", \"cuda:1\", \"cpu\"]}).set_index(\"device\") for device in devices.index:     if device != \"cpu\":         devices.loc[device, \"label\"] = torch.cuda.get_device_properties(device).name     else:         devices.loc[device, \"label\"] = os.popen(\"lscpu\").read().split(\"\\n\")[13].split(\": \")[-1].lstrip() In\u00a0[12]: Copied! <pre>scores = pd.DataFrame({\"device\": devices.index}).set_index(\"device\")\n</pre> scores = pd.DataFrame({\"device\": devices.index}).set_index(\"device\") In\u00a0[14]: Copied! <pre>for device in devices.index:\n    start = time.time()\n    model = chd.models.diff.model.cutnf.Model(\n        fragments,\n        clustering,\n    )\n    model.train_model(fragments, clustering, fold, n_epochs=10, device=device)\n    models[device] = model\n    end = time.time()\n    scores.loc[device, \"train\"] = end - start\n</pre> for device in devices.index:     start = time.time()     model = chd.models.diff.model.cutnf.Model(         fragments,         clustering,     )     model.train_model(fragments, clustering, fold, n_epochs=10, device=device)     models[device] = model     end = time.time()     scores.loc[device, \"train\"] = end - start <pre>  0%|          | 0/410 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/410 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/410 [00:00&lt;?, ?it/s]</pre> In\u00a0[15]: Copied! <pre>for device in devices.index:\n    genepositional = chd.models.diff.interpret.genepositional.GenePositional(\n        path=chd.get_output() / \"interpret\" / \"genepositional\"\n    )\n\n    start = time.time()\n    genepositional.score(fragments, clustering, [models[device]], force=True, device=device)\n    end = time.time()\n    scores.loc[device, \"inference\"] = end - start\n</pre> for device in devices.index:     genepositional = chd.models.diff.interpret.genepositional.GenePositional(         path=chd.get_output() / \"interpret\" / \"genepositional\"     )      start = time.time()     genepositional.score(fragments, clustering, [models[device]], force=True, device=device)     end = time.time()     scores.loc[device, \"inference\"] = end - start <pre>  0%|          | 0/30 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/30 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/30 [00:00&lt;?, ?it/s]</pre> In\u00a0[16]: Copied! <pre>fig = chd.grid.Figure(chd.grid.Wrap(padding_width=0.1))\nheight = len(scores) * 0.2\n\nplotdata = scores.copy().loc[devices.index]\n\npanel, ax = fig.main.add(chd.grid.Ax((1, height)))\nax.barh(plotdata.index, plotdata[\"train\"])\nax.set_yticks(np.arange(len(devices)))\nax.set_yticklabels(devices.label)\nax.axvline(0, color=\"black\", linestyle=\"--\", lw=1)\nax.set_title(\"Training\")\nax.set_xlabel(\"seconds\")\n\npanel, ax = fig.main.add(chd.grid.Ax((1, height)))\nax.barh(plotdata.index, plotdata[\"inference\"])\nax.axvline(0, color=\"black\", linestyle=\"--\", lw=1)\nax.set_title(\"Inference\")\nax.set_yticks([])\nfig.plot()\n</pre> fig = chd.grid.Figure(chd.grid.Wrap(padding_width=0.1)) height = len(scores) * 0.2  plotdata = scores.copy().loc[devices.index]  panel, ax = fig.main.add(chd.grid.Ax((1, height))) ax.barh(plotdata.index, plotdata[\"train\"]) ax.set_yticks(np.arange(len(devices))) ax.set_yticklabels(devices.label) ax.axvline(0, color=\"black\", linestyle=\"--\", lw=1) ax.set_title(\"Training\") ax.set_xlabel(\"seconds\")  panel, ax = fig.main.add(chd.grid.Ax((1, height))) ax.barh(plotdata.index, plotdata[\"inference\"]) ax.axvline(0, color=\"black\", linestyle=\"--\", lw=1) ax.set_title(\"Inference\") ax.set_yticks([]) fig.plot()"},{"location":"quickstart/0_install/","title":"Installation","text":"<pre>\n# using pip\npip install chromatinhd\n\n# (soon) using conda\nconda install -c bioconda chromatinhd\n\n# from github\npip install git+https://github.com/DeplanckeLab/ChromatinHD\n</pre> <p>To use the GPU, ensure that a PyTorch version was installed with cuda enabled:</p> In\u00a0[3]: Copied! <pre>import torch\n\ntorch.cuda.is_available()  # should return True\ntorch.cuda.device_count()  # should be &gt;= 1\n</pre> import torch  torch.cuda.is_available()  # should return True torch.cuda.device_count()  # should be &gt;= 1 <p>If not, follow the instructions at https://pytorch.org/get-started/locally/. You may have to re-install PyTorch.</p> In\u00a0[\u00a0]: hide_output Copied! <pre>import chromatinhd as chd\n</pre> import chromatinhd as chd"},{"location":"quickstart/0_install/#installation","title":"Installation\u00b6","text":""},{"location":"quickstart/0_install/#frequently-asked-questions","title":"Frequently asked questions\u00b6","text":""},{"location":"quickstart/1_data/","title":"Data preparation","text":"In\u00a0[\u00a0]: hide_output Copied! <pre>import chromatinhd as chd\n</pre> import chromatinhd as chd <p>To speed up training and inference, ChromatinHD stores several intermediate files to disk. This includes preprocessed data and models. These will be stored in the example folder.</p> In\u00a0[3]: Copied! <pre>import pathlib\n\ndataset_folder = pathlib.Path(\"example\")\ndataset_folder.mkdir(exist_ok=True)\n</pre> import pathlib  dataset_folder = pathlib.Path(\"example\") dataset_folder.mkdir(exist_ok=True) <p>For this quickstart, we will use a tiny example dataset extracted from the 10X multiome PBMC example data. We'll copy over both the h5ad for the transcriptomics data, and the fragments.tsv for the accessibility data.</p> In\u00a0[5]: Copied! <pre>import pkg_resources\nimport shutil\n\nDATA_PATH = pathlib.Path(pkg_resources.resource_filename(\"chromatinhd\", \"data/examples/pbmc10ktiny/\"))\n\n# copy all files from data path to dataset folder\nfor file in DATA_PATH.iterdir():\n    shutil.copy(file, dataset_folder / file.name)\n</pre> import pkg_resources import shutil  DATA_PATH = pathlib.Path(pkg_resources.resource_filename(\"chromatinhd\", \"data/examples/pbmc10ktiny/\"))  # copy all files from data path to dataset folder for file in DATA_PATH.iterdir():     shutil.copy(file, dataset_folder / file.name) In\u00a0[6]: Copied! <pre>!ls {dataset_folder}\n</pre> !ls {dataset_folder} <pre>fragments.tsv  fragments.tsv.gz  fragments.tsv.gz.tbi  transcriptome.h5ad\n</pre> In\u00a0[7]: Copied! <pre>import scanpy as sc\n\nadata = sc.read(dataset_folder / \"transcriptome.h5ad\")\n</pre> import scanpy as sc  adata = sc.read(dataset_folder / \"transcriptome.h5ad\") In\u00a0[8]: Copied! <pre>transcriptome = chd.data.Transcriptome.from_adata(adata, path=dataset_folder / \"transcriptome\")\n</pre> transcriptome = chd.data.Transcriptome.from_adata(adata, path=dataset_folder / \"transcriptome\") In\u00a0[9]: Copied! <pre>!ls {dataset_folder}/*\n</pre> !ls {dataset_folder}/* <pre>example/fragments.tsv\t  example/fragments.tsv.gz.tbi\nexample/fragments.tsv.gz  example/transcriptome.h5ad\n\nexample/transcriptome:\nadata.pkl  obs.tsv  var.tsv  X.pkl\n</pre> Batch effects <p>       Currently, none of the ChromatinHD models directly supports batch effects, although this will likely be added in the future. If you have batch effects, the current recommended workflow depends on the source of the batch effect:     <ul> <li>If it mainly comes from ambient mRNA, we recommend to use the corrected data. The reason is that this batch effect will likely not be present in the ATAC-seq data.</li> <li>If it mainly comes from biological differences (e.g. cell stress, patient differences, ...), we recommend to use the uncorrected data. The reason is that this batch effect will likely be reflected in the ATAC-seq data as well, given that the genes are truly differentially regulated between the cells.</li> </ul> </p> <p>ChromatinHD defines a set of regions of interest, typically surrounding transcription start sites of a gene. Since we typically do not know which transcription start sites are used, we can either use the canonical ones (as determined by e.g. ENCODE) or use the ATAC-seq data to select the one that is most open. We will use the latter option here.</p> <p>We first get the transcripts for each gene.</p> In\u00a0[10]: Copied! <pre>transcripts = chd.biomart.get_transcripts(chd.biomart.Dataset.from_genome(\"GRCh38\"), gene_ids=transcriptome.var.index)\nfragments_file = dataset_folder / \"fragments.tsv.gz\"\ntranscripts = chd.data.regions.select_tss_from_fragments(transcripts, fragments_file)\n</pre> transcripts = chd.biomart.get_transcripts(chd.biomart.Dataset.from_genome(\"GRCh38\"), gene_ids=transcriptome.var.index) fragments_file = dataset_folder / \"fragments.tsv.gz\" transcripts = chd.data.regions.select_tss_from_fragments(transcripts, fragments_file) <pre>  0%|          | 0/504 [00:00&lt;?, ?it/s]</pre> <p>Now we can define the regions around the TSS. In this case we choose -10kb and +10kb around a TSS, although in real situations this will typically be much bigger (e.g. -100kb - +100kb)</p> In\u00a0[11]: Copied! <pre>regions = chd.data.Regions.from_transcripts(\n    transcripts,\n    path=dataset_folder / \"regions\",\n    window=[-10000, 10000],\n)\n</pre> regions = chd.data.Regions.from_transcripts(     transcripts,     path=dataset_folder / \"regions\",     window=[-10000, 10000], ) In\u00a0[12]: Copied! <pre>!ls {dataset_folder}/*\n</pre> !ls {dataset_folder}/* <pre>example/fragments.tsv\t  example/fragments.tsv.gz.tbi\nexample/fragments.tsv.gz  example/transcriptome.h5ad\n\nexample/regions:\ncoordinates.tsv  window.pkl\n\nexample/transcriptome:\nadata.pkl  obs.tsv  var.tsv  X.pkl\n</pre> Gene vs TSS coordinates <p>The coordinates of the canonical transcript often do not correspond to the gene annotation that are used for e.g. RNA-seq analysis. The reason is that gene coordinates are defined based on the largest transcript in both ends.   </p> <p>The fragment file should be indexed with tabix:</p> In\u00a0[13]: Copied! <pre>if not (dataset_folder / \"fragments.tsv.gz.tbi\").exists():\n    import subprocess\n\n    subprocess.run(\n        [\n            \"tabix\",\n            dataset_folder / \"fragments.tsv.gz\",\n        ]\n    )\n</pre> if not (dataset_folder / \"fragments.tsv.gz.tbi\").exists():     import subprocess      subprocess.run(         [             \"tabix\",             dataset_folder / \"fragments.tsv.gz\",         ]     ) In\u00a0[14]: Copied! <pre>fragments = chd.data.Fragments.from_fragments_tsv(\n    dataset_folder / \"fragments.tsv.gz\",\n    regions,\n    obs=transcriptome.obs,\n    path=dataset_folder / \"fragments\",\n)\n</pre> fragments = chd.data.Fragments.from_fragments_tsv(     dataset_folder / \"fragments.tsv.gz\",     regions,     obs=transcriptome.obs,     path=dataset_folder / \"fragments\", ) <pre>Processing fragments:   0%|          | 0/50 [00:00&lt;?, ?it/s]</pre> In\u00a0[15]: Copied! <pre>fragments.create_cellxgene_indptr()\n</pre> fragments.create_cellxgene_indptr() In\u00a0[16]: Copied! <pre>!ls {dataset_folder}/*\n</pre> !ls {dataset_folder}/* <pre>example/fragments.tsv\t  example/fragments.tsv.gz.tbi\nexample/fragments.tsv.gz  example/transcriptome.h5ad\n\nexample/fragments:\ncellxgene_indptr.pkl  coordinates.pkl  mapping.pkl  obs.tsv  regions  var.tsv\n\nexample/regions:\ncoordinates.tsv  window.pkl\n\nexample/transcriptome:\nadata.pkl  obs.tsv  var.tsv  X.pkl\n</pre> <p>The final set of data are the training folds that will be used to train - and test - the model. For basic models this is simply done by randomly sampling cells.</p> In\u00a0[30]: Copied! <pre>folds = chd.data.folds.Folds(dataset_folder / \"folds\" / \"5x1\").sample_cells(fragments, 5, 1)\n</pre> folds = chd.data.folds.Folds(dataset_folder / \"folds\" / \"5x1\").sample_cells(fragments, 5, 1) <p>Although only needed for some models, e.g. ChromatinHD-diff, for interpretation it can be helpful to store some clustering.</p> In\u00a0[17]: Copied! <pre>clustering = chd.data.Clustering.from_labels(adata.obs[\"celltype\"], path=dataset_folder / \"clustering\")\n</pre> clustering = chd.data.Clustering.from_labels(adata.obs[\"celltype\"], path=dataset_folder / \"clustering\") In\u00a0[19]: Copied! <pre>!ls {clustering.path}\n</pre> !ls {clustering.path} <pre>cluster_info.pkl  labels.pkl\n</pre> This functionality is still under construction and is only used for visualization/interpretation use cases for now. <p>Let's first download the HOCOMOCO motif data:</p> In\u00a0[20]: Copied! <pre>motifs_folder = dataset_folder / \"motifs\"\nmotifs_folder.mkdir(exist_ok=True, parents=True)\n\n# download cutoffs, pwms and annotations\n!wget https://hocomoco11.autosome.org/final_bundle/hocomoco11/core/HUMAN/mono/HOCOMOCOv11_core_standard_thresholds_HUMAN_mono.txt -O {motifs_folder}/pwm_cutoffs.txt\n!wget https://hocomoco11.autosome.org/final_bundle/hocomoco11/core/HUMAN/mono/HOCOMOCOv11_core_pwms_HUMAN_mono.txt -O {motifs_folder}/pwms.txt\n!wget https://hocomoco11.autosome.org/final_bundle/hocomoco11/core/HUMAN/mono/HOCOMOCOv11_core_annotation_HUMAN_mono.tsv -O {motifs_folder}/annot.txt\n</pre> motifs_folder = dataset_folder / \"motifs\" motifs_folder.mkdir(exist_ok=True, parents=True)  # download cutoffs, pwms and annotations !wget https://hocomoco11.autosome.org/final_bundle/hocomoco11/core/HUMAN/mono/HOCOMOCOv11_core_standard_thresholds_HUMAN_mono.txt -O {motifs_folder}/pwm_cutoffs.txt !wget https://hocomoco11.autosome.org/final_bundle/hocomoco11/core/HUMAN/mono/HOCOMOCOv11_core_pwms_HUMAN_mono.txt -O {motifs_folder}/pwms.txt !wget https://hocomoco11.autosome.org/final_bundle/hocomoco11/core/HUMAN/mono/HOCOMOCOv11_core_annotation_HUMAN_mono.tsv -O {motifs_folder}/annot.txt <pre>--2023-08-06 09:31:32--  https://hocomoco11.autosome.org/final_bundle/hocomoco11/core/HUMAN/mono/HOCOMOCOv11_core_standard_thresholds_HUMAN_mono.txt\nResolving hocomoco11.autosome.org (hocomoco11.autosome.org)... 188.166.32.228\nConnecting to hocomoco11.autosome.org (hocomoco11.autosome.org)|188.166.32.228|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 22556 (22K) [text/plain]\nSaving to: \u2018example/motifs/pwm_cutoffs.txt\u2019\n\nexample/motifs/pwm_ 100%[===================&gt;]  22.03K  --.-KB/s    in 0s      \n\n2023-08-06 09:31:32 (338 MB/s) - \u2018example/motifs/pwm_cutoffs.txt\u2019 saved [22556/22556]\n\n--2023-08-06 09:31:33--  https://hocomoco11.autosome.org/final_bundle/hocomoco11/core/HUMAN/mono/HOCOMOCOv11_core_pwms_HUMAN_mono.txt\nResolving hocomoco11.autosome.org (hocomoco11.autosome.org)... 188.166.32.228\nConnecting to hocomoco11.autosome.org (hocomoco11.autosome.org)|188.166.32.228|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 458211 (447K) [text/plain]\nSaving to: \u2018example/motifs/pwms.txt\u2019\n\nexample/motifs/pwms 100%[===================&gt;] 447.47K  --.-KB/s    in 0.06s   \n\n2023-08-06 09:31:33 (7.00 MB/s) - \u2018example/motifs/pwms.txt\u2019 saved [458211/458211]\n\n--2023-08-06 09:31:33--  https://hocomoco11.autosome.org/final_bundle/hocomoco11/core/HUMAN/mono/HOCOMOCOv11_core_annotation_HUMAN_mono.tsv\nResolving hocomoco11.autosome.org (hocomoco11.autosome.org)... 188.166.32.228\nConnecting to hocomoco11.autosome.org (hocomoco11.autosome.org)|188.166.32.228|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 77641 (76K) [application/octet-stream]\nSaving to: \u2018example/motifs/annot.txt\u2019\n\nexample/motifs/anno 100%[===================&gt;]  75.82K  --.-KB/s    in 0.02s   \n\n2023-08-06 09:31:33 (4.75 MB/s) - \u2018example/motifs/annot.txt\u2019 saved [77641/77641]\n\n</pre> In\u00a0[21]: Copied! <pre>import numpy as np\nimport pandas as pd\n</pre> import numpy as np import pandas as pd <p>We read in all PWMs:</p> In\u00a0[23]: Copied! <pre>pwms = {}\nmotif = None\nfor line in (motifs_folder / \"pwms.txt\").open():\n    if line.startswith(\"&gt;\"):\n        if motif is not None:\n            pwms[motif_id] = motif\n        motif_id = line[1:].strip(\"\\n\")\n        motif = []\n    else:\n        motif.append([float(x) for x in line.split(\"\\t\")])\npwms = {motif_id: np.array(pwm) for motif_id, pwm in pwms.items()}\n</pre> pwms = {} motif = None for line in (motifs_folder / \"pwms.txt\").open():     if line.startswith(\"&gt;\"):         if motif is not None:             pwms[motif_id] = motif         motif_id = line[1:].strip(\"\\n\")         motif = []     else:         motif.append([float(x) for x in line.split(\"\\t\")]) pwms = {motif_id: np.array(pwm) for motif_id, pwm in pwms.items()} <p>And gather some metadata about each motif</p> In\u00a0[24]: Copied! <pre>motifs = pd.DataFrame({\"motif\": pwms.keys()}).set_index(\"motif\")\nmotif_cutoffs = pd.read_table(\n    motifs_folder / \"pwm_cutoffs.txt\",\n    names=[\"motif\", \"cutoff_001\", \"cutoff_0005\", \"cutoff_0001\"],\n    skiprows=1,\n).set_index(\"motif\")\nmotifs = motifs.join(motif_cutoffs)\nannot = (\n    pd.read_table(motifs_folder / \"annot.txt\")\n    .rename(columns={\"Model\": \"motif\", \"Transcription factor\": \"gene_label\"})\n    .set_index(\"motif\")\n)\nmotifs = motifs.join(annot)\n</pre> motifs = pd.DataFrame({\"motif\": pwms.keys()}).set_index(\"motif\") motif_cutoffs = pd.read_table(     motifs_folder / \"pwm_cutoffs.txt\",     names=[\"motif\", \"cutoff_001\", \"cutoff_0005\", \"cutoff_0001\"],     skiprows=1, ).set_index(\"motif\") motifs = motifs.join(motif_cutoffs) annot = (     pd.read_table(motifs_folder / \"annot.txt\")     .rename(columns={\"Model\": \"motif\", \"Transcription factor\": \"gene_label\"})     .set_index(\"motif\") ) motifs = motifs.join(annot) <p>You also need to provide the location where the genome fasta file is stored. In our case this is located at /data/genome/GRCh38/, which was installed using <code>genomepy.install_genome(\"GRCh38\", genomes_dir = \"/data/genome/\")</code>.</p> In\u00a0[27]: Copied! <pre>import genomepy\n\ngenomepy.install_genome(\"GRCh38\", genomes_dir=\"/data/genome/\")\n\nfasta_file = \"/data/genome/GRCh38/GRCh38.fa\"\n</pre> import genomepy  genomepy.install_genome(\"GRCh38\", genomes_dir=\"/data/genome/\")  fasta_file = \"/data/genome/GRCh38/GRCh38.fa\" <p>Motifs can than be scanned within the regions as follows:</p> In\u00a0[28]: Copied! <pre>motifscan = chd.data.Motifscan.from_pwms(\n    pwms,\n    regions,\n    motifs=motifs,\n    cutoff_col=\"cutoff_0001\",\n    fasta_file=fasta_file,\n    path=dataset_folder / \"motifscan\",\n    device=\"cuda\",\n)\n</pre> motifscan = chd.data.Motifscan.from_pwms(     pwms,     regions,     motifs=motifs,     cutoff_col=\"cutoff_0001\",     fasta_file=fasta_file,     path=dataset_folder / \"motifscan\",     device=\"cuda\", ) <pre>  0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> In\u00a0[29]: Copied! <pre>!ls {motifscan.path}\n</pre> !ls {motifscan.path} <pre>indices.pkl  indptr.pkl  motifs.pkl  scores.pkl  strands.pkl\n</pre>"},{"location":"quickstart/1_data/#data-preparation","title":"Data preparation\u00b6","text":""},{"location":"quickstart/1_data/#transcriptomics","title":"Transcriptomics\u00b6","text":""},{"location":"quickstart/1_data/#regions-of-interest","title":"Regions of interest\u00b6","text":""},{"location":"quickstart/1_data/#atac-seq","title":"ATAC-seq\u00b6","text":"<p>ChromatinHD simply requires a <code>fragments.tsv</code> file. This contains for each fragment its chromosome, start, end and cell barcode.</p> <ul> <li>When using Cellranger, this file will be produced by the pipeline.</li> <li>If you have a bam file, you can use sinto to create the fragment file</li> </ul>"},{"location":"quickstart/1_data/#training-folds","title":"Training folds\u00b6","text":""},{"location":"quickstart/1_data/#optional-data","title":"Optional data\u00b6\u00b6","text":""},{"location":"quickstart/1_data/#clusters","title":"Clusters\u00b6","text":""},{"location":"quickstart/1_data/#motif-scan","title":"Motif scan\u00b6\u00b6","text":""},{"location":"quickstart/2_pred/","title":"ChromatinHD-pred","text":"In\u00a0[\u00a0]: hide_output Copied! <pre>import chromatinhd as chd\nimport matplotlib.pyplot as plt\n</pre> import chromatinhd as chd import matplotlib.pyplot as plt <p>ChromatinHD-pred uses accessibility fragments to predict gene expression. As such, it can detect features such as broad or narrow positioning of fragments, or fragment sizes, that are predictive for gene expression.</p> <p>We first load in all the input data which was created in the data preparation tutorial.</p> In\u00a0[221]: Copied! <pre>import pathlib\n\ndataset_folder = pathlib.Path(\"example\")\nfragments = chd.data.Fragments(dataset_folder / \"fragments\")\ntranscriptome = chd.data.Transcriptome(dataset_folder / \"transcriptome\")\nfolds = chd.data.folds.Folds(dataset_folder / \"folds\" / \"5x1\")\n</pre> import pathlib  dataset_folder = pathlib.Path(\"example\") fragments = chd.data.Fragments(dataset_folder / \"fragments\") transcriptome = chd.data.Transcriptome(dataset_folder / \"transcriptome\") folds = chd.data.folds.Folds(dataset_folder / \"folds\" / \"5x1\") <p>The basic ChromatinHD-pred model</p> In\u00a0[222]: Copied! <pre>models = chd.models.pred.model.additive.Models(dataset_folder / \"models\" / \"additive\", reset=True)\n</pre> models = chd.models.pred.model.additive.Models(dataset_folder / \"models\" / \"additive\", reset=True) In\u00a0[\u00a0]: hide_output Copied! <pre>models.train_models(fragments, transcriptome, folds, device=\"cuda\")\n</pre> models.train_models(fragments, transcriptome, folds, device=\"cuda\") <p>We will first check whether the model learned something, by comparing the predictive performance with a baseline</p> In\u00a0[224]: Copied! <pre>gene_cors = models.get_gene_cors(fragments, transcriptome, folds, device=\"cuda\")\ngene_cors[\"symbol\"] = gene_cors.index.map(transcriptome.symbol)\n</pre> gene_cors = models.get_gene_cors(fragments, transcriptome, folds, device=\"cuda\") gene_cors[\"symbol\"] = gene_cors.index.map(transcriptome.symbol) In\u00a0[225]: Copied! <pre>gene_cors.sort_values(\"deltacor\", ascending=False).head(10)\n</pre> gene_cors.sort_values(\"deltacor\", ascending=False).head(10) Out[225]: cor_predicted cor_n_fragments n_fragments deltacor symbol gene ENSG00000100721 0.458880 0.135177 41.2 0.323703 TCL1A ENSG00000169429 0.475118 0.169594 330.6 0.305523 CXCL8 ENSG00000185666 0.302153 0.029521 22.2 0.272632 SYN3 ENSG00000143297 0.312124 0.039584 37.8 0.272540 FCRL5 ENSG00000239264 0.332108 0.070291 286.0 0.261817 TXNDC5 ENSG00000105369 0.532800 0.283968 1820.6 0.248832 CD79A ENSG00000221866 0.307623 0.074430 89.8 0.233193 PLXNA4 ENSG00000173068 0.373886 0.152450 81.6 0.221436 BNC2 ENSG00000156738 0.622245 0.402413 204.6 0.219832 MS4A1 ENSG00000012124 0.610037 0.396753 465.8 0.213284 CD22 In\u00a0[226]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(4, 4))\n\nfor name, group in gene_cors.iterrows():\n    ax.plot([0, 1], group[[\"cor_n_fragments\", \"cor_predicted\"]], color=\"#3338\", zorder=0, marker=\"o\", markersize=2)\nax.boxplot(\n    gene_cors[[\"cor_n_fragments\", \"cor_predicted\"]].values,\n    positions=[0, 1],\n    widths=0.1,\n    showfliers=False,\n    showmeans=True,\n    meanline=True,\n    meanprops={\"color\": \"red\", \"linewidth\": 2},\n)\nax.set_xticks([0, 1])\nax.set_xticklabels([\"# fragments\", \"ChromatinHD-pred\"])\nax.set_ylabel(\"$cor$\")\n</pre> import pandas as pd import matplotlib.pyplot as plt  fig, ax = plt.subplots(figsize=(4, 4))  for name, group in gene_cors.iterrows():     ax.plot([0, 1], group[[\"cor_n_fragments\", \"cor_predicted\"]], color=\"#3338\", zorder=0, marker=\"o\", markersize=2) ax.boxplot(     gene_cors[[\"cor_n_fragments\", \"cor_predicted\"]].values,     positions=[0, 1],     widths=0.1,     showfliers=False,     showmeans=True,     meanline=True,     meanprops={\"color\": \"red\", \"linewidth\": 2}, ) ax.set_xticks([0, 1]) ax.set_xticklabels([\"# fragments\", \"ChromatinHD-pred\"]) ax.set_ylabel(\"$cor$\") Out[226]: <pre>''</pre> <p>Note that every gene gains from the ChromatinHD model, even if some only gain a little. The genes with a low $\\Delta cor$ are often those with only a few fragments:</p> In\u00a0[227]: Copied! <pre>fig, ax = plt.subplots(figsize=(4, 4))\nax.scatter(gene_cors[\"n_fragments\"], gene_cors[\"deltacor\"])\nax.set_ylabel(\"$\\\\Delta$ cor\")\nax.set_xlabel(\"# fragments\")\nax.set_xscale(\"log\")\n</pre> fig, ax = plt.subplots(figsize=(4, 4)) ax.scatter(gene_cors[\"n_fragments\"], gene_cors[\"deltacor\"]) ax.set_ylabel(\"$\\\\Delta$ cor\") ax.set_xlabel(\"# fragments\") ax.set_xscale(\"log\") <p>To determine which regions were important for the model to predict gene expression, we will censor fragments from windows of various sizes, and then check whether the model performance on a set of test cells decreased. This functionality is implemented in the <code>GeneMultiWindow</code> class. This will only run the censoring for a subset of genes to speed up interpretation.</p> In\u00a0[228]: Copied! <pre>censorer = chd.models.pred.interpret.MultiWindowCensorer(fragments.regions.window)\ngenemultiwindow = chd.models.pred.interpret.GeneMultiWindow(models.path / \"interpret\" / \"genemultiwindow\")\n</pre> censorer = chd.models.pred.interpret.MultiWindowCensorer(fragments.regions.window) genemultiwindow = chd.models.pred.interpret.GeneMultiWindow(models.path / \"interpret\" / \"genemultiwindow\") In\u00a0[229]: Copied! <pre>genemultiwindow.score(\n    fragments,\n    transcriptome,\n    models,\n    folds,\n    transcriptome.gene_id(\n        [\n            \"CCL4\",\n            \"IL1B\",\n            \"EBF1\",\n            \"PAX5\",\n            \"CD79A\",\n            \"RHEX\",\n        ]\n    ),\n    censorer=censorer,\n)\n</pre> genemultiwindow.score(     fragments,     transcriptome,     models,     folds,     transcriptome.gene_id(         [             \"CCL4\",             \"IL1B\",             \"EBF1\",             \"PAX5\",             \"CD79A\",             \"RHEX\",         ]     ),     censorer=censorer, ) <pre>  0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> In\u00a0[230]: Copied! <pre>genemultiwindow.interpolate()\n</pre> genemultiwindow.interpolate() <pre>  0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> In\u00a0[231]: Copied! <pre>symbol = \"EBF1\"\n\nfig = chd.grid.Figure(chd.grid.Grid(padding_height=0.05))\nwidth = 10\n\nregion = fragments.regions.coordinates.loc[transcriptome.gene_id(symbol)]\npanel_genes = chd.plot.genome.genes.Genes.from_region(region, width=width)\nfig.main.add_under(panel_genes)\n\npanel_pileup = chd.models.pred.plot.Pileup.from_genemultiwindow(\n    genemultiwindow, transcriptome.gene_id(symbol), width=width\n)\nfig.main.add_under(panel_pileup)\n\npanel_predictivity = chd.models.pred.plot.Predictivity.from_genemultiwindow(\n    genemultiwindow, transcriptome.gene_id(symbol), width=width\n)\nfig.main.add_under(panel_predictivity)\n\nfig.plot()\n</pre> symbol = \"EBF1\"  fig = chd.grid.Figure(chd.grid.Grid(padding_height=0.05)) width = 10  region = fragments.regions.coordinates.loc[transcriptome.gene_id(symbol)] panel_genes = chd.plot.genome.genes.Genes.from_region(region, width=width) fig.main.add_under(panel_genes)  panel_pileup = chd.models.pred.plot.Pileup.from_genemultiwindow(     genemultiwindow, transcriptome.gene_id(symbol), width=width ) fig.main.add_under(panel_pileup)  panel_predictivity = chd.models.pred.plot.Predictivity.from_genemultiwindow(     genemultiwindow, transcriptome.gene_id(symbol), width=width ) fig.main.add_under(panel_predictivity)  fig.plot() <p>In a similar fashion we can determine the co-predictivity per position.</p> In\u00a0[216]: Copied! <pre>censorer = chd.models.pred.interpret.WindowCensorer(fragments.regions.window)\ngenepairwindow = chd.models.pred.interpret.GenePairWindow(models.path / \"interpret\" / \"genepairwindow\", reset=True)\ngenepairwindow.score(fragments, transcriptome, models, folds, censorer=censorer, genes=transcriptome.gene_id([\"CCL4\"]))\n</pre> censorer = chd.models.pred.interpret.WindowCensorer(fragments.regions.window) genepairwindow = chd.models.pred.interpret.GenePairWindow(models.path / \"interpret\" / \"genepairwindow\", reset=True) genepairwindow.score(fragments, transcriptome, models, folds, censorer=censorer, genes=transcriptome.gene_id([\"CCL4\"])) <pre>  0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>(5, 200, 200)\n</pre> In\u00a0[217]: Copied! <pre>symbol = \"CCL4\"\n\nfig = chd.grid.Figure(chd.grid.Grid(padding_height=0.05))\nwidth = 10\n\n# genes\nregion = fragments.regions.coordinates.loc[transcriptome.gene_id(symbol)]\npanel_genes = chd.plot.genome.genes.Genes.from_region(region, width=width)\nfig.main.add_under(panel_genes)\n\n# pileup\npanel_pileup = chd.models.pred.plot.Pileup.from_genemultiwindow(\n    genemultiwindow, transcriptome.gene_id(symbol), width=width\n)\nfig.main.add_under(panel_pileup)\n\n# predictivity\npanel_predictivity = chd.models.pred.plot.Predictivity.from_genemultiwindow(\n    genemultiwindow, transcriptome.gene_id(symbol), width=width\n)\nfig.main.add_under(panel_predictivity)\n\n# copredictivity\npanel_copredictivity = chd.models.pred.plot.Copredictivity.from_genepairwindow(\n    genepairwindow, transcriptome.gene_id(symbol), width=width\n)\nfig.main.add_under(panel_copredictivity)\n\nfig.plot()\n</pre> symbol = \"CCL4\"  fig = chd.grid.Figure(chd.grid.Grid(padding_height=0.05)) width = 10  # genes region = fragments.regions.coordinates.loc[transcriptome.gene_id(symbol)] panel_genes = chd.plot.genome.genes.Genes.from_region(region, width=width) fig.main.add_under(panel_genes)  # pileup panel_pileup = chd.models.pred.plot.Pileup.from_genemultiwindow(     genemultiwindow, transcriptome.gene_id(symbol), width=width ) fig.main.add_under(panel_pileup)  # predictivity panel_predictivity = chd.models.pred.plot.Predictivity.from_genemultiwindow(     genemultiwindow, transcriptome.gene_id(symbol), width=width ) fig.main.add_under(panel_predictivity)  # copredictivity panel_copredictivity = chd.models.pred.plot.Copredictivity.from_genepairwindow(     genepairwindow, transcriptome.gene_id(symbol), width=width ) fig.main.add_under(panel_copredictivity)  fig.plot() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"quickstart/2_pred/#chromatinhd-pred","title":"ChromatinHD-pred\u00b6","text":""},{"location":"quickstart/2_pred/#train-the-models","title":"Train the models\u00b6","text":""},{"location":"quickstart/2_pred/#some-quality-checks","title":"Some quality checks\u00b6","text":""},{"location":"quickstart/2_pred/#predictivity-per-position","title":"Predictivity per position\u00b6","text":""},{"location":"quickstart/2_pred/#co-predictivity-per-position","title":"Co-predictivity per position\u00b6","text":""},{"location":"quickstart/3_diff/","title":"ChromatinHD-diff","text":"In\u00a0[\u00a0]: hide_output Copied! <pre>import chromatinhd as chd\nimport matplotlib.pyplot as plt\n</pre> import chromatinhd as chd import matplotlib.pyplot as plt <p>ChromatinHD-pred uses accessibility fragments to predict gene expression. As such, it can detect features such as broad or narrow positioning of fragments, or fragment sizes, that are predictive for gene expression.</p> <p>We first load in all the input data which was created in the data preparation tutorial.</p> In\u00a0[8]: Copied! <pre>import pathlib\n\ndataset_folder = pathlib.Path(\"example\")\nfragments = chd.data.Fragments(dataset_folder / \"fragments\")\ntranscriptome = chd.data.Transcriptome(dataset_folder / \"transcriptome\")\nfolds = chd.data.folds.Folds(dataset_folder / \"folds\" / \"5x1\")\nclustering = chd.data.Clustering(dataset_folder / \"clustering\")\n</pre> import pathlib  dataset_folder = pathlib.Path(\"example\") fragments = chd.data.Fragments(dataset_folder / \"fragments\") transcriptome = chd.data.Transcriptome(dataset_folder / \"transcriptome\") folds = chd.data.folds.Folds(dataset_folder / \"folds\" / \"5x1\") clustering = chd.data.Clustering(dataset_folder / \"clustering\") <p>The basic ChromatinHD-diff model</p> In\u00a0[16]: Copied! <pre>models = chd.models.diff.model.cutnf.Models(dataset_folder / \"models\" / \"cutnf\", reset=True)\n</pre> models = chd.models.diff.model.cutnf.Models(dataset_folder / \"models\" / \"cutnf\", reset=True) In\u00a0[\u00a0]: hide_output Copied! <pre>models.train_models(fragments, clustering, folds)\n</pre> models.train_models(fragments, clustering, folds) <p>Currently, the ChromatinHD-model is purely positional, i.e. it only looks whether Tn5 insertion sites increase or decrease within a region. As such, we can only interpret it positionally:</p> In\u00a0[18]: Copied! <pre>import chromatinhd.models.diff.interpret.genepositional\n</pre> import chromatinhd.models.diff.interpret.genepositional In\u00a0[19]: Copied! <pre>clustering.cluster_info.index.name = \"cluster\"\n</pre> clustering.cluster_info.index.name = \"cluster\" In\u00a0[20]: Copied! <pre>genepositional = chromatinhd.models.diff.interpret.genepositional.GenePositional(\n    path=models.path / \"interpret\" / \"genepositional\"\n)\ngenepositional.score(\n    fragments,\n    clustering,\n    models,\n    force=True,\n)\n</pre> genepositional = chromatinhd.models.diff.interpret.genepositional.GenePositional(     path=models.path / \"interpret\" / \"genepositional\" ) genepositional.score(     fragments,     clustering,     models,     force=True, ) <pre>  0%|          | 0/50 [00:00&lt;?, ?it/s]</pre> In\u00a0[21]: Copied! <pre>symbol = \"EBF1\"\n\nfig = chd.grid.Figure(chd.grid.Grid(padding_height=0.05, padding_width=0.05))\nwidth = 10\n\nregion = fragments.regions.coordinates.loc[transcriptome.gene_id(symbol)]\npanel_genes = chd.plot.genome.genes.Genes.from_region(region, width=width)\nfig.main.add_under(panel_genes)\n\nplotdata, plotdata_mean = genepositional.get_plotdata(transcriptome.gene_id(symbol))\npanel_differential = chd.models.diff.plot.Differential(\n    plotdata, plotdata_mean, cluster_info=clustering.cluster_info, panel_height=0.5, width=width\n)\nfig.main.add_under(panel_differential)\n\npanel_expression = chd.models.diff.plot.DifferentialExpression.from_transcriptome(\n    transcriptome=transcriptome, clustering=clustering, gene=transcriptome.gene_id(symbol), panel_height=0.5\n)\nfig.main.add_right(panel_expression, row=panel_differential)\n\nfig.plot()\n</pre> symbol = \"EBF1\"  fig = chd.grid.Figure(chd.grid.Grid(padding_height=0.05, padding_width=0.05)) width = 10  region = fragments.regions.coordinates.loc[transcriptome.gene_id(symbol)] panel_genes = chd.plot.genome.genes.Genes.from_region(region, width=width) fig.main.add_under(panel_genes)  plotdata, plotdata_mean = genepositional.get_plotdata(transcriptome.gene_id(symbol)) panel_differential = chd.models.diff.plot.Differential(     plotdata, plotdata_mean, cluster_info=clustering.cluster_info, panel_height=0.5, width=width ) fig.main.add_under(panel_differential)  panel_expression = chd.models.diff.plot.DifferentialExpression.from_transcriptome(     transcriptome=transcriptome, clustering=clustering, gene=transcriptome.gene_id(symbol), panel_height=0.5 ) fig.main.add_right(panel_expression, row=panel_differential)  fig.plot() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"quickstart/3_diff/#chromatinhd-diff","title":"ChromatinHD-diff\u00b6","text":""},{"location":"quickstart/3_diff/#train-the-models","title":"Train the models\u00b6","text":""},{"location":"quickstart/3_diff/#interpret-positionally","title":"Interpret positionally\u00b6","text":""},{"location":"reference/data/clustering/","title":"Clustering","text":""},{"location":"reference/data/clustering/#chromatinhd.data.clustering.Clustering","title":"<code>chromatinhd.data.clustering.Clustering</code>","text":"<p>         Bases: <code>Flow</code></p> Source code in <code>src/chromatinhd/data/clustering/clustering.py</code> <pre><code>class Clustering(Flow):\n    labels: pd.DataFrame = Stored()\n    \"Labels for each cell.\"\n\n    cluster_info: pd.DataFrame = StoredDataFrame(index_name=\"cluster\")\n    \"Dataframe containing information for each cluster, such as a label.\"\n\n    @classmethod\n    def from_labels(cls, labels: pd.Series, path=None) -&gt; Clustering:\n\"\"\"\n        Create a Clustering object from a series of labels.\n\n        Parameters:\n            labels:\n                Series of labels for each cell, with index corresponding to cell\n                names.\n            path:\n                Path to save the Clustering object to.\n\n        Returns:\n            Clustering object.\n\n        \"\"\"\n        clustering = cls(path)\n        if not isinstance(labels, pd.Series):\n            labels = pd.Series(labels).astype(\"category\")\n        elif not labels.dtype.name == \"category\":\n            labels = labels.astype(\"category\")\n        clustering.labels = labels\n        clustering.cluster_info = (\n            pd.DataFrame(\n                {\n                    \"cluster\": labels.unique(),\n                    \"n_cells\": labels.value_counts(),\n                    \"label\": labels.unique(),\n                }\n            )\n            .set_index(\"cluster\")\n            .loc[labels.cat.categories]\n        )\n        return clustering\n\n    @property\n    def n_clusters(self):\n        return len(self.labels.unique())\n</code></pre>"},{"location":"reference/data/clustering/#chromatinhd.data.clustering.clustering.Clustering.cluster_info","title":"<code>cluster_info: pd.DataFrame = StoredDataFrame(index_name='cluster')</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Dataframe containing information for each cluster, such as a label.</p>"},{"location":"reference/data/clustering/#chromatinhd.data.clustering.clustering.Clustering.labels","title":"<code>labels: pd.DataFrame = Stored()</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Labels for each cell.</p>"},{"location":"reference/data/clustering/#chromatinhd.data.clustering.clustering.Clustering.from_labels","title":"<code>from_labels(labels, path=None)</code>  <code>classmethod</code>","text":"<p>Create a Clustering object from a series of labels.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>pd.Series</code> <p>Series of labels for each cell, with index corresponding to cell names.</p> required <code>path</code> <p>Path to save the Clustering object to.</p> <code>None</code> <p>Returns:</p> Type Description <code>Clustering</code> <p>Clustering object.</p> Source code in <code>src/chromatinhd/data/clustering/clustering.py</code> <pre><code>@classmethod\ndef from_labels(cls, labels: pd.Series, path=None) -&gt; Clustering:\n\"\"\"\n    Create a Clustering object from a series of labels.\n\n    Parameters:\n        labels:\n            Series of labels for each cell, with index corresponding to cell\n            names.\n        path:\n            Path to save the Clustering object to.\n\n    Returns:\n        Clustering object.\n\n    \"\"\"\n    clustering = cls(path)\n    if not isinstance(labels, pd.Series):\n        labels = pd.Series(labels).astype(\"category\")\n    elif not labels.dtype.name == \"category\":\n        labels = labels.astype(\"category\")\n    clustering.labels = labels\n    clustering.cluster_info = (\n        pd.DataFrame(\n            {\n                \"cluster\": labels.unique(),\n                \"n_cells\": labels.value_counts(),\n                \"label\": labels.unique(),\n            }\n        )\n        .set_index(\"cluster\")\n        .loc[labels.cat.categories]\n    )\n    return clustering\n</code></pre>"},{"location":"reference/data/folds/","title":"Folds","text":""},{"location":"reference/data/folds/#chromatinhd.data.folds.Folds","title":"<code>chromatinhd.data.folds.Folds</code>","text":"<p>         Bases: <code>Flow</code></p> <p>Folds of multiple cell and gene combinations</p> Source code in <code>src/chromatinhd/data/folds/folds.py</code> <pre><code>class Folds(Flow):\n\"\"\"\n    Folds of multiple cell and gene combinations\n    \"\"\"\n\n    folds: dict = Stored()\n\"\"\"The folds\"\"\"\n\n    def sample_cells(\n        self,\n        fragments: Fragments,\n        n_folds: int,\n        n_repeats: int = 1,\n        overwrite: bool = False,\n    ):\n\"\"\"\n        Sample cells and genes into folds\n\n        Parameters:\n            fragments:\n                the fragments\n            n_folds:\n                the number of folds\n            n_repeats:\n                the number of repeats\n            overwrite:\n                whether to overwrite existing folds\n        \"\"\"\n        if not overwrite and self.get(\"folds\").exists(self):\n            return\n\n        folds = []\n\n        for repeat_ix in range(n_repeats):\n            generator = np.random.RandomState(repeat_ix)\n\n            cells_all = generator.permutation(fragments.n_cells)\n\n            cell_bins = np.floor((np.arange(len(cells_all)) / (len(cells_all) / n_folds)))\n\n            for i in range(n_folds):\n                cells_train = cells_all[cell_bins != i]\n                cells_validation_test = cells_all[cell_bins == i]\n                cells_validation = cells_validation_test[: (len(cells_validation_test) // 2)]\n                cells_test = cells_validation_test[(len(cells_validation_test) // 2) :]\n\n                folds.append(\n                    {\n                        \"cells_train\": cells_train,\n                        \"cells_validation\": cells_validation,\n                        \"cells_test\": cells_test,\n                        \"repeat\": repeat_ix,\n                    }\n                )\n        self.folds = folds\n\n    def sample_cellxgene(\n        self,\n        fragments: Fragments,\n        n_folds: int,\n        n_repeats: int = 1,\n        overwrite: bool = False,\n    ):\n\"\"\"\n        Sample cells and genes into folds\n\n        Parameters:\n            fragments:\n                the fragments\n            n_folds:\n                the number of folds\n            n_repeats:\n                the number of repeats\n            overwrite:\n                whether to overwrite existing folds\n        \"\"\"\n        if not overwrite and self.get(\"folds\").exists(self):\n            return\n\n        folds = []\n\n        for repeat_ix in range(n_repeats):\n            generator = np.random.RandomState(repeat_ix)\n\n            cells_all = generator.permutation(fragments.n_cells)\n\n            cell_bins = np.floor((np.arange(len(cells_all)) / (len(cells_all) / n_folds)))\n\n            genes_all = np.arange(fragments.n_genes)\n\n            chr_order = generator.permutation(fragments.regions.coordinates[\"chr\"].unique())\n            gene_chrs = pd.Categorical(fragments.regions.coordinates[\"chr\"].astype(str), categories=chr_order).codes\n            gene_bins = np.floor((gene_chrs / (len(chr_order) / n_folds))).astype(int)\n\n            for i in range(n_folds):\n                cells_train = cells_all[cell_bins != i]\n                cells_validation_test = cells_all[cell_bins == i]\n                cells_validation = cells_validation_test[: (len(cells_validation_test) // 2)]\n                cells_test = cells_validation_test[(len(cells_validation_test) // 2) :]\n\n                genes_train = genes_all[gene_bins != i]\n                genes_validation_test = genes_all[gene_bins == i]\n                genes_validation = genes_validation_test[: (len(genes_validation_test) // 2)]\n                genes_test = genes_validation_test[(len(genes_validation_test) // 2) :]\n\n                folds.append(\n                    {\n                        \"cells_train\": cells_train,\n                        \"cells_validation\": cells_validation,\n                        \"cells_test\": cells_test,\n                        \"genes_train\": genes_train,\n                        \"genes_validation\": genes_validation,\n                        \"genes_test\": genes_test,\n                        \"repeat\": repeat_ix,\n                    }\n                )\n        self.folds = folds\n\n    def __getitem__(self, ix):\n        return self.folds[ix]\n\n    def __len__(self):\n        return len(self.folds)\n</code></pre>"},{"location":"reference/data/folds/#chromatinhd.data.folds.folds.Folds.folds","title":"<code>folds: dict = Stored()</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>The folds</p>"},{"location":"reference/data/folds/#chromatinhd.data.folds.folds.Folds.sample_cells","title":"<code>sample_cells(fragments, n_folds, n_repeats=1, overwrite=False)</code>","text":"<p>Sample cells and genes into folds</p> <p>Parameters:</p> Name Type Description Default <code>fragments</code> <code>Fragments</code> <p>the fragments</p> required <code>n_folds</code> <code>int</code> <p>the number of folds</p> required <code>n_repeats</code> <code>int</code> <p>the number of repeats</p> <code>1</code> <code>overwrite</code> <code>bool</code> <p>whether to overwrite existing folds</p> <code>False</code> Source code in <code>src/chromatinhd/data/folds/folds.py</code> <pre><code>def sample_cells(\n    self,\n    fragments: Fragments,\n    n_folds: int,\n    n_repeats: int = 1,\n    overwrite: bool = False,\n):\n\"\"\"\n    Sample cells and genes into folds\n\n    Parameters:\n        fragments:\n            the fragments\n        n_folds:\n            the number of folds\n        n_repeats:\n            the number of repeats\n        overwrite:\n            whether to overwrite existing folds\n    \"\"\"\n    if not overwrite and self.get(\"folds\").exists(self):\n        return\n\n    folds = []\n\n    for repeat_ix in range(n_repeats):\n        generator = np.random.RandomState(repeat_ix)\n\n        cells_all = generator.permutation(fragments.n_cells)\n\n        cell_bins = np.floor((np.arange(len(cells_all)) / (len(cells_all) / n_folds)))\n\n        for i in range(n_folds):\n            cells_train = cells_all[cell_bins != i]\n            cells_validation_test = cells_all[cell_bins == i]\n            cells_validation = cells_validation_test[: (len(cells_validation_test) // 2)]\n            cells_test = cells_validation_test[(len(cells_validation_test) // 2) :]\n\n            folds.append(\n                {\n                    \"cells_train\": cells_train,\n                    \"cells_validation\": cells_validation,\n                    \"cells_test\": cells_test,\n                    \"repeat\": repeat_ix,\n                }\n            )\n    self.folds = folds\n</code></pre>"},{"location":"reference/data/folds/#chromatinhd.data.folds.folds.Folds.sample_cellxgene","title":"<code>sample_cellxgene(fragments, n_folds, n_repeats=1, overwrite=False)</code>","text":"<p>Sample cells and genes into folds</p> <p>Parameters:</p> Name Type Description Default <code>fragments</code> <code>Fragments</code> <p>the fragments</p> required <code>n_folds</code> <code>int</code> <p>the number of folds</p> required <code>n_repeats</code> <code>int</code> <p>the number of repeats</p> <code>1</code> <code>overwrite</code> <code>bool</code> <p>whether to overwrite existing folds</p> <code>False</code> Source code in <code>src/chromatinhd/data/folds/folds.py</code> <pre><code>def sample_cellxgene(\n    self,\n    fragments: Fragments,\n    n_folds: int,\n    n_repeats: int = 1,\n    overwrite: bool = False,\n):\n\"\"\"\n    Sample cells and genes into folds\n\n    Parameters:\n        fragments:\n            the fragments\n        n_folds:\n            the number of folds\n        n_repeats:\n            the number of repeats\n        overwrite:\n            whether to overwrite existing folds\n    \"\"\"\n    if not overwrite and self.get(\"folds\").exists(self):\n        return\n\n    folds = []\n\n    for repeat_ix in range(n_repeats):\n        generator = np.random.RandomState(repeat_ix)\n\n        cells_all = generator.permutation(fragments.n_cells)\n\n        cell_bins = np.floor((np.arange(len(cells_all)) / (len(cells_all) / n_folds)))\n\n        genes_all = np.arange(fragments.n_genes)\n\n        chr_order = generator.permutation(fragments.regions.coordinates[\"chr\"].unique())\n        gene_chrs = pd.Categorical(fragments.regions.coordinates[\"chr\"].astype(str), categories=chr_order).codes\n        gene_bins = np.floor((gene_chrs / (len(chr_order) / n_folds))).astype(int)\n\n        for i in range(n_folds):\n            cells_train = cells_all[cell_bins != i]\n            cells_validation_test = cells_all[cell_bins == i]\n            cells_validation = cells_validation_test[: (len(cells_validation_test) // 2)]\n            cells_test = cells_validation_test[(len(cells_validation_test) // 2) :]\n\n            genes_train = genes_all[gene_bins != i]\n            genes_validation_test = genes_all[gene_bins == i]\n            genes_validation = genes_validation_test[: (len(genes_validation_test) // 2)]\n            genes_test = genes_validation_test[(len(genes_validation_test) // 2) :]\n\n            folds.append(\n                {\n                    \"cells_train\": cells_train,\n                    \"cells_validation\": cells_validation,\n                    \"cells_test\": cells_test,\n                    \"genes_train\": genes_train,\n                    \"genes_validation\": genes_validation,\n                    \"genes_test\": genes_test,\n                    \"repeat\": repeat_ix,\n                }\n            )\n    self.folds = folds\n</code></pre>"},{"location":"reference/data/fragments/","title":"Fragments","text":""},{"location":"reference/data/fragments/#chromatinhd.data.fragments.Fragments","title":"<code>chromatinhd.data.fragments.Fragments</code>","text":"<p>         Bases: <code>Flow</code></p> <p>Fragments centered around a gene window</p> Source code in <code>src/chromatinhd/data/fragments/fragments.py</code> <pre><code>class Fragments(Flow):\n\"\"\"Fragments centered around a gene window\"\"\"\n\n    regions: Regions = Linked()\n\"\"\"The regions in which (part of) the fragments are located and centered\"\"\"\n\n    coordinates: torch.Tensor = StoredTensor(torch.int64)\n\"\"\"Coordinates of the two cut sites.\"\"\"\n\n    mapping: torch.Tensor = StoredTensor(torch.int64)\n\"\"\"Mapping of a fragment to a gene and a cell\"\"\"\n\n    cellxgene_indptr: torch.Tensor = StoredTensor(torch.int64)\n\"\"\"Index pointers for each cellxgene combination\"\"\"\n\n    def create_cellxgene_indptr(self):\n        cellxgene = self.mapping[:, 0] * self.n_genes + self.mapping[:, 1]\n\n        if not (cellxgene.diff() &gt;= 0).all():\n            raise ValueError(\"Fragments should be ordered by cell then gene (ascending)\")\n\n        n_cellxgene = self.n_genes * self.n_cells\n        cellxgene_indptr = torch.nn.functional.pad(\n            torch.cumsum(torch.bincount(cellxgene, minlength=n_cellxgene), 0), (1, 0)\n        )\n        assert self.coordinates.shape[0] == cellxgene_indptr[-1]\n        if not (cellxgene_indptr.diff() &gt;= 0).all():\n            raise ValueError(\"Fragments should be ordered by cell then gene (ascending)\")\n        self.cellxgene_indptr = cellxgene_indptr\n\n    _genemapping = None\n\n    @property\n    def genemapping(self):\n        if self._genemapping is None:\n            self._genemapping = self.mapping[:, 1].contiguous()\n        return self._genemapping\n\n    _cellmapping = None\n\n    @property\n    def cellmapping(self):\n        if self._cellmapping is None:\n            self._cellmapping = self.mapping[:, 0].contiguous()\n        return self._cellmapping\n\n    var = TSV()\n\"\"\"DataFrame containing information about regions.\"\"\"\n\n    obs = TSV()\n\"\"\"DataFrame containing information about cells.\"\"\"\n\n    _n_genes = None\n\n    @property\n    def n_genes(self):\n        if self._n_genes is None:\n            self._n_genes = self.var.shape[0]\n        return self._n_genes\n\n    _n_cells = None\n\n    @property\n    def n_cells(self):\n        if self._n_cells is None:\n            self._n_cells = self.obs.shape[0]\n        return self._n_cells\n\n    @property\n    def local_cellxgene_ix(self):\n        return self.cellmapping * self.n_genes + self.genemapping\n\n    def estimate_fragment_per_cellxgene(self):\n        return math.ceil(self.coordinates.shape[0] / self.n_cells / self.n_genes * 2)\n\n    # def create_cut_data(self):\n    #     cut_coordinates = self.coordinates.flatten()\n    #     cut_coordinates = (cut_coordinates - self.window[0]) / (\n    #         self.window[1] - self.window[0]\n    #     )\n    #     keep_cuts = (cut_coordinates &gt;= 0) &amp; (cut_coordinates &lt;= 1)\n    #     cut_coordinates = cut_coordinates[keep_cuts]\n\n    #     self.cut_coordinates = cut_coordinates\n\n    #     self.cut_local_gene_ix = self.genemapping.expand(2, -1).T.flatten()[keep_cuts]\n    #     self.cut_local_cell_ix = self.cellmapping.expand(2, -1).T.flatten()[keep_cuts]\n\n    @property\n    def genes_oi_torch(self):\n        return torch.from_numpy(self.genes_oi).to(self.coordinates.device)\n\n    @property\n    def cells_oi_torch(self):\n        return torch.from_numpy(self.genes_oi).to(self.coordinates.device)\n\n    @class_or_instancemethod\n    def from_fragments_tsv(\n        cls,\n        fragments_file: PathLike,\n        regions: Regions,\n        obs: pd.DataFrame,\n        cell_column: str = None,\n        path: PathLike = None,\n        overwrite: bool = True,\n    ) -&gt; Fragments:\n\"\"\"\n        Create a Fragments object from a fragments tsv file\n\n        Parameters:\n            fragments_file:\n                Location of the fragments tab-separate file created by e.g. CellRanger or sinto\n            obs:\n                DataFrame containing information about cells.\n                The index should be the cell names as present in the fragments file.\n                Alternatively, the column containing cell ids can be specified using the `cell_column` argument.\n            regions:\n                Regions from which the fragments will be extracted.\n            cell_column:\n                Column name in the `obs` DataFrame containing the cell names.\n                If not specified, the index of the `obs` DataFrame is used.\n            path:\n                Folder in which the fragments data will be stored.\n            overwrite:\n                Whether to overwrite the data if it already exists.\n        Returns:\n            A new Fragments object\n        \"\"\"\n\n        if isinstance(fragments_file, str):\n            fragments_file = pathlib.Path(fragments_file)\n        if isinstance(path, str):\n            path = pathlib.Path(path)\n        if not fragments_file.exists():\n            raise FileNotFoundError(f\"File {fragments_file} does not exist\")\n        if not overwrite and path.exists():\n            raise FileExistsError(f\"Folder {path} already exists\")\n        path.mkdir(parents=True, exist_ok=True)\n\n        # regions information\n        var = pd.DataFrame(index=regions.coordinates.index)\n        var[\"ix\"] = np.arange(var.shape[0])\n\n        # cell information\n        obs = obs.copy()\n        obs[\"ix\"] = np.arange(obs.shape[0])\n        if cell_column is None:\n            cell_to_cell_ix = obs[\"ix\"].to_dict()\n        else:\n            cell_to_cell_ix = obs.set_index(cell_column)[\"ix\"].to_dict()\n\n        # load fragments tabix\n        import pysam\n\n        fragments_tabix = pysam.TabixFile(str(fragments_file))\n\n        coordinates_raw = []\n        mapping_raw = []\n\n        for _, (gene, promoter_info) in tqdm.tqdm(\n            enumerate(regions.coordinates.iterrows()),\n            total=regions.coordinates.shape[0],\n            leave=False,\n            desc=\"Processing fragments\",\n        ):\n            gene_ix = var.loc[gene, \"ix\"]\n            start = max(0, promoter_info[\"start\"])\n\n            fragments_promoter = fragments_tabix.fetch(\n                promoter_info[\"chrom\"],\n                start,\n                promoter_info[\"end\"],\n                parser=pysam.asTuple(),\n            )\n\n            tss = promoter_info[\"tss\"]\n            strand = promoter_info[\"strand\"]\n\n            for fragment in fragments_promoter:\n                cell = fragment[3]\n\n                # only store the fragment if the cell is actually of interest\n                if cell in cell_to_cell_ix:\n                    # add raw data of fragment relative to tss\n                    coordinates_raw.append(\n                        [\n                            (int(fragment[1]) - tss) * strand,\n                            (int(fragment[2]) - tss) * strand,\n                        ][::strand]\n                    )\n\n                    # add mapping of cell/gene\n                    mapping_raw.append([cell_to_cell_ix[fragment[3]], gene_ix])\n\n        coordinates = torch.tensor(np.array(coordinates_raw, dtype=np.int64))\n        mapping = torch.tensor(np.array(mapping_raw), dtype=torch.int64)\n\n        # Sort `coordinates` and `mapping` according to `mapping`\n        sorted_idx = torch.argsort((mapping[:, 0] * var.shape[0] + mapping[:, 1]))\n        mapping = mapping[sorted_idx]\n        coordinates = coordinates[sorted_idx]\n\n        return cls.create(\n            path=path,\n            coordinates=coordinates,\n            mapping=mapping,\n            regions=regions,\n            var=var,\n            obs=obs,\n        )\n\n    def filter_genes(self, regions: Regions, path: PathLike = None) -&gt; Fragments:\n\"\"\"\n        Filter based on new regions\n\n        Parameters:\n            regions:\n                Regions to filter.\n        Returns:\n            A new Fragments object\n        \"\"\"\n\n        # test if new regions are a subset of the existing ones\n        if not regions.coordinates.index.isin(self.regions.coordinates.index).all():\n            raise ValueError(\"New regions should be a subset of the existing ones\")\n\n        # filter genes\n        self.regions.coordinates[\"ix\"] = np.arange(self.regions.coordinates.shape[0])\n        regions.coordinates[\"ix\"] = self.regions.coordinates[\"ix\"].loc[regions.coordinates.index]\n        fragments_oi = np.isin(self.mapping[:, 1].numpy(), regions.coordinates[\"ix\"])\n\n        mapping = self.mapping[fragments_oi]\n        coordinates = self.coordinates[fragments_oi]\n        var = self.regions.coordinates.copy()\n        var[\"original_ix\"] = np.arange(var.shape[0])\n        var = var.loc[regions.coordinates.index].copy()\n        var[\"ix\"] = np.arange(var.shape[0])\n        mapping[:, 1] = torch.from_numpy(var.set_index(\"original_ix\").loc[mapping[:, 1].cpu().numpy(), \"ix\"].values)\n\n        # Sort `coordinates` and `mapping` according to `mapping`\n        sorted_idx = torch.argsort((mapping[:, 0] * var.shape[0] + mapping[:, 1]))\n        mapping = mapping[sorted_idx]\n        coordinates = coordinates[sorted_idx]\n\n        return Fragments.create(\n            coordinates=coordinates, mapping=mapping, regions=regions, var=var, obs=self.obs, path=path\n        )\n</code></pre>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.fragments.Fragments.cellxgene_indptr","title":"<code>cellxgene_indptr: torch.Tensor = StoredTensor(torch.int64)</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Index pointers for each cellxgene combination</p>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.fragments.Fragments.coordinates","title":"<code>coordinates: torch.Tensor = StoredTensor(torch.int64)</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Coordinates of the two cut sites.</p>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.fragments.Fragments.mapping","title":"<code>mapping: torch.Tensor = StoredTensor(torch.int64)</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Mapping of a fragment to a gene and a cell</p>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.fragments.Fragments.obs","title":"<code>obs = TSV()</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>DataFrame containing information about cells.</p>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.fragments.Fragments.regions","title":"<code>regions: Regions = Linked()</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>The regions in which (part of) the fragments are located and centered</p>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.fragments.Fragments.var","title":"<code>var = TSV()</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>DataFrame containing information about regions.</p>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.fragments.Fragments.filter_genes","title":"<code>filter_genes(regions, path=None)</code>","text":"<p>Filter based on new regions</p> <p>Parameters:</p> Name Type Description Default <code>regions</code> <code>Regions</code> <p>Regions to filter.</p> required <p>Returns:</p> Type Description <code>Fragments</code> <p>A new Fragments object</p> Source code in <code>src/chromatinhd/data/fragments/fragments.py</code> <pre><code>def filter_genes(self, regions: Regions, path: PathLike = None) -&gt; Fragments:\n\"\"\"\n    Filter based on new regions\n\n    Parameters:\n        regions:\n            Regions to filter.\n    Returns:\n        A new Fragments object\n    \"\"\"\n\n    # test if new regions are a subset of the existing ones\n    if not regions.coordinates.index.isin(self.regions.coordinates.index).all():\n        raise ValueError(\"New regions should be a subset of the existing ones\")\n\n    # filter genes\n    self.regions.coordinates[\"ix\"] = np.arange(self.regions.coordinates.shape[0])\n    regions.coordinates[\"ix\"] = self.regions.coordinates[\"ix\"].loc[regions.coordinates.index]\n    fragments_oi = np.isin(self.mapping[:, 1].numpy(), regions.coordinates[\"ix\"])\n\n    mapping = self.mapping[fragments_oi]\n    coordinates = self.coordinates[fragments_oi]\n    var = self.regions.coordinates.copy()\n    var[\"original_ix\"] = np.arange(var.shape[0])\n    var = var.loc[regions.coordinates.index].copy()\n    var[\"ix\"] = np.arange(var.shape[0])\n    mapping[:, 1] = torch.from_numpy(var.set_index(\"original_ix\").loc[mapping[:, 1].cpu().numpy(), \"ix\"].values)\n\n    # Sort `coordinates` and `mapping` according to `mapping`\n    sorted_idx = torch.argsort((mapping[:, 0] * var.shape[0] + mapping[:, 1]))\n    mapping = mapping[sorted_idx]\n    coordinates = coordinates[sorted_idx]\n\n    return Fragments.create(\n        coordinates=coordinates, mapping=mapping, regions=regions, var=var, obs=self.obs, path=path\n    )\n</code></pre>"},{"location":"reference/data/fragments/#chromatinhd.data.fragments.fragments.Fragments.from_fragments_tsv","title":"<code>from_fragments_tsv(fragments_file, regions, obs, cell_column=None, path=None, overwrite=True)</code>","text":"<p>Create a Fragments object from a fragments tsv file</p> <p>Parameters:</p> Name Type Description Default <code>fragments_file</code> <code>PathLike</code> <p>Location of the fragments tab-separate file created by e.g. CellRanger or sinto</p> required <code>obs</code> <code>pd.DataFrame</code> <p>DataFrame containing information about cells. The index should be the cell names as present in the fragments file. Alternatively, the column containing cell ids can be specified using the <code>cell_column</code> argument.</p> required <code>regions</code> <code>Regions</code> <p>Regions from which the fragments will be extracted.</p> required <code>cell_column</code> <code>str</code> <p>Column name in the <code>obs</code> DataFrame containing the cell names. If not specified, the index of the <code>obs</code> DataFrame is used.</p> <code>None</code> <code>path</code> <code>PathLike</code> <p>Folder in which the fragments data will be stored.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the data if it already exists.</p> <code>True</code> <p>Returns:</p> Type Description <code>Fragments</code> <p>A new Fragments object</p> Source code in <code>src/chromatinhd/data/fragments/fragments.py</code> <pre><code>@class_or_instancemethod\ndef from_fragments_tsv(\n    cls,\n    fragments_file: PathLike,\n    regions: Regions,\n    obs: pd.DataFrame,\n    cell_column: str = None,\n    path: PathLike = None,\n    overwrite: bool = True,\n) -&gt; Fragments:\n\"\"\"\n    Create a Fragments object from a fragments tsv file\n\n    Parameters:\n        fragments_file:\n            Location of the fragments tab-separate file created by e.g. CellRanger or sinto\n        obs:\n            DataFrame containing information about cells.\n            The index should be the cell names as present in the fragments file.\n            Alternatively, the column containing cell ids can be specified using the `cell_column` argument.\n        regions:\n            Regions from which the fragments will be extracted.\n        cell_column:\n            Column name in the `obs` DataFrame containing the cell names.\n            If not specified, the index of the `obs` DataFrame is used.\n        path:\n            Folder in which the fragments data will be stored.\n        overwrite:\n            Whether to overwrite the data if it already exists.\n    Returns:\n        A new Fragments object\n    \"\"\"\n\n    if isinstance(fragments_file, str):\n        fragments_file = pathlib.Path(fragments_file)\n    if isinstance(path, str):\n        path = pathlib.Path(path)\n    if not fragments_file.exists():\n        raise FileNotFoundError(f\"File {fragments_file} does not exist\")\n    if not overwrite and path.exists():\n        raise FileExistsError(f\"Folder {path} already exists\")\n    path.mkdir(parents=True, exist_ok=True)\n\n    # regions information\n    var = pd.DataFrame(index=regions.coordinates.index)\n    var[\"ix\"] = np.arange(var.shape[0])\n\n    # cell information\n    obs = obs.copy()\n    obs[\"ix\"] = np.arange(obs.shape[0])\n    if cell_column is None:\n        cell_to_cell_ix = obs[\"ix\"].to_dict()\n    else:\n        cell_to_cell_ix = obs.set_index(cell_column)[\"ix\"].to_dict()\n\n    # load fragments tabix\n    import pysam\n\n    fragments_tabix = pysam.TabixFile(str(fragments_file))\n\n    coordinates_raw = []\n    mapping_raw = []\n\n    for _, (gene, promoter_info) in tqdm.tqdm(\n        enumerate(regions.coordinates.iterrows()),\n        total=regions.coordinates.shape[0],\n        leave=False,\n        desc=\"Processing fragments\",\n    ):\n        gene_ix = var.loc[gene, \"ix\"]\n        start = max(0, promoter_info[\"start\"])\n\n        fragments_promoter = fragments_tabix.fetch(\n            promoter_info[\"chrom\"],\n            start,\n            promoter_info[\"end\"],\n            parser=pysam.asTuple(),\n        )\n\n        tss = promoter_info[\"tss\"]\n        strand = promoter_info[\"strand\"]\n\n        for fragment in fragments_promoter:\n            cell = fragment[3]\n\n            # only store the fragment if the cell is actually of interest\n            if cell in cell_to_cell_ix:\n                # add raw data of fragment relative to tss\n                coordinates_raw.append(\n                    [\n                        (int(fragment[1]) - tss) * strand,\n                        (int(fragment[2]) - tss) * strand,\n                    ][::strand]\n                )\n\n                # add mapping of cell/gene\n                mapping_raw.append([cell_to_cell_ix[fragment[3]], gene_ix])\n\n    coordinates = torch.tensor(np.array(coordinates_raw, dtype=np.int64))\n    mapping = torch.tensor(np.array(mapping_raw), dtype=torch.int64)\n\n    # Sort `coordinates` and `mapping` according to `mapping`\n    sorted_idx = torch.argsort((mapping[:, 0] * var.shape[0] + mapping[:, 1]))\n    mapping = mapping[sorted_idx]\n    coordinates = coordinates[sorted_idx]\n\n    return cls.create(\n        path=path,\n        coordinates=coordinates,\n        mapping=mapping,\n        regions=regions,\n        var=var,\n        obs=obs,\n    )\n</code></pre>"},{"location":"reference/data/motifscan/","title":"Motifscan","text":""},{"location":"reference/data/motifscan/#chromatinhd.data.motifscan.Motifscan","title":"<code>chromatinhd.data.motifscan.Motifscan</code>","text":"<p>         Bases: <code>Flow</code></p> <p>A sprase representation of locations of different motifs in regions of the genome</p> Source code in <code>src/chromatinhd/data/motifscan/motifscan.py</code> <pre><code>class Motifscan(Flow):\n\"\"\"\n    A sprase representation of locations of different motifs in regions of the genome\n    \"\"\"\n\n    regions = Linked()\n    \"The regions\"\n\n    indptr = CompressedNumpyInt64()\n    \"The index pointers for each position in the regions\"\n\n    position = CompressedNumpyInt64()\n    \"Position associated to each site\"\n\n    indices = CompressedNumpyInt64()\n    \"Motif index associated to each site\"\n\n    scores = CompressedNumpyFloat64()\n    \"Scores associated with each detected site\"\n\n    strands = CompressedNumpyFloat64()\n    \"Strand associated with each detected site\"\n\n    shape = Stored()\n\n    n_motifs = Stored()\n    \"Number of motifs\"\n\n    motifs = StoredDataFrame()\n    \"Dataframe storing auxilliary information for each motif\"\n\n    @classmethod\n    def from_pwms(\n        cls,\n        pwms: dict,\n        regions: Regions,\n        fasta_file: Union[str, pathlib.Path],\n        path: Union[str, pathlib.Path],\n        cutoffs: Union[int, float, pd.Series] = None,\n        cutoff_col: str = None,\n        motifs: pd.DataFrame = None,\n        device=None,\n        batch_size: int = 5000000,\n    ):\n\"\"\"\n        Create a motifscan object from a set of pwms and a set of regions\n\n        Parameters:\n            pwms:\n                A dictionary of pwms, where the keys are the motif ids and the values are the pwms\n            regions:\n                A regions object\n            fasta_file:\n                The location of the fasta file containing the genome\n            motifs:\n                A dataframe containing auxilliary information for each motif\n            path:\n                The folder where the motifscan data will be stored.\n            cutoffs:\n                A dictionary containing the cutoffs for each motif.\n            cutoff_col:\n                The column in the motifs dataframe containing the cutoffs\n            device:\n                The device to use for the scanning\n            batch_size:\n                The batch size to use for scanning. Lower batch size if the GPU runs out of memory\n        \"\"\"\n\n        if device is None:\n            device = get_default_device()\n\n        self = cls(path)\n\n        # check or create cutoffs\n        if cutoffs is None:\n            if cutoff_col is None:\n                raise ValueError(\"Either motifs+cutoff_col or cutoffs need to be specified.\")\n            if motifs is None:\n                raise ValueError(\"Either motifs+cutoff_col or cutoffs need to be specified. motifs is not given\")\n\n            cutoffs = motifs[cutoff_col].to_dict()\n        else:\n            if isinstance(cutoffs, (float, int)):\n                cutoffs = {motif: cutoffs for motif in pwms.keys()}\n            elif isinstance(cutoffs, pd.Series):\n                cutoffs = cutoffs.to_dict()\n            else:\n                raise ValueError(\"cutoffs should be a float, int, dict or pd.Series\")\n            assert set(cutoffs.keys()) == set(pwms.keys())\n\n        # check or create motifs\n        if motifs is None:\n            motifs = pd.DataFrame(\n                {\n                    \"motif\": list(pwms.keys()),\n                }\n            ).set_index(\"motif\")\n\n        # divide regions into batches according to batch size\n        region_coordinates = regions.coordinates\n\n        region_coordinates = divide_regions_in_batches(region_coordinates, batch_size=batch_size)\n\n        region_size = region_coordinates[\"end\"].values[0] - region_coordinates[\"start\"].values[0]\n\n        # load in fasta file\n        import pysam\n\n        fasta = pysam.FastaFile(fasta_file)\n\n        # do the actual counting by looping over the batches, extract the sequences and scan\n        positions = []\n        indices = []\n        scores = []\n        strands = []\n\n        for batch, region_coordinates_batch in tqdm.tqdm(region_coordinates.groupby(\"batch\")):\n            sequences = [\n                fasta.fetch(chrom, start, end + 1)\n                for chrom, start, end in region_coordinates_batch[[\"chrom\", \"start\", \"end\"]].values\n            ]\n            assert (\n                len(set(len(sequence) for sequence in sequences)) == 1\n            ), \"All regions/sequences should have the same length\"\n            onehot = torch.stack([create_onehot(digitize_sequence(sequence)) for sequence in sequences]).to(device)\n            for motif_ix, motif in enumerate(motifs.index):\n                cutoff = cutoffs[motif]\n\n                # get pwm\n                pwm = pwms[motif]\n                if not torch.is_tensor(pwm):\n                    pwm = torch.from_numpy(pwm)\n                pwm = pwm.to(dtype=torch.float32, device=onehot.device)\n\n                (\n                    scores_motif,\n                    positions_motif,\n                    strands_motif,\n                ) = scan(onehot, pwm, cutoff=cutoff)\n\n                positions_motif[0] = positions_motif[0] + region_coordinates_batch[\"ix\"].values[0]\n\n                positions.append(positions_motif)\n                indices.append(torch.ones_like(scores_motif, dtype=torch.int) * motif_ix)\n                scores.append(scores_motif)\n                strands.append(strands_motif)\n\n        # positions = [batch_dim (region and position), site]\n        positions = torch.concat(positions, -1)\n        positions = positions[0] * region_size + positions[1]\n        indices = torch.concat(indices, -1)\n        scores = torch.concat(scores, -1)\n        strands = torch.concat(strands, -1)\n\n        # sort by position\n        sorted_idx = torch.argsort(positions)\n        positions = positions[sorted_idx]\n        indices = indices[sorted_idx]\n        scores = scores[sorted_idx]\n        strands = strands[sorted_idx]\n\n        # calculate indptr\n        indptr = ind2ptr(positions, region_size * len(region_coordinates))\n\n        # store\n        self.positions = positions.cpu().numpy()\n        self.indptr = indptr.cpu().numpy()\n        self.indices = indices.cpu().numpy()\n        self.scores = scores.cpu().numpy()\n        self.strands = strands.cpu().numpy()\n        self.motifs = motifs\n\n        return self\n</code></pre>"},{"location":"reference/data/motifscan/#chromatinhd.data.motifscan.motifscan.Motifscan.indices","title":"<code>indices = CompressedNumpyInt64()</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Motif index associated to each site</p>"},{"location":"reference/data/motifscan/#chromatinhd.data.motifscan.motifscan.Motifscan.indptr","title":"<code>indptr = CompressedNumpyInt64()</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>The index pointers for each position in the regions</p>"},{"location":"reference/data/motifscan/#chromatinhd.data.motifscan.motifscan.Motifscan.motifs","title":"<code>motifs = StoredDataFrame()</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Dataframe storing auxilliary information for each motif</p>"},{"location":"reference/data/motifscan/#chromatinhd.data.motifscan.motifscan.Motifscan.n_motifs","title":"<code>n_motifs = Stored()</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Number of motifs</p>"},{"location":"reference/data/motifscan/#chromatinhd.data.motifscan.motifscan.Motifscan.position","title":"<code>position = CompressedNumpyInt64()</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Position associated to each site</p>"},{"location":"reference/data/motifscan/#chromatinhd.data.motifscan.motifscan.Motifscan.regions","title":"<code>regions = Linked()</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>The regions</p>"},{"location":"reference/data/motifscan/#chromatinhd.data.motifscan.motifscan.Motifscan.scores","title":"<code>scores = CompressedNumpyFloat64()</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Scores associated with each detected site</p>"},{"location":"reference/data/motifscan/#chromatinhd.data.motifscan.motifscan.Motifscan.strands","title":"<code>strands = CompressedNumpyFloat64()</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Strand associated with each detected site</p>"},{"location":"reference/data/motifscan/#chromatinhd.data.motifscan.motifscan.Motifscan.from_pwms","title":"<code>from_pwms(pwms, regions, fasta_file, path, cutoffs=None, cutoff_col=None, motifs=None, device=None, batch_size=5000000)</code>  <code>classmethod</code>","text":"<p>Create a motifscan object from a set of pwms and a set of regions</p> <p>Parameters:</p> Name Type Description Default <code>pwms</code> <code>dict</code> <p>A dictionary of pwms, where the keys are the motif ids and the values are the pwms</p> required <code>regions</code> <code>Regions</code> <p>A regions object</p> required <code>fasta_file</code> <code>Union[str, pathlib.Path]</code> <p>The location of the fasta file containing the genome</p> required <code>motifs</code> <code>pd.DataFrame</code> <p>A dataframe containing auxilliary information for each motif</p> <code>None</code> <code>path</code> <code>Union[str, pathlib.Path]</code> <p>The folder where the motifscan data will be stored.</p> required <code>cutoffs</code> <code>Union[int, float, pd.Series]</code> <p>A dictionary containing the cutoffs for each motif.</p> <code>None</code> <code>cutoff_col</code> <code>str</code> <p>The column in the motifs dataframe containing the cutoffs</p> <code>None</code> <code>device</code> <p>The device to use for the scanning</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>The batch size to use for scanning. Lower batch size if the GPU runs out of memory</p> <code>5000000</code> Source code in <code>src/chromatinhd/data/motifscan/motifscan.py</code> <pre><code>@classmethod\ndef from_pwms(\n    cls,\n    pwms: dict,\n    regions: Regions,\n    fasta_file: Union[str, pathlib.Path],\n    path: Union[str, pathlib.Path],\n    cutoffs: Union[int, float, pd.Series] = None,\n    cutoff_col: str = None,\n    motifs: pd.DataFrame = None,\n    device=None,\n    batch_size: int = 5000000,\n):\n\"\"\"\n    Create a motifscan object from a set of pwms and a set of regions\n\n    Parameters:\n        pwms:\n            A dictionary of pwms, where the keys are the motif ids and the values are the pwms\n        regions:\n            A regions object\n        fasta_file:\n            The location of the fasta file containing the genome\n        motifs:\n            A dataframe containing auxilliary information for each motif\n        path:\n            The folder where the motifscan data will be stored.\n        cutoffs:\n            A dictionary containing the cutoffs for each motif.\n        cutoff_col:\n            The column in the motifs dataframe containing the cutoffs\n        device:\n            The device to use for the scanning\n        batch_size:\n            The batch size to use for scanning. Lower batch size if the GPU runs out of memory\n    \"\"\"\n\n    if device is None:\n        device = get_default_device()\n\n    self = cls(path)\n\n    # check or create cutoffs\n    if cutoffs is None:\n        if cutoff_col is None:\n            raise ValueError(\"Either motifs+cutoff_col or cutoffs need to be specified.\")\n        if motifs is None:\n            raise ValueError(\"Either motifs+cutoff_col or cutoffs need to be specified. motifs is not given\")\n\n        cutoffs = motifs[cutoff_col].to_dict()\n    else:\n        if isinstance(cutoffs, (float, int)):\n            cutoffs = {motif: cutoffs for motif in pwms.keys()}\n        elif isinstance(cutoffs, pd.Series):\n            cutoffs = cutoffs.to_dict()\n        else:\n            raise ValueError(\"cutoffs should be a float, int, dict or pd.Series\")\n        assert set(cutoffs.keys()) == set(pwms.keys())\n\n    # check or create motifs\n    if motifs is None:\n        motifs = pd.DataFrame(\n            {\n                \"motif\": list(pwms.keys()),\n            }\n        ).set_index(\"motif\")\n\n    # divide regions into batches according to batch size\n    region_coordinates = regions.coordinates\n\n    region_coordinates = divide_regions_in_batches(region_coordinates, batch_size=batch_size)\n\n    region_size = region_coordinates[\"end\"].values[0] - region_coordinates[\"start\"].values[0]\n\n    # load in fasta file\n    import pysam\n\n    fasta = pysam.FastaFile(fasta_file)\n\n    # do the actual counting by looping over the batches, extract the sequences and scan\n    positions = []\n    indices = []\n    scores = []\n    strands = []\n\n    for batch, region_coordinates_batch in tqdm.tqdm(region_coordinates.groupby(\"batch\")):\n        sequences = [\n            fasta.fetch(chrom, start, end + 1)\n            for chrom, start, end in region_coordinates_batch[[\"chrom\", \"start\", \"end\"]].values\n        ]\n        assert (\n            len(set(len(sequence) for sequence in sequences)) == 1\n        ), \"All regions/sequences should have the same length\"\n        onehot = torch.stack([create_onehot(digitize_sequence(sequence)) for sequence in sequences]).to(device)\n        for motif_ix, motif in enumerate(motifs.index):\n            cutoff = cutoffs[motif]\n\n            # get pwm\n            pwm = pwms[motif]\n            if not torch.is_tensor(pwm):\n                pwm = torch.from_numpy(pwm)\n            pwm = pwm.to(dtype=torch.float32, device=onehot.device)\n\n            (\n                scores_motif,\n                positions_motif,\n                strands_motif,\n            ) = scan(onehot, pwm, cutoff=cutoff)\n\n            positions_motif[0] = positions_motif[0] + region_coordinates_batch[\"ix\"].values[0]\n\n            positions.append(positions_motif)\n            indices.append(torch.ones_like(scores_motif, dtype=torch.int) * motif_ix)\n            scores.append(scores_motif)\n            strands.append(strands_motif)\n\n    # positions = [batch_dim (region and position), site]\n    positions = torch.concat(positions, -1)\n    positions = positions[0] * region_size + positions[1]\n    indices = torch.concat(indices, -1)\n    scores = torch.concat(scores, -1)\n    strands = torch.concat(strands, -1)\n\n    # sort by position\n    sorted_idx = torch.argsort(positions)\n    positions = positions[sorted_idx]\n    indices = indices[sorted_idx]\n    scores = scores[sorted_idx]\n    strands = strands[sorted_idx]\n\n    # calculate indptr\n    indptr = ind2ptr(positions, region_size * len(region_coordinates))\n\n    # store\n    self.positions = positions.cpu().numpy()\n    self.indptr = indptr.cpu().numpy()\n    self.indices = indices.cpu().numpy()\n    self.scores = scores.cpu().numpy()\n    self.strands = strands.cpu().numpy()\n    self.motifs = motifs\n\n    return self\n</code></pre>"},{"location":"reference/data/regions/","title":"Regions","text":""},{"location":"reference/data/regions/#chromatinhd.data.Regions","title":"<code>chromatinhd.data.Regions</code>","text":"<p>         Bases: <code>Flow</code></p> <p>Regions, typically centered around a transcription start site</p> Source code in <code>src/chromatinhd/data/regions.py</code> <pre><code>class Regions(Flow):\n\"\"\"\n    Regions, typically centered around a transcription start site\n    \"\"\"\n\n    coordinates = TSV(columns=[\"chrom\", \"start\", \"end\"])\n    window = Stored()\n\n    @classmethod\n    def from_transcripts(cls, transcripts: pd.DataFrame, window: [list, np.ndarray], path: pathlib.Path) -&gt; Regions:\n\"\"\"\n        Create regions from a dataframe of transcripts,\n        using a specified window around each transcription start site.\n\n        Parameters:\n            transcripts:\n                Dataframe of transcripts, with columns chrom, start, end, strand, ensembl_transcript_id\n            window:\n                Window around each transcription start site. Should be a 2-element array, e.g. [-10000, 10000]\n            path:\n                Folder in which the regions data will be stored\n        Returns:\n            Regions\n        \"\"\"\n        transcripts[\"tss\"] = transcripts[\"start\"] * (transcripts[\"strand\"] == 1) + transcripts[\"end\"] * (\n            transcripts[\"strand\"] == -1\n        )\n\n        regions = transcripts[[\"chrom\", \"tss\", \"ensembl_transcript_id\"]].copy()\n\n        regions[\"strand\"] = transcripts[\"strand\"]\n        regions[\"positive_strand\"] = (regions[\"strand\"] == 1).astype(int)\n        regions[\"negative_strand\"] = (regions[\"strand\"] == -1).astype(int)\n        regions[\"chrom\"] = transcripts.loc[regions.index, \"chrom\"]\n\n        regions[\"start\"] = regions[\"tss\"] + window[0] * (regions[\"strand\"] == 1) - window[1] * (regions[\"strand\"] == -1)\n        regions[\"end\"] = regions[\"tss\"] + window[1] * (regions[\"strand\"] == -1) - window[0] * (regions[\"strand\"] == 1)\n\n        return cls.create(\n            path=path,\n            coordinates=regions[[\"chrom\", \"start\", \"end\", \"tss\", \"strand\", \"ensembl_transcript_id\"]],\n            window=window,\n        )\n\n    def filter_genes(self, genes, path=None) -&gt; Regions:\n\"\"\"\n        Filter genes to those in the regions\n\n        Parameters:\n            genes:\n                Genes to filter. Should be a pandas Series with the index being the ensembl transcript ids.\n            path:\n                Path to store the filtered regions\n        Returns:\n            Regions with only the specified genes\n        \"\"\"\n\n        return Regions.create(coordinates=self.coordinates.loc[genes], window=self.window, path=path)\n</code></pre>"},{"location":"reference/data/regions/#chromatinhd.data.regions.Regions.filter_genes","title":"<code>filter_genes(genes, path=None)</code>","text":"<p>Filter genes to those in the regions</p> <p>Parameters:</p> Name Type Description Default <code>genes</code> <p>Genes to filter. Should be a pandas Series with the index being the ensembl transcript ids.</p> required <code>path</code> <p>Path to store the filtered regions</p> <code>None</code> <p>Returns:</p> Type Description <code>Regions</code> <p>Regions with only the specified genes</p> Source code in <code>src/chromatinhd/data/regions.py</code> <pre><code>def filter_genes(self, genes, path=None) -&gt; Regions:\n\"\"\"\n    Filter genes to those in the regions\n\n    Parameters:\n        genes:\n            Genes to filter. Should be a pandas Series with the index being the ensembl transcript ids.\n        path:\n            Path to store the filtered regions\n    Returns:\n        Regions with only the specified genes\n    \"\"\"\n\n    return Regions.create(coordinates=self.coordinates.loc[genes], window=self.window, path=path)\n</code></pre>"},{"location":"reference/data/regions/#chromatinhd.data.regions.Regions.from_transcripts","title":"<code>from_transcripts(transcripts, window, path)</code>  <code>classmethod</code>","text":"<p>Create regions from a dataframe of transcripts, using a specified window around each transcription start site.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts</code> <code>pd.DataFrame</code> <p>Dataframe of transcripts, with columns chrom, start, end, strand, ensembl_transcript_id</p> required <code>window</code> <code>[list, np.ndarray]</code> <p>Window around each transcription start site. Should be a 2-element array, e.g. [-10000, 10000]</p> required <code>path</code> <code>pathlib.Path</code> <p>Folder in which the regions data will be stored</p> required <p>Returns:</p> Type Description <code>Regions</code> <p>Regions</p> Source code in <code>src/chromatinhd/data/regions.py</code> <pre><code>@classmethod\ndef from_transcripts(cls, transcripts: pd.DataFrame, window: [list, np.ndarray], path: pathlib.Path) -&gt; Regions:\n\"\"\"\n    Create regions from a dataframe of transcripts,\n    using a specified window around each transcription start site.\n\n    Parameters:\n        transcripts:\n            Dataframe of transcripts, with columns chrom, start, end, strand, ensembl_transcript_id\n        window:\n            Window around each transcription start site. Should be a 2-element array, e.g. [-10000, 10000]\n        path:\n            Folder in which the regions data will be stored\n    Returns:\n        Regions\n    \"\"\"\n    transcripts[\"tss\"] = transcripts[\"start\"] * (transcripts[\"strand\"] == 1) + transcripts[\"end\"] * (\n        transcripts[\"strand\"] == -1\n    )\n\n    regions = transcripts[[\"chrom\", \"tss\", \"ensembl_transcript_id\"]].copy()\n\n    regions[\"strand\"] = transcripts[\"strand\"]\n    regions[\"positive_strand\"] = (regions[\"strand\"] == 1).astype(int)\n    regions[\"negative_strand\"] = (regions[\"strand\"] == -1).astype(int)\n    regions[\"chrom\"] = transcripts.loc[regions.index, \"chrom\"]\n\n    regions[\"start\"] = regions[\"tss\"] + window[0] * (regions[\"strand\"] == 1) - window[1] * (regions[\"strand\"] == -1)\n    regions[\"end\"] = regions[\"tss\"] + window[1] * (regions[\"strand\"] == -1) - window[0] * (regions[\"strand\"] == 1)\n\n    return cls.create(\n        path=path,\n        coordinates=regions[[\"chrom\", \"start\", \"end\", \"tss\", \"strand\", \"ensembl_transcript_id\"]],\n        window=window,\n    )\n</code></pre>"},{"location":"reference/data/regions/#chromatinhd.data.regions.select_tss_from_fragments","title":"<code>chromatinhd.data.regions.select_tss_from_fragments(transcripts, fragments_file, window=(-100, 100))</code>","text":"<p>Select the TSS with the most fragments within a window of the TSS</p> <p>Parameters:</p> Name Type Description Default <code>transcripts</code> <code>pd.DataFrame</code> <p>Dataframe of transcripts, with columns chrom, tss, ensembl_gene_id</p> required <code>fragments_file</code> <code>PathLike</code> <p>Path to fragments file</p> required <code>window</code> <code>[np.ndarray, tuple]</code> <p>Window around the TSS to count fragments</p> <code>(-100, 100)</code> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>Dataframe of transcripts, with columns chrom, tss, ensembl_gene_id, n_fragments</p> Source code in <code>src/chromatinhd/data/regions.py</code> <pre><code>def select_tss_from_fragments(\n    transcripts: pd.DataFrame, fragments_file: PathLike, window: [np.ndarray, tuple] = (-100, 100)\n) -&gt; pd.DataFrame:\n\"\"\"\n    Select the TSS with the most fragments within a window of the TSS\n\n    Parameters:\n        transcripts:\n            Dataframe of transcripts, with columns chrom, tss, ensembl_gene_id\n        fragments_file:\n            Path to fragments file\n        window:\n            Window around the TSS to count fragments\n    Returns:\n        Dataframe of transcripts, with columns chrom, tss, ensembl_gene_id, n_fragments\n    \"\"\"\n    if not ([col in transcripts.columns for col in [\"chrom\", \"tss\", \"ensembl_gene_id\"]]):\n        raise ValueError(\"Transcripts should have columns chrom, tss, ensembl_gene_id. \")\n\n    import pysam\n\n    fragments_tabix = pysam.TabixFile(str(fragments_file))\n\n    nfrags = []\n    for chrom, tss in tqdm.tqdm(zip(transcripts[\"chrom\"], transcripts[\"tss\"]), total=transcripts.shape[0]):\n        frags = list(fragments_tabix.fetch(chrom, tss + window[0], tss + window[1]))\n        nfrags.append(len(frags))\n    transcripts[\"n_fragments\"] = nfrags\n    selected_transcripts = (\n        transcripts.reset_index().sort_values(\"n_fragments\", ascending=False).groupby(\"ensembl_gene_id\").first()\n    )\n    selected_transcripts.index.name = \"gene\"\n    return selected_transcripts\n</code></pre>"},{"location":"reference/data/transcriptome/","title":"Transcriptome","text":""},{"location":"reference/data/transcriptome/#chromatinhd.data.Transcriptome","title":"<code>chromatinhd.data.Transcriptome</code>","text":"<p>         Bases: <code>Flow</code></p> <p>A transcriptome containing counts for each gene in each cell.</p> Source code in <code>src/chromatinhd/data/transcriptome/transcriptome.py</code> <pre><code>class Transcriptome(Flow):\n\"\"\"\n    A transcriptome containing counts for each gene in each cell.\n    \"\"\"\n\n    var: pd.DataFrame = TSV(index_name=\"gene\")\n    obs: pd.DataFrame = TSV(index_name=\"cell\")\n\n    adata = Stored()\n    \"Anndata object containing the transcriptome data.\"\n\n    def gene_id(self, symbol, column=\"symbol\"):\n\"\"\"\n        Get the gene id for a given gene symbol.\n        \"\"\"\n        assert all(pd.Series(symbol).isin(self.var[column])), set(\n            pd.Series(symbol)[~pd.Series(symbol).isin(self.var[column])]\n        )\n        return self.var.reset_index(\"gene\").set_index(column).loc[symbol][\"gene\"]\n\n    def symbol(self, gene_id, column=\"symbol\"):\n\"\"\"\n        Get the gene symbol for a given gene ID (e.g. Ensembl ID).\n        \"\"\"\n        assert all(pd.Series(gene_id).isin(self.var.index)), set(\n            pd.Series(gene_id)[~pd.Series(gene_id).isin(self.var.index)]\n        )\n        return self.var.loc[gene_id][column]\n\n    def gene_ix(self, symbol):\n\"\"\"\n        Get the gene index for a given gene symbol.\n        \"\"\"\n        self.var[\"ix\"] = np.arange(self.var.shape[0])\n        assert all(pd.Series(symbol).isin(self.var[\"symbol\"])), set(\n            pd.Series(symbol)[~pd.Series(symbol).isin(self.var[\"symbol\"])]\n        )\n        return self.var.reset_index(\"gene\").set_index(\"symbol\").loc[symbol][\"ix\"]\n\n    def create_X(self):\n        X_scipy = self.adata.X\n        if isinstance(X_scipy, np.ndarray):\n            import scipy.sparse\n\n            X_scipy = scipy.sparse.csr_matrix(X_scipy)\n        X = sparse.COOMatrix.from_scipy_csr(X_scipy)\n        X.populate_mapping()\n\n        self.X = X\n\n    X = Stored()\n    \"The main transcriptome data, typically normalized counts.\"\n\n    @classmethod\n    def from_adata(cls, adata, path: Union[pathlib.Path, str] = None):\n\"\"\"\n        Create a Transcriptome object from an AnnData object.\n\n        Parameters:\n            adata:\n                Anndata object containing the transcriptome data.\n            path:\n                Folder in which the transcriptome data will be stored.\n        \"\"\"\n        transcriptome = cls(path=path)\n        transcriptome.adata = adata\n\n        for k, v in adata.layers.items():\n            transcriptome.layers[k] = v\n        transcriptome.X = adata.X\n        transcriptome.var = adata.var\n        transcriptome.obs = adata.obs\n        return transcriptome\n\n    layers = StoredDict(Stored)\n    \"Dictionary of layers, such as raw, normalized and imputed data.\"\n\n    def filter_genes(self, genes, path=None):\n\"\"\"\n        Filter genes\n\n        Parameters:\n            genes:\n                Genes to filter. Should be a pandas Series with the index being the ensembl transcript ids.\n        \"\"\"\n\n        self.var[\"ix\"] = np.arange(self.var.shape[0])\n        gene_ixs = self.var[\"ix\"].loc[genes]\n\n        layers = {}\n        for k, v in self.layers.items():\n            layers[k] = v[:, gene_ixs]\n        X = self.X[:, gene_ixs]\n\n        return Transcriptome.create(\n            var=self.var.loc[genes],\n            obs=self.obs,\n            X=X,\n            layers=layers,\n            path=path,\n        )\n\n    def get_X(self, gene_ids):\n\"\"\"\n        Get the counts for a given set of genes.\n        \"\"\"\n        gene_ixs = self.var.index.get_loc(gene_ids)\n        value = self.X[:, gene_ixs]\n\n        if sparse.is_scipysparse(value):\n            value = np.array(value.todense())\n            if isinstance(gene_ids, str):\n                value = value[:, 0]\n        return value\n</code></pre>"},{"location":"reference/data/transcriptome/#chromatinhd.data.transcriptome.transcriptome.Transcriptome.X","title":"<code>X = Stored()</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>The main transcriptome data, typically normalized counts.</p>"},{"location":"reference/data/transcriptome/#chromatinhd.data.transcriptome.transcriptome.Transcriptome.adata","title":"<code>adata = Stored()</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Anndata object containing the transcriptome data.</p>"},{"location":"reference/data/transcriptome/#chromatinhd.data.transcriptome.transcriptome.Transcriptome.layers","title":"<code>layers = StoredDict(Stored)</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Dictionary of layers, such as raw, normalized and imputed data.</p>"},{"location":"reference/data/transcriptome/#chromatinhd.data.transcriptome.transcriptome.Transcriptome.filter_genes","title":"<code>filter_genes(genes, path=None)</code>","text":"<p>Filter genes</p> <p>Parameters:</p> Name Type Description Default <code>genes</code> <p>Genes to filter. Should be a pandas Series with the index being the ensembl transcript ids.</p> required Source code in <code>src/chromatinhd/data/transcriptome/transcriptome.py</code> <pre><code>def filter_genes(self, genes, path=None):\n\"\"\"\n    Filter genes\n\n    Parameters:\n        genes:\n            Genes to filter. Should be a pandas Series with the index being the ensembl transcript ids.\n    \"\"\"\n\n    self.var[\"ix\"] = np.arange(self.var.shape[0])\n    gene_ixs = self.var[\"ix\"].loc[genes]\n\n    layers = {}\n    for k, v in self.layers.items():\n        layers[k] = v[:, gene_ixs]\n    X = self.X[:, gene_ixs]\n\n    return Transcriptome.create(\n        var=self.var.loc[genes],\n        obs=self.obs,\n        X=X,\n        layers=layers,\n        path=path,\n    )\n</code></pre>"},{"location":"reference/data/transcriptome/#chromatinhd.data.transcriptome.transcriptome.Transcriptome.from_adata","title":"<code>from_adata(adata, path=None)</code>  <code>classmethod</code>","text":"<p>Create a Transcriptome object from an AnnData object.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <p>Anndata object containing the transcriptome data.</p> required <code>path</code> <code>Union[pathlib.Path, str]</code> <p>Folder in which the transcriptome data will be stored.</p> <code>None</code> Source code in <code>src/chromatinhd/data/transcriptome/transcriptome.py</code> <pre><code>@classmethod\ndef from_adata(cls, adata, path: Union[pathlib.Path, str] = None):\n\"\"\"\n    Create a Transcriptome object from an AnnData object.\n\n    Parameters:\n        adata:\n            Anndata object containing the transcriptome data.\n        path:\n            Folder in which the transcriptome data will be stored.\n    \"\"\"\n    transcriptome = cls(path=path)\n    transcriptome.adata = adata\n\n    for k, v in adata.layers.items():\n        transcriptome.layers[k] = v\n    transcriptome.X = adata.X\n    transcriptome.var = adata.var\n    transcriptome.obs = adata.obs\n    return transcriptome\n</code></pre>"},{"location":"reference/data/transcriptome/#chromatinhd.data.transcriptome.transcriptome.Transcriptome.gene_id","title":"<code>gene_id(symbol, column='symbol')</code>","text":"<p>Get the gene id for a given gene symbol.</p> Source code in <code>src/chromatinhd/data/transcriptome/transcriptome.py</code> <pre><code>def gene_id(self, symbol, column=\"symbol\"):\n\"\"\"\n    Get the gene id for a given gene symbol.\n    \"\"\"\n    assert all(pd.Series(symbol).isin(self.var[column])), set(\n        pd.Series(symbol)[~pd.Series(symbol).isin(self.var[column])]\n    )\n    return self.var.reset_index(\"gene\").set_index(column).loc[symbol][\"gene\"]\n</code></pre>"},{"location":"reference/data/transcriptome/#chromatinhd.data.transcriptome.transcriptome.Transcriptome.gene_ix","title":"<code>gene_ix(symbol)</code>","text":"<p>Get the gene index for a given gene symbol.</p> Source code in <code>src/chromatinhd/data/transcriptome/transcriptome.py</code> <pre><code>def gene_ix(self, symbol):\n\"\"\"\n    Get the gene index for a given gene symbol.\n    \"\"\"\n    self.var[\"ix\"] = np.arange(self.var.shape[0])\n    assert all(pd.Series(symbol).isin(self.var[\"symbol\"])), set(\n        pd.Series(symbol)[~pd.Series(symbol).isin(self.var[\"symbol\"])]\n    )\n    return self.var.reset_index(\"gene\").set_index(\"symbol\").loc[symbol][\"ix\"]\n</code></pre>"},{"location":"reference/data/transcriptome/#chromatinhd.data.transcriptome.transcriptome.Transcriptome.get_X","title":"<code>get_X(gene_ids)</code>","text":"<p>Get the counts for a given set of genes.</p> Source code in <code>src/chromatinhd/data/transcriptome/transcriptome.py</code> <pre><code>def get_X(self, gene_ids):\n\"\"\"\n    Get the counts for a given set of genes.\n    \"\"\"\n    gene_ixs = self.var.index.get_loc(gene_ids)\n    value = self.X[:, gene_ixs]\n\n    if sparse.is_scipysparse(value):\n        value = np.array(value.todense())\n        if isinstance(gene_ids, str):\n            value = value[:, 0]\n    return value\n</code></pre>"},{"location":"reference/data/transcriptome/#chromatinhd.data.transcriptome.transcriptome.Transcriptome.symbol","title":"<code>symbol(gene_id, column='symbol')</code>","text":"<p>Get the gene symbol for a given gene ID (e.g. Ensembl ID).</p> Source code in <code>src/chromatinhd/data/transcriptome/transcriptome.py</code> <pre><code>def symbol(self, gene_id, column=\"symbol\"):\n\"\"\"\n    Get the gene symbol for a given gene ID (e.g. Ensembl ID).\n    \"\"\"\n    assert all(pd.Series(gene_id).isin(self.var.index)), set(\n        pd.Series(gene_id)[~pd.Series(gene_id).isin(self.var.index)]\n    )\n    return self.var.loc[gene_id][column]\n</code></pre>"},{"location":"reference/models/diff/interpret/","title":"Interpret","text":""},{"location":"reference/models/diff/interpret/#chromatinhd.models.diff.interpret.GenePositional","title":"<code>chromatinhd.models.diff.interpret.GenePositional</code>","text":"<p>         Bases: <code>chd.flow.Flow</code></p> <p>Positional interpretation of diff models</p> Source code in <code>src/chromatinhd/models/diff/interpret/genepositional.py</code> <pre><code>class GenePositional(chd.flow.Flow):\n\"\"\"\n    Positional interpretation of *diff* models\n    \"\"\"\n\n    genes = chd.flow.Stored(default=set)\n\n    def score(\n        self,\n        fragments: Fragments,\n        clustering: Clustering,\n        models: Models,\n        genes: list = None,\n        force: bool = False,\n        device: str = None,\n        step: int = 25,\n        batch_size: int = 5000,\n    ):\n\"\"\"\n        Main scoring function\n\n        Parameters:\n            fragments:\n                the fragments\n            clustering:\n                the clustering\n            models:\n                the models\n            genes:\n                the genes to score, if None, all genes are scored\n            force:\n                whether to force rescoring even if the scores already exist\n            device:\n                the device to use\n        \"\"\"\n        force_ = force\n\n        if genes is None:\n            genes = fragments.var.index\n\n        pbar = tqdm.tqdm(genes, leave=False)\n\n        window = fragments.regions.window\n\n        if device is None:\n            device = get_default_device()\n\n        for gene in pbar:\n            pbar.set_description(gene)\n            probs_file = self.get_scoring_path(gene) / \"probs.pkl\"\n\n            force = force_\n            if not probs_file.exists():\n                force = True\n\n            if force:\n                design_gene = pd.DataFrame({\"gene_ix\": [fragments.var.index.get_loc(gene)]}).astype(\"category\")\n                design_gene.index = pd.Series([gene], name=\"gene\")\n                design_clustering = pd.DataFrame({\"active_cluster\": np.arange(clustering.n_clusters)}).astype(\n                    \"category\"\n                )\n                design_clustering.index = clustering.cluster_info.index\n                design_coord = pd.DataFrame({\"coord\": np.arange(window[0], window[1] + 1, step=step)}).astype(\n                    \"category\"\n                )\n                design_coord.index = design_coord[\"coord\"]\n                design = chd.utils.crossing(design_gene, design_clustering, design_coord)\n\n                design[\"batch\"] = np.floor(np.arange(design.shape[0]) / batch_size).astype(int)\n\n                probs = []\n                for model in models:\n                    probs_model = []\n                    for _, design_subset in design.groupby(\"batch\"):\n                        pseudocoordinates = torch.from_numpy(design_subset[\"coord\"].values.astype(int))\n                        pseudocoordinates = (pseudocoordinates - window[0]) / (window[1] - window[0])\n                        pseudocluster = torch.nn.functional.one_hot(\n                            torch.from_numpy(design_subset[\"active_cluster\"].values.astype(int)),\n                            clustering.n_clusters,\n                        ).to(torch.float)\n                        gene_ix = torch.from_numpy(design_subset[\"gene_ix\"].values.astype(int))\n\n                        prob = model.evaluate_pseudo(\n                            pseudocoordinates,\n                            clustering=pseudocluster,\n                            gene_ix=gene_ix,\n                            device=device,\n                        )\n\n                        probs_model.append(prob.numpy())\n                    probs_model = np.hstack(probs_model)\n                    probs.append(probs_model)\n\n                probs = np.vstack(probs)\n                probs = probs.mean(axis=0)\n\n                probs = xr.DataArray(\n                    probs.reshape(  # we have only one gene anyway\n                        (\n                            design_clustering.shape[0],\n                            design_coord.shape[0],\n                        )\n                    ),\n                    coords=[\n                        design_clustering.index,\n                        design_coord.index,\n                    ],\n                )\n\n                pickle.dump(probs, probs_file.open(\"wb\"))\n\n                self.genes = self.genes | {gene}\n\n    def get_plotdata(self, gene: str) -&gt; (pd.DataFrame, pd.DataFrame):\n\"\"\"\n        Returns average and differential probabilities for a particular gene.\n\n        Parameters:\n            gene:\n                the gene\n\n        Returns:\n            Two dataframes, one with the probabilities per cluster, one with the mean\n        \"\"\"\n        probs_file = self.get_scoring_path(gene) / \"probs.pkl\"\n        if not probs_file.exists():\n            raise FileNotFoundError(f\"File {probs_file} does not exist\")\n\n        probs = pickle.load(probs_file.open(\"rb\"))\n        plotdata = probs.to_dataframe(\"prob\")\n\n        window = probs.coords[\"coord\"].values[[0, -1]]\n\n        plotdata[\"prob\"] = (\n            plotdata[\"prob\"]\n            - np.log(\n                plotdata.reset_index()\n                .groupby([\"cluster\"])\n                .apply(\n                    lambda x: np.trapz(\n                        np.exp(x[\"prob\"]),\n                        x[\"coord\"].astype(float) / (window[1] - window[0]),\n                    )\n                )\n            ).mean()\n        )\n        plotdata_mean = plotdata[[\"prob\"]].groupby(\"coord\").mean()\n\n        return plotdata, plotdata_mean\n\n    def get_scoring_path(self, gene: str):\n        path = self.path / f\"{gene}\"\n        path.mkdir(parents=True, exist_ok=True)\n        return path\n</code></pre>"},{"location":"reference/models/diff/interpret/#chromatinhd.models.diff.interpret.genepositional.GenePositional.get_plotdata","title":"<code>get_plotdata(gene)</code>","text":"<p>Returns average and differential probabilities for a particular gene.</p> <p>Parameters:</p> Name Type Description Default <code>gene</code> <code>str</code> <p>the gene</p> required <p>Returns:</p> Type Description <code>pd.DataFrame, pd.DataFrame</code> <p>Two dataframes, one with the probabilities per cluster, one with the mean</p> Source code in <code>src/chromatinhd/models/diff/interpret/genepositional.py</code> <pre><code>def get_plotdata(self, gene: str) -&gt; (pd.DataFrame, pd.DataFrame):\n\"\"\"\n    Returns average and differential probabilities for a particular gene.\n\n    Parameters:\n        gene:\n            the gene\n\n    Returns:\n        Two dataframes, one with the probabilities per cluster, one with the mean\n    \"\"\"\n    probs_file = self.get_scoring_path(gene) / \"probs.pkl\"\n    if not probs_file.exists():\n        raise FileNotFoundError(f\"File {probs_file} does not exist\")\n\n    probs = pickle.load(probs_file.open(\"rb\"))\n    plotdata = probs.to_dataframe(\"prob\")\n\n    window = probs.coords[\"coord\"].values[[0, -1]]\n\n    plotdata[\"prob\"] = (\n        plotdata[\"prob\"]\n        - np.log(\n            plotdata.reset_index()\n            .groupby([\"cluster\"])\n            .apply(\n                lambda x: np.trapz(\n                    np.exp(x[\"prob\"]),\n                    x[\"coord\"].astype(float) / (window[1] - window[0]),\n                )\n            )\n        ).mean()\n    )\n    plotdata_mean = plotdata[[\"prob\"]].groupby(\"coord\").mean()\n\n    return plotdata, plotdata_mean\n</code></pre>"},{"location":"reference/models/diff/interpret/#chromatinhd.models.diff.interpret.genepositional.GenePositional.score","title":"<code>score(fragments, clustering, models, genes=None, force=False, device=None, step=25, batch_size=5000)</code>","text":"<p>Main scoring function</p> <p>Parameters:</p> Name Type Description Default <code>fragments</code> <code>Fragments</code> <p>the fragments</p> required <code>clustering</code> <code>Clustering</code> <p>the clustering</p> required <code>models</code> <code>Models</code> <p>the models</p> required <code>genes</code> <code>list</code> <p>the genes to score, if None, all genes are scored</p> <code>None</code> <code>force</code> <code>bool</code> <p>whether to force rescoring even if the scores already exist</p> <code>False</code> <code>device</code> <code>str</code> <p>the device to use</p> <code>None</code> Source code in <code>src/chromatinhd/models/diff/interpret/genepositional.py</code> <pre><code>def score(\n    self,\n    fragments: Fragments,\n    clustering: Clustering,\n    models: Models,\n    genes: list = None,\n    force: bool = False,\n    device: str = None,\n    step: int = 25,\n    batch_size: int = 5000,\n):\n\"\"\"\n    Main scoring function\n\n    Parameters:\n        fragments:\n            the fragments\n        clustering:\n            the clustering\n        models:\n            the models\n        genes:\n            the genes to score, if None, all genes are scored\n        force:\n            whether to force rescoring even if the scores already exist\n        device:\n            the device to use\n    \"\"\"\n    force_ = force\n\n    if genes is None:\n        genes = fragments.var.index\n\n    pbar = tqdm.tqdm(genes, leave=False)\n\n    window = fragments.regions.window\n\n    if device is None:\n        device = get_default_device()\n\n    for gene in pbar:\n        pbar.set_description(gene)\n        probs_file = self.get_scoring_path(gene) / \"probs.pkl\"\n\n        force = force_\n        if not probs_file.exists():\n            force = True\n\n        if force:\n            design_gene = pd.DataFrame({\"gene_ix\": [fragments.var.index.get_loc(gene)]}).astype(\"category\")\n            design_gene.index = pd.Series([gene], name=\"gene\")\n            design_clustering = pd.DataFrame({\"active_cluster\": np.arange(clustering.n_clusters)}).astype(\n                \"category\"\n            )\n            design_clustering.index = clustering.cluster_info.index\n            design_coord = pd.DataFrame({\"coord\": np.arange(window[0], window[1] + 1, step=step)}).astype(\n                \"category\"\n            )\n            design_coord.index = design_coord[\"coord\"]\n            design = chd.utils.crossing(design_gene, design_clustering, design_coord)\n\n            design[\"batch\"] = np.floor(np.arange(design.shape[0]) / batch_size).astype(int)\n\n            probs = []\n            for model in models:\n                probs_model = []\n                for _, design_subset in design.groupby(\"batch\"):\n                    pseudocoordinates = torch.from_numpy(design_subset[\"coord\"].values.astype(int))\n                    pseudocoordinates = (pseudocoordinates - window[0]) / (window[1] - window[0])\n                    pseudocluster = torch.nn.functional.one_hot(\n                        torch.from_numpy(design_subset[\"active_cluster\"].values.astype(int)),\n                        clustering.n_clusters,\n                    ).to(torch.float)\n                    gene_ix = torch.from_numpy(design_subset[\"gene_ix\"].values.astype(int))\n\n                    prob = model.evaluate_pseudo(\n                        pseudocoordinates,\n                        clustering=pseudocluster,\n                        gene_ix=gene_ix,\n                        device=device,\n                    )\n\n                    probs_model.append(prob.numpy())\n                probs_model = np.hstack(probs_model)\n                probs.append(probs_model)\n\n            probs = np.vstack(probs)\n            probs = probs.mean(axis=0)\n\n            probs = xr.DataArray(\n                probs.reshape(  # we have only one gene anyway\n                    (\n                        design_clustering.shape[0],\n                        design_coord.shape[0],\n                    )\n                ),\n                coords=[\n                    design_clustering.index,\n                    design_coord.index,\n                ],\n            )\n\n            pickle.dump(probs, probs_file.open(\"wb\"))\n\n            self.genes = self.genes | {gene}\n</code></pre>"},{"location":"reference/models/diff/model/","title":"Model","text":""},{"location":"reference/models/diff/model/#cutnf","title":"CutNF","text":"<p>The basic differential model that only looks at cut sites individually, regardless of the fragment's and cell's other cut sites</p>"},{"location":"reference/models/diff/model/#chromatinhd.models.diff.model.cutnf.Model","title":"<code>chromatinhd.models.diff.model.cutnf.Model</code>","text":"<p>         Bases: <code>torch.nn.Module</code>, <code>HybridModel</code></p> <p>A ChromatinHD-diff model that models the probability density of observing a cut site between clusterings</p> Source code in <code>src/chromatinhd/models/diff/model/cutnf.py</code> <pre><code>class Model(torch.nn.Module, HybridModel):\n\"\"\"\n    A ChromatinHD-diff model that models the probability density of observing a cut site between clusterings\n    \"\"\"\n\n    def __init__(\n        self,\n        fragments,\n        clustering,\n        nbins=(\n            128,\n            64,\n            32,\n        ),\n        decoder_n_layers=0,\n        baseline=False,\n        scale_likelihood=False,\n        rho_delta_regularization=True,\n        rho_delta_p_scale_free=False,\n        mixture_delta_regularization=True,\n        mixture_delta_p_scale_free=False,\n        mixture_delta_p_scale_dist=\"normal\",\n        mixture_delta_p_scale=1.0,\n    ):\n        super().__init__()\n\n        self.n_total_genes = fragments.n_genes\n\n        self.n_clusters = clustering.n_clusters\n\n        transform = DifferentialQuadraticSplineStack(\n            nbins=nbins,\n            n_genes=fragments.n_genes,\n        )\n        self.mixture = TransformedDistribution(transform)\n        n_delta_mixture_components = sum(transform.split_deltas)\n\n        if not baseline:\n            self.decoder = Decoder(\n                self.n_clusters,\n                fragments.n_genes,\n                n_delta_mixture_components,\n                n_layers=decoder_n_layers,\n            )\n        else:\n            self.decoder = BaselineDecoder(\n                self.n_clusters,\n                fragments.n_genes,\n                n_delta_mixture_components,\n                n_layers=decoder_n_layers,\n            )\n\n        # calculate libsizes and rho bias\n        libsize = torch.bincount(fragments.mapping[:, 0], minlength=fragments.n_cells)\n\n        rho_bias = (\n            torch.bincount(fragments.mapping[:, 1], minlength=fragments.n_genes)\n            / fragments.n_cells\n            / libsize.to(torch.float).mean()\n        )\n        min_rho_bias = 1e-5\n        rho_bias = min_rho_bias + (1 - min_rho_bias) * rho_bias\n        self.register_buffer(\"rho_bias\", rho_bias)\n\n        self.track = {}\n\n        self.mixture_delta_regularization = mixture_delta_regularization\n        if self.mixture_delta_regularization:\n            if mixture_delta_p_scale_free:\n                self.mixture_delta_p_scale = torch.nn.Parameter(\n                    torch.tensor(math.log(mixture_delta_p_scale), requires_grad=True)\n                )\n            else:\n                self.register_buffer(\n                    \"mixture_delta_p_scale\",\n                    torch.tensor(math.log(mixture_delta_p_scale)),\n                )\n        self.mixture_delta_p_scale_dist = mixture_delta_p_scale_dist\n\n        self.rho_delta_regularization = rho_delta_regularization\n        if self.rho_delta_regularization:\n            if rho_delta_p_scale_free:\n                self.rho_delta_p_scale = torch.nn.Parameter(torch.log(torch.tensor(0.1, requires_grad=True)))\n            else:\n                self.register_buffer(\"rho_delta_p_scale\", torch.tensor(math.log(1.0)))\n\n    def forward_(\n        self,\n        coordinates,\n        clustering,\n        genes_oi,\n        local_cellxgene_ix,\n        localcellxgene_ix,\n        local_gene_ix,\n    ):\n        # decode\n        mixture_delta, rho_delta = self.decoder(clustering, genes_oi)\n\n        # rho\n        rho = torch.nn.functional.softmax(torch.log(self.rho_bias) + rho_delta, -1)\n        rho_cuts = rho.flatten()[localcellxgene_ix]\n\n        # fragment counts\n        mixture_delta_cellxgene = mixture_delta.view(np.prod(mixture_delta.shape[:2]), mixture_delta.shape[-1])\n        mixture_delta = mixture_delta_cellxgene[local_cellxgene_ix]\n\n        self.track[\"likelihood_mixture\"] = likelihood_mixture = self.mixture.log_prob(\n            coordinates, genes_oi=genes_oi, local_gene_ix=local_gene_ix, delta=mixture_delta\n        )\n\n        self.track[\"likelihood_overall\"] = likelihood_overall = torch.log(rho_cuts) + math.log(self.n_total_genes)\n\n        # likelihood\n        likelihood = self.track[\"likelihood\"] = likelihood_mixture + likelihood_overall\n\n        elbo = -likelihood.sum()\n\n        # regularization\n        # mixture\n        if self.mixture_delta_regularization:\n            mixture_delta_p = torch.distributions.Normal(0.0, torch.exp(self.mixture_delta_p_scale))\n            mixture_delta_kl = mixture_delta_p.log_prob(self.decoder.logit_weight(genes_oi))\n\n            elbo -= mixture_delta_kl.sum()\n\n        # rho delta\n        if self.rho_delta_regularization:\n            rho_delta_p = torch.distributions.Normal(0.0, torch.exp(self.rho_delta_p_scale))\n            rho_delta_kl = rho_delta_p.log_prob(self.decoder.rho_weight(genes_oi))\n\n            elbo -= rho_delta_kl.sum()\n\n        return elbo\n\n    def forward(self, data):\n        return self.forward_(\n            coordinates=data.cuts.coordinates,\n            clustering=data.clustering.onehot,\n            genes_oi=data.minibatch.genes_oi_torch,\n            local_gene_ix=data.cuts.local_gene_ix,\n            local_cellxgene_ix=data.cuts.local_cellxgene_ix,\n            localcellxgene_ix=data.cuts.localcellxgene_ix,\n        )\n\n    def train_model(self, fragments, clustering, fold, device=None, n_epochs=30, lr=1e-2):\n\"\"\"\n        Trains the model\n        \"\"\"\n\n        if device is None:\n            device = get_default_device()\n\n        # set up minibatchers and loaders\n        minibatcher_train = Minibatcher(\n            fold[\"cells_train\"],\n            range(fragments.n_genes),\n            n_genes_step=500,\n            n_cells_step=200,\n        )\n        minibatcher_validation = Minibatcher(\n            fold[\"cells_validation\"],\n            range(fragments.n_genes),\n            n_genes_step=10,\n            n_cells_step=10000,\n            permute_cells=False,\n            permute_genes=False,\n        )\n\n        loaders_train = LoaderPool(\n            ClusteringCuts,\n            dict(\n                clustering=clustering,\n                fragments=fragments,\n                cellxgene_batch_size=minibatcher_train.cellxgene_batch_size,\n            ),\n            n_workers=10,\n        )\n        loaders_validation = LoaderPool(\n            ClusteringCuts,\n            dict(\n                clustering=clustering,\n                fragments=fragments,\n                cellxgene_batch_size=minibatcher_validation.cellxgene_batch_size,\n            ),\n            n_workers=5,\n        )\n\n        trainer = Trainer(\n            self,\n            loaders_train,\n            loaders_validation,\n            minibatcher_train,\n            minibatcher_validation,\n            SparseDenseAdam(\n                self.parameters_sparse(),\n                self.parameters_dense(),\n                lr=lr,\n                weight_decay=1e-5,\n            ),\n            n_epochs=n_epochs,\n            checkpoint_every_epoch=1,\n            optimize_every_step=1,\n            device=device,\n        )\n        self.trace = trainer.trace\n\n        trainer.train()\n\n    def _get_likelihood_cell_gene(self, likelihood, local_cellxgene_ix, n_cells, n_genes):\n        return torch_scatter.segment_sum_coo(likelihood, local_cellxgene_ix, dim_size=n_cells * n_genes).reshape(\n            (n_cells, n_genes)\n        )\n\n    def get_prediction(\n        self,\n        fragments: Fragments,\n        clustering: Clustering,\n        cells: List[str] = None,\n        cell_ixs: List[int] = None,\n        genes: List[str] = None,\n        gene_ixs: List[int] = None,\n        device: str = None,\n    ) -&gt; xr.Dataset:\n\"\"\"\n        Returns the likelihoods of the observed cut sites for each cell and gene\n\n        Parameters:\n            fragments: Fragments object\n            clustering: Clustering object\n            cells: Cells to predict\n            cell_ixs: Cell indices to predict\n            genes: Genes to predict\n            gene_ixs: Gene indices to predict\n            device: Device to use\n\n        Returns:\n            **likelihood_mixture**, likelihood of the observing a cut site at the particular genomic location, conditioned on the gene region. **likelihood_overall**, likelihood of observing a cut site in the gene region\n        \"\"\"\n\n        if cell_ixs is None:\n            if cells is None:\n                cells = fragments.obs.index\n            fragments.obs[\"ix\"] = np.arange(len(fragments.obs))\n            cell_ixs = fragments.obs.loc[cells][\"ix\"].values\n        if cells is None:\n            cells = fragments.obs.index[cell_ixs]\n\n        if gene_ixs is None:\n            if genes is None:\n                genes = fragments.var.index\n            fragments.var[\"ix\"] = np.arange(len(fragments.var))\n            gene_ixs = fragments.var.loc[genes][\"ix\"].values\n        if genes is None:\n            genes = fragments.var.index[gene_ixs]\n\n        minibatches = Minibatcher(\n            cell_ixs,\n            gene_ixs,\n            n_genes_step=500,\n            n_cells_step=200,\n            use_all_cells=True,\n            use_all_genes=True,\n            permute_cells=False,\n            permute_genes=False,\n        )\n        loaders = LoaderPool(\n            ClusteringCuts,\n            dict(\n                clustering=clustering,\n                fragments=fragments,\n                cellxgene_batch_size=minibatches.cellxgene_batch_size,\n            ),\n            n_workers=5,\n        )\n        loaders.initialize(minibatches)\n\n        likelihood_mixture = np.zeros((len(cell_ixs), len(gene_ixs)))\n        likelihood_overall = np.zeros((len(cell_ixs), len(gene_ixs)))\n\n        cell_mapping = np.zeros(fragments.n_cells, dtype=np.int64)\n        cell_mapping[cell_ixs] = np.arange(len(cell_ixs))\n\n        gene_mapping = np.zeros(fragments.n_genes, dtype=np.int64)\n        gene_mapping[gene_ixs] = np.arange(len(gene_ixs))\n\n        self.eval()\n        self = self.to(device)\n\n        for data in loaders:\n            data = data.to(device)\n            with torch.no_grad():\n                self.forward(data)\n\n            likelihood_mixture[\n                np.ix_(\n                    cell_mapping[data.minibatch.cells_oi],\n                    gene_mapping[data.minibatch.genes_oi],\n                )\n            ] += (\n                self._get_likelihood_cell_gene(\n                    self.track[\"likelihood_mixture\"],\n                    data.cuts.local_cellxgene_ix,\n                    data.minibatch.n_cells,\n                    data.minibatch.n_genes,\n                )\n                .cpu()\n                .numpy()\n            )\n            likelihood_overall[\n                np.ix_(\n                    cell_mapping[data.minibatch.cells_oi],\n                    gene_mapping[data.minibatch.genes_oi],\n                )\n            ] += (\n                self._get_likelihood_cell_gene(\n                    self.track[\"likelihood_overall\"],\n                    data.cuts.local_cellxgene_ix,\n                    data.minibatch.n_cells,\n                    data.minibatch.n_genes,\n                )\n                .cpu()\n                .numpy()\n            )\n\n        self = self.to(\"cpu\")\n\n        result = xr.Dataset(\n            {\n                \"likelihood_mixture\": xr.DataArray(\n                    likelihood_mixture,\n                    dims=(\"cell\", \"gene\"),\n                    coords={\"cell\": cells, \"gene\": fragments.var.index},\n                ),\n                \"likelihood_overall\": xr.DataArray(\n                    likelihood_overall,\n                    dims=(\"cell\", \"gene\"),\n                    coords={\"cell\": cells, \"gene\": fragments.var.index},\n                ),\n            }\n        )\n        return result\n\n    def evaluate_pseudo(\n        self,\n        coordinates,\n        clustering=None,\n        gene_oi=None,\n        gene_ix=None,\n        device=None,\n    ):\n        from chromatinhd.models.diff.loader.clustering import Result as ClusteringResult\n        from chromatinhd.models.diff.loader.clustering_cuts import (\n            Result as ClusteringCutsResult,\n        )\n        from chromatinhd.models.diff.loader.cuts import Result as CutsResult\n        from chromatinhd.models.diff.loader.minibatches import Minibatch\n\n        if not torch.is_tensor(clustering):\n            if clustering is None:\n                clustering = 0.0\n            clustering = torch.ones((1, self.n_clusters)) * clustering\n\n            print(clustering)\n\n        cells_oi = torch.ones((1,), dtype=torch.long)\n\n        local_cellxgene_ix = torch.tensor([], dtype=torch.long)\n        if gene_ix is None:\n            if gene_oi is None:\n                gene_oi = 0\n            genes_oi = torch.tensor([gene_oi], dtype=torch.long)\n            local_gene_ix = torch.zeros_like(coordinates).to(torch.long)\n            local_cellxgene_ix = torch.zeros_like(coordinates).to(torch.long)\n            localcellxgene_ix = torch.ones_like(coordinates).to(torch.long) * gene_oi\n        else:\n            assert len(gene_ix) == len(coordinates)\n            genes_oi = torch.unique(gene_ix)\n\n            local_gene_mapping = torch.zeros(genes_oi.max() + 1, dtype=torch.long)\n            local_gene_mapping.index_add_(0, genes_oi, torch.arange(len(genes_oi)))\n\n            local_gene_ix = local_gene_mapping[gene_ix]\n            local_cell_ix = torch.arange(clustering.shape[0])\n            local_cellxgene_ix = local_cell_ix * len(genes_oi) + local_gene_ix\n            localcellxgene_ix = local_cell_ix * self.n_total_genes + gene_ix\n\n        data = ClusteringCutsResult(\n            cuts=CutsResult(\n                coordinates=coordinates,\n                local_cellxgene_ix=local_cellxgene_ix,\n                localcellxgene_ix=localcellxgene_ix,\n                n_genes=len(genes_oi),\n            ),\n            clustering=ClusteringResult(\n                onehot=clustering,\n            ),\n            minibatch=Minibatch(\n                cells_oi=cells_oi.cpu().numpy(),\n                genes_oi=genes_oi.cpu().numpy(),\n            ),\n        ).to(device)\n\n        self = self.to(device).eval()\n\n        with torch.no_grad():\n            self.forward(data)\n\n        self = self.to(\"cpu\")\n\n        prob = self.track[\"likelihood\"].detach().cpu()\n        return prob.detach().cpu()\n</code></pre>"},{"location":"reference/models/diff/model/#chromatinhd.models.diff.model.cutnf.Model.get_prediction","title":"<code>get_prediction(fragments, clustering, cells=None, cell_ixs=None, genes=None, gene_ixs=None, device=None)</code>","text":"<p>Returns the likelihoods of the observed cut sites for each cell and gene</p> <p>Parameters:</p> Name Type Description Default <code>fragments</code> <code>Fragments</code> <p>Fragments object</p> required <code>clustering</code> <code>Clustering</code> <p>Clustering object</p> required <code>cells</code> <code>List[str]</code> <p>Cells to predict</p> <code>None</code> <code>cell_ixs</code> <code>List[int]</code> <p>Cell indices to predict</p> <code>None</code> <code>genes</code> <code>List[str]</code> <p>Genes to predict</p> <code>None</code> <code>gene_ixs</code> <code>List[int]</code> <p>Gene indices to predict</p> <code>None</code> <code>device</code> <code>str</code> <p>Device to use</p> <code>None</code> <p>Returns:</p> Type Description <code>xr.Dataset</code> <p>likelihood_mixture, likelihood of the observing a cut site at the particular genomic location, conditioned on the gene region. likelihood_overall, likelihood of observing a cut site in the gene region</p> Source code in <code>src/chromatinhd/models/diff/model/cutnf.py</code> <pre><code>def get_prediction(\n    self,\n    fragments: Fragments,\n    clustering: Clustering,\n    cells: List[str] = None,\n    cell_ixs: List[int] = None,\n    genes: List[str] = None,\n    gene_ixs: List[int] = None,\n    device: str = None,\n) -&gt; xr.Dataset:\n\"\"\"\n    Returns the likelihoods of the observed cut sites for each cell and gene\n\n    Parameters:\n        fragments: Fragments object\n        clustering: Clustering object\n        cells: Cells to predict\n        cell_ixs: Cell indices to predict\n        genes: Genes to predict\n        gene_ixs: Gene indices to predict\n        device: Device to use\n\n    Returns:\n        **likelihood_mixture**, likelihood of the observing a cut site at the particular genomic location, conditioned on the gene region. **likelihood_overall**, likelihood of observing a cut site in the gene region\n    \"\"\"\n\n    if cell_ixs is None:\n        if cells is None:\n            cells = fragments.obs.index\n        fragments.obs[\"ix\"] = np.arange(len(fragments.obs))\n        cell_ixs = fragments.obs.loc[cells][\"ix\"].values\n    if cells is None:\n        cells = fragments.obs.index[cell_ixs]\n\n    if gene_ixs is None:\n        if genes is None:\n            genes = fragments.var.index\n        fragments.var[\"ix\"] = np.arange(len(fragments.var))\n        gene_ixs = fragments.var.loc[genes][\"ix\"].values\n    if genes is None:\n        genes = fragments.var.index[gene_ixs]\n\n    minibatches = Minibatcher(\n        cell_ixs,\n        gene_ixs,\n        n_genes_step=500,\n        n_cells_step=200,\n        use_all_cells=True,\n        use_all_genes=True,\n        permute_cells=False,\n        permute_genes=False,\n    )\n    loaders = LoaderPool(\n        ClusteringCuts,\n        dict(\n            clustering=clustering,\n            fragments=fragments,\n            cellxgene_batch_size=minibatches.cellxgene_batch_size,\n        ),\n        n_workers=5,\n    )\n    loaders.initialize(minibatches)\n\n    likelihood_mixture = np.zeros((len(cell_ixs), len(gene_ixs)))\n    likelihood_overall = np.zeros((len(cell_ixs), len(gene_ixs)))\n\n    cell_mapping = np.zeros(fragments.n_cells, dtype=np.int64)\n    cell_mapping[cell_ixs] = np.arange(len(cell_ixs))\n\n    gene_mapping = np.zeros(fragments.n_genes, dtype=np.int64)\n    gene_mapping[gene_ixs] = np.arange(len(gene_ixs))\n\n    self.eval()\n    self = self.to(device)\n\n    for data in loaders:\n        data = data.to(device)\n        with torch.no_grad():\n            self.forward(data)\n\n        likelihood_mixture[\n            np.ix_(\n                cell_mapping[data.minibatch.cells_oi],\n                gene_mapping[data.minibatch.genes_oi],\n            )\n        ] += (\n            self._get_likelihood_cell_gene(\n                self.track[\"likelihood_mixture\"],\n                data.cuts.local_cellxgene_ix,\n                data.minibatch.n_cells,\n                data.minibatch.n_genes,\n            )\n            .cpu()\n            .numpy()\n        )\n        likelihood_overall[\n            np.ix_(\n                cell_mapping[data.minibatch.cells_oi],\n                gene_mapping[data.minibatch.genes_oi],\n            )\n        ] += (\n            self._get_likelihood_cell_gene(\n                self.track[\"likelihood_overall\"],\n                data.cuts.local_cellxgene_ix,\n                data.minibatch.n_cells,\n                data.minibatch.n_genes,\n            )\n            .cpu()\n            .numpy()\n        )\n\n    self = self.to(\"cpu\")\n\n    result = xr.Dataset(\n        {\n            \"likelihood_mixture\": xr.DataArray(\n                likelihood_mixture,\n                dims=(\"cell\", \"gene\"),\n                coords={\"cell\": cells, \"gene\": fragments.var.index},\n            ),\n            \"likelihood_overall\": xr.DataArray(\n                likelihood_overall,\n                dims=(\"cell\", \"gene\"),\n                coords={\"cell\": cells, \"gene\": fragments.var.index},\n            ),\n        }\n    )\n    return result\n</code></pre>"},{"location":"reference/models/diff/model/#chromatinhd.models.diff.model.cutnf.Model.train_model","title":"<code>train_model(fragments, clustering, fold, device=None, n_epochs=30, lr=0.01)</code>","text":"<p>Trains the model</p> Source code in <code>src/chromatinhd/models/diff/model/cutnf.py</code> <pre><code>def train_model(self, fragments, clustering, fold, device=None, n_epochs=30, lr=1e-2):\n\"\"\"\n    Trains the model\n    \"\"\"\n\n    if device is None:\n        device = get_default_device()\n\n    # set up minibatchers and loaders\n    minibatcher_train = Minibatcher(\n        fold[\"cells_train\"],\n        range(fragments.n_genes),\n        n_genes_step=500,\n        n_cells_step=200,\n    )\n    minibatcher_validation = Minibatcher(\n        fold[\"cells_validation\"],\n        range(fragments.n_genes),\n        n_genes_step=10,\n        n_cells_step=10000,\n        permute_cells=False,\n        permute_genes=False,\n    )\n\n    loaders_train = LoaderPool(\n        ClusteringCuts,\n        dict(\n            clustering=clustering,\n            fragments=fragments,\n            cellxgene_batch_size=minibatcher_train.cellxgene_batch_size,\n        ),\n        n_workers=10,\n    )\n    loaders_validation = LoaderPool(\n        ClusteringCuts,\n        dict(\n            clustering=clustering,\n            fragments=fragments,\n            cellxgene_batch_size=minibatcher_validation.cellxgene_batch_size,\n        ),\n        n_workers=5,\n    )\n\n    trainer = Trainer(\n        self,\n        loaders_train,\n        loaders_validation,\n        minibatcher_train,\n        minibatcher_validation,\n        SparseDenseAdam(\n            self.parameters_sparse(),\n            self.parameters_dense(),\n            lr=lr,\n            weight_decay=1e-5,\n        ),\n        n_epochs=n_epochs,\n        checkpoint_every_epoch=1,\n        optimize_every_step=1,\n        device=device,\n    )\n    self.trace = trainer.trace\n\n    trainer.train()\n</code></pre>"},{"location":"reference/models/diff/model/#chromatinhd.models.diff.model.cutnf.Models","title":"<code>chromatinhd.models.diff.model.cutnf.Models</code>","text":"<p>         Bases: <code>Flow</code></p> Source code in <code>src/chromatinhd/models/diff/model/cutnf.py</code> <pre><code>class Models(Flow):\n    n_models = Stored()\n\n    @property\n    def models_path(self):\n        path = self.path / \"models\"\n        path.mkdir(exist_ok=True)\n        return path\n\n    def train_models(self, fragments, clustering, folds, device=None, n_epochs=30, **kwargs):\n\"\"\"\n        Trains the models\n\n        Parameters:\n            fragments:\n                Fragments object\n        \"\"\"\n        self.n_models = len(folds)\n        for fold_ix, fold in [(fold_ix, fold) for fold_ix, fold in enumerate(folds)]:\n            desired_outputs = [self.models_path / (\"model_\" + str(fold_ix) + \".pkl\")]\n            force = False\n            if not all([desired_output.exists() for desired_output in desired_outputs]):\n                force = True\n\n            if force:\n                model = Model(fragments, clustering, **kwargs)\n                model.train_model(fragments, clustering, fold, device=device, n_epochs=n_epochs)\n\n                model = model.to(\"cpu\")\n\n                pickle.dump(\n                    model,\n                    open(self.models_path / (\"model_\" + str(fold_ix) + \".pkl\"), \"wb\"),\n                )\n\n    def __getitem__(self, ix):\n        return pickle.load((self.models_path / (\"model_\" + str(ix) + \".pkl\")).open(\"rb\"))\n\n    def __len__(self):\n        return self.n_models\n\n    def __iter__(self):\n        for ix in range(len(self)):\n            yield self[ix]\n</code></pre>"},{"location":"reference/models/diff/model/#chromatinhd.models.diff.model.cutnf.Models.train_models","title":"<code>train_models(fragments, clustering, folds, device=None, n_epochs=30, **kwargs)</code>","text":"<p>Trains the models</p> <p>Parameters:</p> Name Type Description Default <code>fragments</code> <p>Fragments object</p> required Source code in <code>src/chromatinhd/models/diff/model/cutnf.py</code> <pre><code>def train_models(self, fragments, clustering, folds, device=None, n_epochs=30, **kwargs):\n\"\"\"\n    Trains the models\n\n    Parameters:\n        fragments:\n            Fragments object\n    \"\"\"\n    self.n_models = len(folds)\n    for fold_ix, fold in [(fold_ix, fold) for fold_ix, fold in enumerate(folds)]:\n        desired_outputs = [self.models_path / (\"model_\" + str(fold_ix) + \".pkl\")]\n        force = False\n        if not all([desired_output.exists() for desired_output in desired_outputs]):\n            force = True\n\n        if force:\n            model = Model(fragments, clustering, **kwargs)\n            model.train_model(fragments, clustering, fold, device=device, n_epochs=n_epochs)\n\n            model = model.to(\"cpu\")\n\n            pickle.dump(\n                model,\n                open(self.models_path / (\"model_\" + str(fold_ix) + \".pkl\"), \"wb\"),\n            )\n</code></pre>"},{"location":"reference/models/diff/plot/","title":"Plot","text":""},{"location":"reference/models/diff/plot/#chromatinhd.models.diff.plot","title":"<code>chromatinhd.models.diff.plot</code>","text":""},{"location":"reference/models/pred/interpret/","title":"Interpret","text":""},{"location":"reference/models/pred/interpret/#chromatinhd.models.pred.interpret.GeneMultiWindow","title":"<code>chromatinhd.models.pred.interpret.GeneMultiWindow</code>","text":"<p>         Bases: <code>chd.flow.Flow</code></p> <p>Interpret a pred model positionally by censoring windows of across multiple window sizes.</p> Source code in <code>src/chromatinhd/models/pred/interpret/genemultiwindow.py</code> <pre><code>class GeneMultiWindow(chd.flow.Flow):\n\"\"\"\n    Interpret a *pred* model positionally by censoring windows of across multiple window sizes.\n    \"\"\"\n\n    design = chd.flow.Stored()\n\"\"\"\n    The design of the censoring windows.\n    \"\"\"\n\n    genes = chd.flow.Stored(default=set)\n\"\"\"\n    The genes that have been scored.\n    \"\"\"\n\n    def score(\n        self,\n        fragments,\n        transcriptome,\n        models,\n        folds,\n        genes,\n        censorer,\n        force=False,\n        device=None,\n    ):\n        force_ = force\n        design = censorer.design.iloc[1:].copy()\n        self.design = design\n\n        pbar = tqdm.tqdm(genes, leave=False)\n\n        for gene in pbar:\n            pbar.set_description(gene)\n            scores_file = self.get_scoring_path(gene) / \"scores.pkl\"\n\n            force = force_\n            if not scores_file.exists():\n                force = True\n\n            if force:\n                deltacor_folds = []\n                lost_folds = []\n                effect_folds = []\n                for fold, model in zip(folds, models):\n                    predicted, expected, n_fragments = model.get_prediction_censored(\n                        fragments,\n                        transcriptome,\n                        censorer,\n                        cell_ixs=np.concatenate([fold[\"cells_validation\"], fold[\"cells_test\"]]),\n                        genes=[gene],\n                        device=device,\n                    )\n\n                    # select 1st gene, given that we're working with one gene anyway\n                    predicted = predicted[..., 0]\n                    expected = expected[..., 0]\n                    n_fragments = n_fragments[..., 0]\n\n                    cor = chd.utils.paircor(predicted, expected, dim=-1)\n                    deltacor = cor[1:] - cor[0]\n\n                    lost = (n_fragments[0] - n_fragments[1:]).mean(-1)\n\n                    effect = (predicted[0] - predicted[1:]).mean(-1)\n\n                    deltacor_folds.append(deltacor)\n                    lost_folds.append(lost)\n                    effect_folds.append(effect)\n\n                deltacor_folds = np.stack(deltacor_folds, 0)\n                lost_folds = np.stack(lost_folds, 0)\n                effect_folds = np.stack(effect_folds, 0)\n\n                result = xr.Dataset(\n                    {\n                        \"deltacor\": xr.DataArray(\n                            deltacor_folds,\n                            coords=[\n                                (\"model\", np.arange(len(models))),\n                                (\"window\", design.index),\n                            ],\n                        ),\n                        \"lost\": xr.DataArray(\n                            lost_folds,\n                            coords=[\n                                (\"model\", np.arange(len(models))),\n                                (\"window\", design.index),\n                            ],\n                        ),\n                        \"effect\": xr.DataArray(\n                            effect_folds,\n                            coords=[\n                                (\"model\", np.arange(len(models))),\n                                (\"window\", design.index),\n                            ],\n                        ),\n                    }\n                )\n\n                pickle.dump(result, scores_file.open(\"wb\"))\n\n                self.genes = self.genes | {gene}\n\n    def interpolate(self, genes=None, force=False):\n        force_ = force\n\n        if genes is None:\n            genes = self.genes\n\n        pbar = tqdm.tqdm(genes, leave=False)\n\n        for gene in pbar:\n            pbar.set_description(gene)\n            scores_file = self.get_scoring_path(gene) / \"scores.pkl\"\n\n            if not scores_file.exists():\n                continue\n\n            interpolate_file = self.get_scoring_path(gene) / \"interpolated.pkl\"\n\n            force = force_\n            if not interpolate_file.exists():\n                force = True\n\n            if force:\n                scores = pickle.load(scores_file.open(\"rb\"))\n                x = scores[\"deltacor\"].values\n                scores_statistical = []\n                for i in range(x.shape[1]):\n                    scores_statistical.append(scipy.stats.ttest_1samp(x[:, i], 0, alternative=\"less\").pvalue)\n                scores_statistical = pd.DataFrame({\"pvalue\": scores_statistical})\n                scores_statistical[\"qval\"] = fdr(scores_statistical[\"pvalue\"])\n\n                plotdata = scores.mean(\"model\").stack().to_dataframe()\n                plotdata = self.design.join(plotdata)\n\n                plotdata[\"qval\"] = scores_statistical[\"qval\"].values\n\n                window_sizes_info = pd.DataFrame({\"window_size\": self.design[\"window_size\"].unique()}).set_index(\n                    \"window_size\"\n                )\n                window_sizes_info[\"ix\"] = np.arange(len(window_sizes_info))\n\n                # interpolate\n                positions_oi = np.arange(\n                    self.design[\"window_start\"].min(),\n                    self.design[\"window_end\"].max() + 1,\n                )\n\n                deltacor_interpolated = np.zeros((len(window_sizes_info), len(positions_oi)))\n                lost_interpolated = np.zeros((len(window_sizes_info), len(positions_oi)))\n                effect_interpolated = np.zeros((len(window_sizes_info), len(positions_oi)))\n                for window_size, window_size_info in window_sizes_info.iterrows():\n                    plotdata_oi = plotdata.query(\"window_size == @window_size\")\n                    x = plotdata_oi[\"window_mid\"].values.copy()\n                    y = plotdata_oi[\"deltacor\"].values.copy()\n                    y[plotdata_oi[\"qval\"] &gt; 0.1] = 0.0\n                    deltacor_interpolated_ = np.clip(\n                        np.interp(positions_oi, x, y) / window_size * 1000,\n                        -np.inf,\n                        0,\n                        # np.inf,\n                    )\n                    deltacor_interpolated[window_size_info[\"ix\"], :] = deltacor_interpolated_\n\n                    lost_interpolated_ = (\n                        np.interp(positions_oi, plotdata_oi[\"window_mid\"], plotdata_oi[\"lost\"]) / window_size * 1000\n                    )\n                    lost_interpolated[window_size_info[\"ix\"], :] = lost_interpolated_\n\n                    effect_interpolated_ = (\n                        np.interp(\n                            positions_oi,\n                            plotdata_oi[\"window_mid\"],\n                            plotdata_oi[\"effect\"],\n                        )\n                        / window_size\n                        * 1000\n                    )\n                    effect_interpolated[window_size_info[\"ix\"], :] = effect_interpolated_\n\n                deltacor = xr.DataArray(\n                    deltacor_interpolated.mean(0),\n                    coords=[\n                        (\"position\", positions_oi),\n                    ],\n                )\n                lost = xr.DataArray(\n                    lost_interpolated.mean(0),\n                    coords=[\n                        (\"position\", positions_oi),\n                    ],\n                )\n\n                effect = xr.DataArray(\n                    effect_interpolated.mean(0),\n                    coords=[\n                        (\"position\", positions_oi),\n                    ],\n                )\n\n                # save\n                interpolated = xr.Dataset({\"deltacor\": deltacor, \"lost\": lost, \"effect\": effect})\n                pickle.dump(\n                    interpolated,\n                    interpolate_file.open(\"wb\"),\n                )\n\n    def get_plotdata(self, gene):\n        interpolated_file = self.get_scoring_path(gene) / \"interpolated.pkl\"\n        if not interpolated_file.exists():\n            raise FileNotFoundError(f\"File {interpolated_file} does not exist\")\n\n        interpolated = pickle.load(interpolated_file.open(\"rb\"))\n\n        plotdata = interpolated.to_dataframe()\n\n        return plotdata\n\n    def get_scoring_path(self, gene):\n        path = self.path / f\"{gene}\"\n        path.mkdir(parents=True, exist_ok=True)\n        return path\n</code></pre>"},{"location":"reference/models/pred/interpret/#chromatinhd.models.pred.interpret.genemultiwindow.GeneMultiWindow.design","title":"<code>design = chd.flow.Stored()</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>The design of the censoring windows.</p>"},{"location":"reference/models/pred/interpret/#chromatinhd.models.pred.interpret.genemultiwindow.GeneMultiWindow.genes","title":"<code>genes = chd.flow.Stored(default=set)</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>The genes that have been scored.</p>"},{"location":"reference/models/pred/interpret/#chromatinhd.models.pred.interpret.GenePairWindow","title":"<code>chromatinhd.models.pred.interpret.GenePairWindow</code>","text":"<p>         Bases: <code>chd.flow.Flow</code></p> <p>Interpret a pred model positionally by censoring windows and comparing the decrease in predictivity per cell between pairs of windows</p> Source code in <code>src/chromatinhd/models/pred/interpret/genepairwindow.py</code> <pre><code>class GenePairWindow(chd.flow.Flow):\n\"\"\"\n    Interpret a *pred* model positionally by censoring windows and comparing\n    the decrease in predictivity per cell between pairs of windows\n    \"\"\"\n\n    design = chd.flow.Stored()\n\n    genes = chd.flow.Stored(default=set)\n\n    def score(\n        self,\n        fragments: Fragments,\n        transcriptome: Transcriptome,\n        models: Models,\n        folds: Folds,\n        censorer,\n        genes: Optional[List] = None,\n        force=False,\n        device=None,\n    ):\n\"\"\"\n        Score the models\n\n        Parameters:\n            fragments:\n                the fragments\n            transcriptome:\n                the transcriptome\n            models:\n                the models\n            folds:\n                the folds\n            genes:\n                which genes to score, defaults to all\n\n        \"\"\"\n        force_ = force\n        design = censorer.design.iloc[1:].copy()\n        self.design = design\n\n        if device is None:\n            device = get_default_device()\n\n        if genes is None:\n            genes = transcriptome.var.index\n\n        pbar = tqdm.tqdm(genes, leave=False)\n\n        for gene in pbar:\n            pbar.set_description(gene)\n            scores_file = self.get_scoring_path(gene) / \"scores.pkl\"\n            interaction_file = self.get_scoring_path(gene) / \"interaction.pkl\"\n\n            force = force_\n            if not all([file.exists() for file in [scores_file, interaction_file]]):\n                force = True\n\n            if force:\n                deltacor_folds = []\n                copredictivity_folds = []\n                lost_folds = []\n                for fold, model in zip(folds, models):\n                    predicted, expected, n_fragments = model.get_prediction_censored(\n                        fragments,\n                        transcriptome,\n                        censorer,\n                        cell_ixs=np.concatenate([fold[\"cells_validation\"], fold[\"cells_test\"]]),\n                        genes=[gene],\n                        device=device,\n                    )\n\n                    # select 1st gene, given that we're working with one gene anyway\n                    predicted = predicted[..., 0]\n                    expected = expected[..., 0]\n                    n_fragments = n_fragments[..., 0]\n\n                    # calculate delta cor per cell\n                    # calculate effect per cellxgene combination\n                    predicted_censored = predicted[1:]\n                    predicted_full = predicted[0][None, ...]\n                    predicted_full_norm = zscore(predicted_full, 1)\n                    predicted_censored_norm = zscore_relative(predicted_censored, predicted_full, 1)\n\n                    expected_norm = zscore(expected[None, ...], 1)\n\n                    celldeltacor = -np.abs(predicted_censored_norm - expected_norm) - -np.abs(\n                        predicted_full_norm - expected_norm\n                    )\n                    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n                        copredictivity = np.corrcoef(celldeltacor)\n                    copredictivity[np.isnan(copredictivity)] = 0.0\n\n                    copredictivity_folds.append(copredictivity)\n\n                    cor = chd.utils.paircor(predicted, expected, dim=-1)\n                    deltacor = cor[1:] - cor[0]\n\n                    lost = (n_fragments[0] - n_fragments[1:]).mean(-1)\n\n                    deltacor_folds.append(deltacor)\n                    lost_folds.append(lost)\n\n                lost_folds = np.stack(lost_folds, 0)\n                deltacor_folds = np.stack(deltacor_folds, 0)\n                copredictivity_folds = np.stack(copredictivity_folds, 0)\n\n                result = xr.Dataset(\n                    {\n                        \"deltacor\": xr.DataArray(\n                            deltacor_folds,\n                            coords=[\n                                (\"model\", np.arange(len(models))),\n                                (\"window\", design.index),\n                            ],\n                        ),\n                        \"lost\": xr.DataArray(\n                            lost_folds,\n                            coords=[\n                                (\"model\", np.arange(len(models))),\n                                (\"window\", design.index),\n                            ],\n                        ),\n                    }\n                )\n\n                windows_oi = lost_folds.mean(0) &gt; 1e-3\n\n                interaction = xr.DataArray(\n                    copredictivity_folds[:, windows_oi][:, :, windows_oi],\n                    coords=[\n                        (\"model\", np.arange(len(models))),\n                        (\"window1\", design.index[windows_oi]),\n                        (\"window2\", design.index[windows_oi]),\n                    ],\n                )\n\n                pickle.dump(result, scores_file.open(\"wb\"))\n                pickle.dump(interaction, interaction_file.open(\"wb\"))\n\n                self.genes = self.genes | {gene}\n\n    def get_plotdata(self, gene):\n\"\"\"\n        Get plotdata for a gene\n        \"\"\"\n        interaction = pickle.load(\n            open(\n                self.get_scoring_path(gene) / \"interaction.pkl\",\n                \"rb\",\n            )\n        )\n\n        plotdata = interaction.mean(\"model\").to_dataframe(\"cor\").reset_index()\n        plotdata[\"window1\"] = plotdata[\"window1\"].astype(\"category\")\n        plotdata[\"window2\"] = plotdata[\"window2\"].astype(\"category\")\n\n        plotdata = (\n            pd.DataFrame(\n                itertools.combinations(self.design.index, 2),\n                columns=[\"window1\", \"window2\"],\n            )\n            .set_index([\"window1\", \"window2\"])\n            .join(plotdata.set_index([\"window1\", \"window2\"]))\n        )\n        plotdata = plotdata.reset_index().fillna({\"cor\": 0.0})\n        plotdata[\"window_mid1\"] = self.design.loc[plotdata[\"window1\"]][\"window_mid\"].values\n        plotdata[\"window_mid2\"] = self.design.loc[plotdata[\"window2\"]][\"window_mid\"].values\n        plotdata[\"dist\"] = np.abs(plotdata[\"window_mid1\"] - plotdata[\"window_mid2\"])\n        plotdata = plotdata.query(\"(window_mid1 &lt; window_mid2)\")\n        # plotdata = plotdata.query(\"dist &gt; 1000\")\n\n        # x = interaction.stack({\"window1_window2\": [\"window1\", \"window2\"]}).values\n        # print(x.shape)\n        # scores_statistical = []\n        # for i in range(x.shape[1]):\n        #     scores_statistical.append(scipy.stats.ttest_1samp(x[:, i], 0).pvalue)\n        # scores_statistical = pd.DataFrame({\"pval\": scores_statistical})\n        # scores_statistical[\"pval\"] = scores_statistical[\"pval\"].fillna(1.0)\n        # scores_statistical[\"qval\"] = fdr(scores_statistical[\"pval\"])\n\n        # plotdata[\"pval\"] = scores_statistical[\"pval\"].values\n        # plotdata[\"qval\"] = scores_statistical[\"qval\"].values\n\n        plotdata.loc[plotdata[\"dist\"] &lt; 1000, \"cor\"] = 0.0\n\n        return plotdata\n\n    def get_scoring_path(self, gene):\n        path = self.path / f\"{gene}\"\n        path.mkdir(parents=True, exist_ok=True)\n        return path\n</code></pre>"},{"location":"reference/models/pred/interpret/#chromatinhd.models.pred.interpret.genepairwindow.GenePairWindow.get_plotdata","title":"<code>get_plotdata(gene)</code>","text":"<p>Get plotdata for a gene</p> Source code in <code>src/chromatinhd/models/pred/interpret/genepairwindow.py</code> <pre><code>def get_plotdata(self, gene):\n\"\"\"\n    Get plotdata for a gene\n    \"\"\"\n    interaction = pickle.load(\n        open(\n            self.get_scoring_path(gene) / \"interaction.pkl\",\n            \"rb\",\n        )\n    )\n\n    plotdata = interaction.mean(\"model\").to_dataframe(\"cor\").reset_index()\n    plotdata[\"window1\"] = plotdata[\"window1\"].astype(\"category\")\n    plotdata[\"window2\"] = plotdata[\"window2\"].astype(\"category\")\n\n    plotdata = (\n        pd.DataFrame(\n            itertools.combinations(self.design.index, 2),\n            columns=[\"window1\", \"window2\"],\n        )\n        .set_index([\"window1\", \"window2\"])\n        .join(plotdata.set_index([\"window1\", \"window2\"]))\n    )\n    plotdata = plotdata.reset_index().fillna({\"cor\": 0.0})\n    plotdata[\"window_mid1\"] = self.design.loc[plotdata[\"window1\"]][\"window_mid\"].values\n    plotdata[\"window_mid2\"] = self.design.loc[plotdata[\"window2\"]][\"window_mid\"].values\n    plotdata[\"dist\"] = np.abs(plotdata[\"window_mid1\"] - plotdata[\"window_mid2\"])\n    plotdata = plotdata.query(\"(window_mid1 &lt; window_mid2)\")\n    # plotdata = plotdata.query(\"dist &gt; 1000\")\n\n    # x = interaction.stack({\"window1_window2\": [\"window1\", \"window2\"]}).values\n    # print(x.shape)\n    # scores_statistical = []\n    # for i in range(x.shape[1]):\n    #     scores_statistical.append(scipy.stats.ttest_1samp(x[:, i], 0).pvalue)\n    # scores_statistical = pd.DataFrame({\"pval\": scores_statistical})\n    # scores_statistical[\"pval\"] = scores_statistical[\"pval\"].fillna(1.0)\n    # scores_statistical[\"qval\"] = fdr(scores_statistical[\"pval\"])\n\n    # plotdata[\"pval\"] = scores_statistical[\"pval\"].values\n    # plotdata[\"qval\"] = scores_statistical[\"qval\"].values\n\n    plotdata.loc[plotdata[\"dist\"] &lt; 1000, \"cor\"] = 0.0\n\n    return plotdata\n</code></pre>"},{"location":"reference/models/pred/interpret/#chromatinhd.models.pred.interpret.genepairwindow.GenePairWindow.score","title":"<code>score(fragments, transcriptome, models, folds, censorer, genes=None, force=False, device=None)</code>","text":"<p>Score the models</p> <p>Parameters:</p> Name Type Description Default <code>fragments</code> <code>Fragments</code> <p>the fragments</p> required <code>transcriptome</code> <code>Transcriptome</code> <p>the transcriptome</p> required <code>models</code> <code>Models</code> <p>the models</p> required <code>folds</code> <code>Folds</code> <p>the folds</p> required <code>genes</code> <code>Optional[List]</code> <p>which genes to score, defaults to all</p> <code>None</code> Source code in <code>src/chromatinhd/models/pred/interpret/genepairwindow.py</code> <pre><code>def score(\n    self,\n    fragments: Fragments,\n    transcriptome: Transcriptome,\n    models: Models,\n    folds: Folds,\n    censorer,\n    genes: Optional[List] = None,\n    force=False,\n    device=None,\n):\n\"\"\"\n    Score the models\n\n    Parameters:\n        fragments:\n            the fragments\n        transcriptome:\n            the transcriptome\n        models:\n            the models\n        folds:\n            the folds\n        genes:\n            which genes to score, defaults to all\n\n    \"\"\"\n    force_ = force\n    design = censorer.design.iloc[1:].copy()\n    self.design = design\n\n    if device is None:\n        device = get_default_device()\n\n    if genes is None:\n        genes = transcriptome.var.index\n\n    pbar = tqdm.tqdm(genes, leave=False)\n\n    for gene in pbar:\n        pbar.set_description(gene)\n        scores_file = self.get_scoring_path(gene) / \"scores.pkl\"\n        interaction_file = self.get_scoring_path(gene) / \"interaction.pkl\"\n\n        force = force_\n        if not all([file.exists() for file in [scores_file, interaction_file]]):\n            force = True\n\n        if force:\n            deltacor_folds = []\n            copredictivity_folds = []\n            lost_folds = []\n            for fold, model in zip(folds, models):\n                predicted, expected, n_fragments = model.get_prediction_censored(\n                    fragments,\n                    transcriptome,\n                    censorer,\n                    cell_ixs=np.concatenate([fold[\"cells_validation\"], fold[\"cells_test\"]]),\n                    genes=[gene],\n                    device=device,\n                )\n\n                # select 1st gene, given that we're working with one gene anyway\n                predicted = predicted[..., 0]\n                expected = expected[..., 0]\n                n_fragments = n_fragments[..., 0]\n\n                # calculate delta cor per cell\n                # calculate effect per cellxgene combination\n                predicted_censored = predicted[1:]\n                predicted_full = predicted[0][None, ...]\n                predicted_full_norm = zscore(predicted_full, 1)\n                predicted_censored_norm = zscore_relative(predicted_censored, predicted_full, 1)\n\n                expected_norm = zscore(expected[None, ...], 1)\n\n                celldeltacor = -np.abs(predicted_censored_norm - expected_norm) - -np.abs(\n                    predicted_full_norm - expected_norm\n                )\n                with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n                    copredictivity = np.corrcoef(celldeltacor)\n                copredictivity[np.isnan(copredictivity)] = 0.0\n\n                copredictivity_folds.append(copredictivity)\n\n                cor = chd.utils.paircor(predicted, expected, dim=-1)\n                deltacor = cor[1:] - cor[0]\n\n                lost = (n_fragments[0] - n_fragments[1:]).mean(-1)\n\n                deltacor_folds.append(deltacor)\n                lost_folds.append(lost)\n\n            lost_folds = np.stack(lost_folds, 0)\n            deltacor_folds = np.stack(deltacor_folds, 0)\n            copredictivity_folds = np.stack(copredictivity_folds, 0)\n\n            result = xr.Dataset(\n                {\n                    \"deltacor\": xr.DataArray(\n                        deltacor_folds,\n                        coords=[\n                            (\"model\", np.arange(len(models))),\n                            (\"window\", design.index),\n                        ],\n                    ),\n                    \"lost\": xr.DataArray(\n                        lost_folds,\n                        coords=[\n                            (\"model\", np.arange(len(models))),\n                            (\"window\", design.index),\n                        ],\n                    ),\n                }\n            )\n\n            windows_oi = lost_folds.mean(0) &gt; 1e-3\n\n            interaction = xr.DataArray(\n                copredictivity_folds[:, windows_oi][:, :, windows_oi],\n                coords=[\n                    (\"model\", np.arange(len(models))),\n                    (\"window1\", design.index[windows_oi]),\n                    (\"window2\", design.index[windows_oi]),\n                ],\n            )\n\n            pickle.dump(result, scores_file.open(\"wb\"))\n            pickle.dump(interaction, interaction_file.open(\"wb\"))\n\n            self.genes = self.genes | {gene}\n</code></pre>"},{"location":"reference/models/pred/model/","title":"Model","text":""},{"location":"reference/models/pred/model/#additive","title":"Additive","text":""},{"location":"reference/models/pred/model/#chromatinhd.models.pred.model.additive.Model","title":"<code>chromatinhd.models.pred.model.additive.Model</code>","text":"<p>         Bases: <code>torch.nn.Module</code>, <code>HybridModel</code></p> <p>Predicting gene expression from raw fragments using an additive model across fragments from the same cell</p> <p>Parameters:</p> Name Type Description Default <code>n_genes</code> <code>int</code> <p>the number of genes</p> required <code>dummy</code> <code>bool</code> <p>whether to use a dummy model that just counts fragments</p> <code>False</code> <code>n_frequencies</code> <code>int</code> <p>the number of frequencies to use for sine encoding</p> <code>50</code> <code>reduce</code> <code>str</code> <p>the reduction to use for pooling fragments across genes and cells</p> <code>'sum'</code> <code>nonlinear</code> <code>bool</code> <p>whether to use a non-linear activation function</p> <code>True</code> <code>n_embedding_dimensions</code> <code>int</code> <p>the number of embedding dimensions</p> <code>10</code> <code>dropout_rate</code> <code>float</code> <p>the dropout rate</p> <code>0.0</code> Source code in <code>src/chromatinhd/models/pred/model/additive.py</code> <pre><code>class Model(torch.nn.Module, HybridModel):\n\"\"\"\n    Predicting gene expression from raw fragments using an additive model across fragments from the same cell\n\n    Parameters:\n        n_genes:\n            the number of genes\n        dummy:\n            whether to use a dummy model that just counts fragments\n        n_frequencies:\n            the number of frequencies to use for sine encoding\n        reduce:\n            the reduction to use for pooling fragments across genes and cells\n        nonlinear:\n            whether to use a non-linear activation function\n        n_embedding_dimensions:\n            the number of embedding dimensions\n        dropout_rate:\n            the dropout rate\n    \"\"\"\n\n    def __init__(\n        self,\n        n_genes: int,\n        dummy: bool = False,\n        n_frequencies: int = 50,\n        reduce: str = \"sum\",\n        nonlinear: bool = True,\n        n_embedding_dimensions: int = 10,\n        dropout_rate: float = 0.0,\n        embedding_to_expression_initialization: str = \"default\",\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__()\n\n        if dummy:\n            self.fragment_embedder = FragmentEmbedderCounter()\n        else:\n            self.fragment_embedder = FragmentEmbedder(\n                n_frequencies=n_frequencies,\n                n_genes=n_genes,\n                nonlinear=nonlinear,\n                n_embedding_dimensions=n_embedding_dimensions,\n                dropout_rate=dropout_rate,\n            )\n        self.embedding_gene_pooler = EmbeddingGenePooler(reduce=reduce)\n        self.embedding_to_expression = EmbeddingToExpression(\n            n_genes=n_genes,\n            n_embedding_dimensions=self.fragment_embedder.n_embedding_dimensions,\n            initialization=embedding_to_expression_initialization,\n        )\n\n    def forward(self, data):\n\"\"\"\n        Make a prediction given a data object\n        \"\"\"\n        fragment_embedding = self.fragment_embedder(data.fragments.coordinates, data.fragments.genemapping)\n        cell_gene_embedding = self.embedding_gene_pooler(\n            fragment_embedding,\n            data.fragments.local_cellxgene_ix,\n            data.minibatch.n_cells,\n            data.minibatch.n_genes,\n        )\n        expression_predicted = self.embedding_to_expression(cell_gene_embedding, data.minibatch.genes_oi_torch)\n        return expression_predicted\n\n    def forward_loss(self, data):\n\"\"\"\n        Make a prediction and calculate the loss given a data object\n        \"\"\"\n        expression_predicted = self.forward(data)\n        expression_true = data.transcriptome.value\n        return paircor_loss(expression_predicted, expression_true)\n\n    def forward_gene_loss(self, data):\n\"\"\"\n        Make a prediction and calculate the loss given a data object\n        \"\"\"\n        expression_predicted = self.forward(data)\n        expression_true = data.transcriptome.value\n        return gene_paircor_loss(expression_predicted, expression_true)\n\n    def forward_multiple(self, data, fragments_oi, min_fragments=1):\n        fragment_embedding = self.fragment_embedder(data.fragments.coordinates, data.fragments.genemapping)\n\n        total_n_fragments = torch.bincount(\n            data.fragments.local_cellxgene_ix,\n            minlength=data.minibatch.n_genes * data.minibatch.n_cells,\n        ).reshape((data.minibatch.n_cells, data.minibatch.n_genes))\n\n        total_cell_gene_embedding = self.embedding_gene_pooler.forward(\n            fragment_embedding,\n            data.fragments.local_cellxgene_ix,\n            data.minibatch.n_cells,\n            data.minibatch.n_genes,\n        )\n\n        total_expression_predicted = self.embedding_to_expression.forward(\n            total_cell_gene_embedding, data.minibatch.genes_oi_torch\n        )\n\n        for fragments_oi_ in fragments_oi:\n            if (fragments_oi_ is not None) and ((~fragments_oi_).sum() &gt; min_fragments):\n                lost_fragments_oi = ~fragments_oi_\n                lost_local_cellxgene_ix = data.fragments.local_cellxgene_ix[lost_fragments_oi]\n                n_fragments = total_n_fragments - torch.bincount(\n                    lost_local_cellxgene_ix,\n                    minlength=data.minibatch.n_genes * data.minibatch.n_cells,\n                ).reshape((data.minibatch.n_cells, data.minibatch.n_genes))\n                cell_gene_embedding = total_cell_gene_embedding - self.embedding_gene_pooler.forward(\n                    fragment_embedding[lost_fragments_oi],\n                    lost_local_cellxgene_ix,\n                    data.minibatch.n_cells,\n                    data.minibatch.n_genes,\n                )\n\n                expression_predicted = self.embedding_to_expression.forward(\n                    cell_gene_embedding, data.minibatch.genes_oi_torch\n                )\n            else:\n                n_fragments = total_n_fragments\n                expression_predicted = total_expression_predicted\n\n            yield expression_predicted, n_fragments\n\n    def train_model(\n        self,\n        fragments: Fragments,\n        transcriptome: Transcriptome,\n        fold: list,\n        device=None,\n        lr=1e-2,\n        n_epochs=30,\n    ):\n\"\"\"\n        Train the model\n        \"\"\"\n        # set up minibatchers and loaders\n        minibatcher_train = Minibatcher(\n            fold[\"cells_train\"],\n            range(fragments.n_genes),\n            n_genes_step=500,\n            n_cells_step=200,\n        )\n        minibatcher_validation = Minibatcher(\n            fold[\"cells_validation\"],\n            range(fragments.n_genes),\n            n_genes_step=10,\n            n_cells_step=10000,\n            permute_cells=False,\n            permute_genes=False,\n        )\n\n        if device is None:\n            device = get_default_device()\n\n        loaders_train = LoaderPool(\n            TranscriptomeFragments,\n            dict(\n                transcriptome=transcriptome,\n                fragments=fragments,\n                cellxgene_batch_size=minibatcher_train.cellxgene_batch_size,\n            ),\n            n_workers=10,\n        )\n        loaders_validation = LoaderPool(\n            TranscriptomeFragments,\n            dict(\n                transcriptome=transcriptome,\n                fragments=fragments,\n                cellxgene_batch_size=minibatcher_validation.cellxgene_batch_size,\n            ),\n            n_workers=5,\n        )\n\n        trainer = Trainer(\n            self,\n            loaders_train,\n            loaders_validation,\n            minibatcher_train,\n            minibatcher_validation,\n            SparseDenseAdam(\n                self.parameters_sparse(),\n                self.parameters_dense(),\n                lr=lr,\n                weight_decay=1e-5,\n            ),\n            n_epochs=n_epochs,\n            checkpoint_every_epoch=1,\n            optimize_every_step=1,\n            device=device,\n        )\n\n        trainer.train()\n        # trainer.trace.plot()\n\n    def get_prediction(\n        self,\n        fragments,\n        transcriptome,\n        cells=None,\n        cell_ixs=None,\n        genes=None,\n        gene_ixs=None,\n        device=None,\n        return_raw=False,\n    ):\n\"\"\"\n        Returns the prediction of a dataset\n        \"\"\"\n        if cell_ixs is None:\n            if cells is None:\n                cells = fragments.obs.index\n            fragments.obs[\"ix\"] = np.arange(len(fragments.obs))\n            cell_ixs = fragments.obs.loc[cells][\"ix\"].values\n        if cells is None:\n            cells = fragments.obs.index[cell_ixs]\n\n        if gene_ixs is None:\n            if genes is None:\n                genes = fragments.var.index\n            fragments.var[\"ix\"] = np.arange(len(fragments.var))\n            gene_ixs = fragments.var.loc[genes][\"ix\"].values\n        if genes is None:\n            genes = fragments.var.index[gene_ixs]\n\n        if device is None:\n            device = get_default_device()\n\n        minibatches = Minibatcher(\n            cell_ixs,\n            gene_ixs,\n            n_genes_step=500,\n            n_cells_step=200,\n            use_all_cells=True,\n            use_all_genes=True,\n            permute_cells=False,\n            permute_genes=False,\n        )\n        loaders = LoaderPool(\n            TranscriptomeFragments,\n            dict(\n                transcriptome=transcriptome,\n                fragments=fragments,\n                cellxgene_batch_size=minibatches.cellxgene_batch_size,\n            ),\n            n_workers=5,\n        )\n        loaders.initialize(minibatches)\n\n        predicted = np.zeros((len(cell_ixs), len(gene_ixs)))\n        expected = np.zeros((len(cell_ixs), len(gene_ixs)))\n        n_fragments = np.zeros((len(cell_ixs), len(gene_ixs)))\n\n        cell_mapping = np.zeros(fragments.n_cells, dtype=np.int64)\n        cell_mapping[cell_ixs] = np.arange(len(cell_ixs))\n\n        gene_mapping = np.zeros(fragments.n_genes, dtype=np.int64)\n        gene_mapping[gene_ixs] = np.arange(len(gene_ixs))\n\n        self.eval()\n        self = self.to(device)\n\n        for data in loaders:\n            data = data.to(device)\n            with torch.no_grad():\n                pred_mb = self.forward(data)\n            predicted[np.ix_(cell_mapping[data.minibatch.cells_oi], data.minibatch.genes_oi)] = pred_mb.cpu().numpy()\n            expected[\n                np.ix_(cell_mapping[data.minibatch.cells_oi], data.minibatch.genes_oi)\n            ] = data.transcriptome.value.cpu().numpy()\n            n_fragments[np.ix_(cell_mapping[data.minibatch.cells_oi], data.minibatch.genes_oi)] = (\n                torch.bincount(\n                    data.fragments.local_cellxgene_ix,\n                    minlength=len(data.minibatch.cells_oi) * len(data.minibatch.genes_oi),\n                )\n                .reshape(len(data.minibatch.cells_oi), len(data.minibatch.genes_oi))\n                .cpu()\n                .numpy()\n            )\n\n        self = self.to(\"cpu\")\n\n        if return_raw:\n            return predicted, expected, n_fragments\n\n        result = xr.Dataset(\n            {\n                \"predicted\": xr.DataArray(\n                    predicted,\n                    dims=(\"cell\", \"gene\"),\n                    coords={\"cell\": cells, \"gene\": genes},\n                ),\n                \"expected\": xr.DataArray(\n                    expected,\n                    dims=(\"cell\", \"gene\"),\n                    coords={\"cell\": cells, \"gene\": genes},\n                ),\n                \"n_fragments\": xr.DataArray(\n                    n_fragments,\n                    dims=(\"cell\", \"gene\"),\n                    coords={\"cell\": cells, \"gene\": genes},\n                ),\n            }\n        )\n        return result\n\n    def get_prediction_censored(\n        self,\n        fragments,\n        transcriptome,\n        censorer,\n        cells=None,\n        cell_ixs=None,\n        genes=None,\n        gene_ixs=None,\n        device=None,\n    ):\n\"\"\"\n        Returns the prediction of multiple censored dataset\n        \"\"\"\n        if cell_ixs is None:\n            if cells is None:\n                cells = fragments.obs.index\n            fragments.obs[\"ix\"] = np.arange(len(fragments.obs))\n            cell_ixs = fragments.obs.loc[cells][\"ix\"].values\n        if cells is None:\n            cells = fragments.obs.index[cell_ixs]\n\n        if gene_ixs is None:\n            if genes is None:\n                genes = fragments.var.index\n            fragments.var[\"ix\"] = np.arange(len(fragments.var))\n            gene_ixs = fragments.var.loc[genes][\"ix\"].values\n        if genes is None:\n            genes = fragments.var.index[gene_ixs]\n\n        if device is None:\n            device = get_default_device()\n\n        minibatcher = Minibatcher(\n            cell_ixs,\n            gene_ixs,\n            n_genes_step=500,\n            n_cells_step=5000,\n            use_all_cells=True,\n            use_all_genes=True,\n            permute_cells=False,\n            permute_genes=False,\n        )\n        loaders = LoaderPool(\n            TranscriptomeFragments,\n            dict(\n                transcriptome=transcriptome,\n                fragments=fragments,\n                cellxgene_batch_size=minibatcher.cellxgene_batch_size,\n            ),\n            n_workers=10,\n        )\n        loaders.initialize(minibatcher)\n\n        predicted = np.zeros((len(censorer), len(cell_ixs), len(gene_ixs)), dtype=float)\n        expected = np.zeros((len(cell_ixs), len(gene_ixs)), dtype=float)\n        n_fragments = np.zeros((len(censorer), len(cell_ixs), len(gene_ixs)), dtype=int)\n\n        cell_mapping = np.zeros(fragments.n_cells, dtype=np.int64)\n        cell_mapping[cell_ixs] = np.arange(len(cell_ixs))\n        gene_mapping = np.zeros(fragments.n_genes, dtype=np.int64)\n        gene_mapping[gene_ixs] = np.arange(len(gene_ixs))\n\n        self.eval()\n        self.to(device)\n        for data in loaders:\n            data = data.to(device)\n            fragments_oi = censorer(data)\n\n            with torch.no_grad():\n                for design_ix, (\n                    pred_mb,\n                    n_fragments_oi_mb,\n                ) in enumerate(self.forward_multiple(data, fragments_oi)):\n                    ix = np.ix_(\n                        [design_ix],\n                        cell_mapping[data.minibatch.cells_oi],\n                        gene_mapping[data.minibatch.genes_oi],\n                    )\n                    predicted[ix] = pred_mb.cpu().numpy()\n                    n_fragments[ix] = n_fragments_oi_mb.cpu().numpy()\n            expected[\n                np.ix_(\n                    cell_mapping[data.minibatch.cells_oi],\n                    gene_mapping[data.minibatch.genes_oi],\n                )\n            ] = data.transcriptome.value.cpu().numpy()\n\n        self.to(\"cpu\")\n\n        return predicted, expected, n_fragments\n</code></pre>"},{"location":"reference/models/pred/model/#chromatinhd.models.pred.model.additive.Model.forward","title":"<code>forward(data)</code>","text":"<p>Make a prediction given a data object</p> Source code in <code>src/chromatinhd/models/pred/model/additive.py</code> <pre><code>def forward(self, data):\n\"\"\"\n    Make a prediction given a data object\n    \"\"\"\n    fragment_embedding = self.fragment_embedder(data.fragments.coordinates, data.fragments.genemapping)\n    cell_gene_embedding = self.embedding_gene_pooler(\n        fragment_embedding,\n        data.fragments.local_cellxgene_ix,\n        data.minibatch.n_cells,\n        data.minibatch.n_genes,\n    )\n    expression_predicted = self.embedding_to_expression(cell_gene_embedding, data.minibatch.genes_oi_torch)\n    return expression_predicted\n</code></pre>"},{"location":"reference/models/pred/model/#chromatinhd.models.pred.model.additive.Model.forward_gene_loss","title":"<code>forward_gene_loss(data)</code>","text":"<p>Make a prediction and calculate the loss given a data object</p> Source code in <code>src/chromatinhd/models/pred/model/additive.py</code> <pre><code>def forward_gene_loss(self, data):\n\"\"\"\n    Make a prediction and calculate the loss given a data object\n    \"\"\"\n    expression_predicted = self.forward(data)\n    expression_true = data.transcriptome.value\n    return gene_paircor_loss(expression_predicted, expression_true)\n</code></pre>"},{"location":"reference/models/pred/model/#chromatinhd.models.pred.model.additive.Model.forward_loss","title":"<code>forward_loss(data)</code>","text":"<p>Make a prediction and calculate the loss given a data object</p> Source code in <code>src/chromatinhd/models/pred/model/additive.py</code> <pre><code>def forward_loss(self, data):\n\"\"\"\n    Make a prediction and calculate the loss given a data object\n    \"\"\"\n    expression_predicted = self.forward(data)\n    expression_true = data.transcriptome.value\n    return paircor_loss(expression_predicted, expression_true)\n</code></pre>"},{"location":"reference/models/pred/model/#chromatinhd.models.pred.model.additive.Model.get_prediction","title":"<code>get_prediction(fragments, transcriptome, cells=None, cell_ixs=None, genes=None, gene_ixs=None, device=None, return_raw=False)</code>","text":"<p>Returns the prediction of a dataset</p> Source code in <code>src/chromatinhd/models/pred/model/additive.py</code> <pre><code>def get_prediction(\n    self,\n    fragments,\n    transcriptome,\n    cells=None,\n    cell_ixs=None,\n    genes=None,\n    gene_ixs=None,\n    device=None,\n    return_raw=False,\n):\n\"\"\"\n    Returns the prediction of a dataset\n    \"\"\"\n    if cell_ixs is None:\n        if cells is None:\n            cells = fragments.obs.index\n        fragments.obs[\"ix\"] = np.arange(len(fragments.obs))\n        cell_ixs = fragments.obs.loc[cells][\"ix\"].values\n    if cells is None:\n        cells = fragments.obs.index[cell_ixs]\n\n    if gene_ixs is None:\n        if genes is None:\n            genes = fragments.var.index\n        fragments.var[\"ix\"] = np.arange(len(fragments.var))\n        gene_ixs = fragments.var.loc[genes][\"ix\"].values\n    if genes is None:\n        genes = fragments.var.index[gene_ixs]\n\n    if device is None:\n        device = get_default_device()\n\n    minibatches = Minibatcher(\n        cell_ixs,\n        gene_ixs,\n        n_genes_step=500,\n        n_cells_step=200,\n        use_all_cells=True,\n        use_all_genes=True,\n        permute_cells=False,\n        permute_genes=False,\n    )\n    loaders = LoaderPool(\n        TranscriptomeFragments,\n        dict(\n            transcriptome=transcriptome,\n            fragments=fragments,\n            cellxgene_batch_size=minibatches.cellxgene_batch_size,\n        ),\n        n_workers=5,\n    )\n    loaders.initialize(minibatches)\n\n    predicted = np.zeros((len(cell_ixs), len(gene_ixs)))\n    expected = np.zeros((len(cell_ixs), len(gene_ixs)))\n    n_fragments = np.zeros((len(cell_ixs), len(gene_ixs)))\n\n    cell_mapping = np.zeros(fragments.n_cells, dtype=np.int64)\n    cell_mapping[cell_ixs] = np.arange(len(cell_ixs))\n\n    gene_mapping = np.zeros(fragments.n_genes, dtype=np.int64)\n    gene_mapping[gene_ixs] = np.arange(len(gene_ixs))\n\n    self.eval()\n    self = self.to(device)\n\n    for data in loaders:\n        data = data.to(device)\n        with torch.no_grad():\n            pred_mb = self.forward(data)\n        predicted[np.ix_(cell_mapping[data.minibatch.cells_oi], data.minibatch.genes_oi)] = pred_mb.cpu().numpy()\n        expected[\n            np.ix_(cell_mapping[data.minibatch.cells_oi], data.minibatch.genes_oi)\n        ] = data.transcriptome.value.cpu().numpy()\n        n_fragments[np.ix_(cell_mapping[data.minibatch.cells_oi], data.minibatch.genes_oi)] = (\n            torch.bincount(\n                data.fragments.local_cellxgene_ix,\n                minlength=len(data.minibatch.cells_oi) * len(data.minibatch.genes_oi),\n            )\n            .reshape(len(data.minibatch.cells_oi), len(data.minibatch.genes_oi))\n            .cpu()\n            .numpy()\n        )\n\n    self = self.to(\"cpu\")\n\n    if return_raw:\n        return predicted, expected, n_fragments\n\n    result = xr.Dataset(\n        {\n            \"predicted\": xr.DataArray(\n                predicted,\n                dims=(\"cell\", \"gene\"),\n                coords={\"cell\": cells, \"gene\": genes},\n            ),\n            \"expected\": xr.DataArray(\n                expected,\n                dims=(\"cell\", \"gene\"),\n                coords={\"cell\": cells, \"gene\": genes},\n            ),\n            \"n_fragments\": xr.DataArray(\n                n_fragments,\n                dims=(\"cell\", \"gene\"),\n                coords={\"cell\": cells, \"gene\": genes},\n            ),\n        }\n    )\n    return result\n</code></pre>"},{"location":"reference/models/pred/model/#chromatinhd.models.pred.model.additive.Model.get_prediction_censored","title":"<code>get_prediction_censored(fragments, transcriptome, censorer, cells=None, cell_ixs=None, genes=None, gene_ixs=None, device=None)</code>","text":"<p>Returns the prediction of multiple censored dataset</p> Source code in <code>src/chromatinhd/models/pred/model/additive.py</code> <pre><code>def get_prediction_censored(\n    self,\n    fragments,\n    transcriptome,\n    censorer,\n    cells=None,\n    cell_ixs=None,\n    genes=None,\n    gene_ixs=None,\n    device=None,\n):\n\"\"\"\n    Returns the prediction of multiple censored dataset\n    \"\"\"\n    if cell_ixs is None:\n        if cells is None:\n            cells = fragments.obs.index\n        fragments.obs[\"ix\"] = np.arange(len(fragments.obs))\n        cell_ixs = fragments.obs.loc[cells][\"ix\"].values\n    if cells is None:\n        cells = fragments.obs.index[cell_ixs]\n\n    if gene_ixs is None:\n        if genes is None:\n            genes = fragments.var.index\n        fragments.var[\"ix\"] = np.arange(len(fragments.var))\n        gene_ixs = fragments.var.loc[genes][\"ix\"].values\n    if genes is None:\n        genes = fragments.var.index[gene_ixs]\n\n    if device is None:\n        device = get_default_device()\n\n    minibatcher = Minibatcher(\n        cell_ixs,\n        gene_ixs,\n        n_genes_step=500,\n        n_cells_step=5000,\n        use_all_cells=True,\n        use_all_genes=True,\n        permute_cells=False,\n        permute_genes=False,\n    )\n    loaders = LoaderPool(\n        TranscriptomeFragments,\n        dict(\n            transcriptome=transcriptome,\n            fragments=fragments,\n            cellxgene_batch_size=minibatcher.cellxgene_batch_size,\n        ),\n        n_workers=10,\n    )\n    loaders.initialize(minibatcher)\n\n    predicted = np.zeros((len(censorer), len(cell_ixs), len(gene_ixs)), dtype=float)\n    expected = np.zeros((len(cell_ixs), len(gene_ixs)), dtype=float)\n    n_fragments = np.zeros((len(censorer), len(cell_ixs), len(gene_ixs)), dtype=int)\n\n    cell_mapping = np.zeros(fragments.n_cells, dtype=np.int64)\n    cell_mapping[cell_ixs] = np.arange(len(cell_ixs))\n    gene_mapping = np.zeros(fragments.n_genes, dtype=np.int64)\n    gene_mapping[gene_ixs] = np.arange(len(gene_ixs))\n\n    self.eval()\n    self.to(device)\n    for data in loaders:\n        data = data.to(device)\n        fragments_oi = censorer(data)\n\n        with torch.no_grad():\n            for design_ix, (\n                pred_mb,\n                n_fragments_oi_mb,\n            ) in enumerate(self.forward_multiple(data, fragments_oi)):\n                ix = np.ix_(\n                    [design_ix],\n                    cell_mapping[data.minibatch.cells_oi],\n                    gene_mapping[data.minibatch.genes_oi],\n                )\n                predicted[ix] = pred_mb.cpu().numpy()\n                n_fragments[ix] = n_fragments_oi_mb.cpu().numpy()\n        expected[\n            np.ix_(\n                cell_mapping[data.minibatch.cells_oi],\n                gene_mapping[data.minibatch.genes_oi],\n            )\n        ] = data.transcriptome.value.cpu().numpy()\n\n    self.to(\"cpu\")\n\n    return predicted, expected, n_fragments\n</code></pre>"},{"location":"reference/models/pred/model/#chromatinhd.models.pred.model.additive.Model.train_model","title":"<code>train_model(fragments, transcriptome, fold, device=None, lr=0.01, n_epochs=30)</code>","text":"<p>Train the model</p> Source code in <code>src/chromatinhd/models/pred/model/additive.py</code> <pre><code>def train_model(\n    self,\n    fragments: Fragments,\n    transcriptome: Transcriptome,\n    fold: list,\n    device=None,\n    lr=1e-2,\n    n_epochs=30,\n):\n\"\"\"\n    Train the model\n    \"\"\"\n    # set up minibatchers and loaders\n    minibatcher_train = Minibatcher(\n        fold[\"cells_train\"],\n        range(fragments.n_genes),\n        n_genes_step=500,\n        n_cells_step=200,\n    )\n    minibatcher_validation = Minibatcher(\n        fold[\"cells_validation\"],\n        range(fragments.n_genes),\n        n_genes_step=10,\n        n_cells_step=10000,\n        permute_cells=False,\n        permute_genes=False,\n    )\n\n    if device is None:\n        device = get_default_device()\n\n    loaders_train = LoaderPool(\n        TranscriptomeFragments,\n        dict(\n            transcriptome=transcriptome,\n            fragments=fragments,\n            cellxgene_batch_size=minibatcher_train.cellxgene_batch_size,\n        ),\n        n_workers=10,\n    )\n    loaders_validation = LoaderPool(\n        TranscriptomeFragments,\n        dict(\n            transcriptome=transcriptome,\n            fragments=fragments,\n            cellxgene_batch_size=minibatcher_validation.cellxgene_batch_size,\n        ),\n        n_workers=5,\n    )\n\n    trainer = Trainer(\n        self,\n        loaders_train,\n        loaders_validation,\n        minibatcher_train,\n        minibatcher_validation,\n        SparseDenseAdam(\n            self.parameters_sparse(),\n            self.parameters_dense(),\n            lr=lr,\n            weight_decay=1e-5,\n        ),\n        n_epochs=n_epochs,\n        checkpoint_every_epoch=1,\n        optimize_every_step=1,\n        device=device,\n    )\n\n    trainer.train()\n</code></pre>"},{"location":"reference/models/pred/model/#chromatinhd.models.pred.model.additive.Models","title":"<code>chromatinhd.models.pred.model.additive.Models</code>","text":"<p>         Bases: <code>Flow</code></p> Source code in <code>src/chromatinhd/models/pred/model/additive.py</code> <pre><code>class Models(Flow):\n    n_models = Stored()\n\n    @property\n    def models_path(self):\n        path = self.path / \"models\"\n        path.mkdir(exist_ok=True)\n        return path\n\n    def train_models(self, fragments, transcriptome, folds, device=None):\n        self.n_models = len(folds)\n        for fold_ix, fold in [(fold_ix, fold) for fold_ix, fold in enumerate(folds)]:\n            desired_outputs = [self.models_path / (\"model_\" + str(fold_ix) + \".pkl\")]\n            force = False\n            if not all([desired_output.exists() for desired_output in desired_outputs]):\n                force = True\n\n            if force:\n                model = Model(\n                    n_genes=fragments.n_genes,\n                )\n                model.train_model(fragments, transcriptome, fold, device=device)\n\n                model = model.to(\"cpu\")\n\n                pickle.dump(\n                    model,\n                    open(self.models_path / (\"model_\" + str(fold_ix) + \".pkl\"), \"wb\"),\n                )\n\n    def __getitem__(self, ix):\n        return pickle.load((self.models_path / (\"model_\" + str(ix) + \".pkl\")).open(\"rb\"))\n\n    def __len__(self):\n        return self.n_models\n\n    def __iter__(self):\n        for ix in range(len(self)):\n            yield self[ix]\n\n    def get_gene_cors(self, fragments, transcriptome, folds, device=None):\n        cor_predicted = np.zeros((len(fragments.var.index), len(folds)))\n        cor_n_fragments = np.zeros((len(fragments.var.index), len(folds)))\n        n_fragments = np.zeros((len(fragments.var.index), len(folds)))\n\n        if device is None:\n            device = get_default_device()\n        for model_ix, (model, fold) in enumerate(zip(self, folds)):\n            prediction = model.get_prediction(fragments, transcriptome, cell_ixs=fold[\"cells_test\"], device=device)\n\n            cor_predicted[:, model_ix] = paircor(prediction[\"predicted\"].values, prediction[\"expected\"].values)\n            cor_n_fragments[:, model_ix] = paircor(prediction[\"n_fragments\"].values, prediction[\"expected\"].values)\n\n            n_fragments[:, model_ix] = prediction[\"n_fragments\"].values.sum(0)\n        cor_predicted = pd.Series(cor_predicted.mean(1), index=fragments.var.index, name=\"cor_predicted\")\n        cor_n_fragments = pd.Series(cor_n_fragments.mean(1), index=fragments.var.index, name=\"cor_n_fragments\")\n        n_fragments = pd.Series(n_fragments.mean(1), index=fragments.var.index, name=\"n_fragments\")\n        result = pd.concat([cor_predicted, cor_n_fragments, n_fragments], axis=1)\n        result[\"deltacor\"] = result[\"cor_predicted\"] - result[\"cor_n_fragments\"]\n\n        return result\n</code></pre>"},{"location":"reference/models/pred/plot/","title":"Plot","text":""},{"location":"reference/models/pred/plot/#chromatinhd.models.pred.plot","title":"<code>chromatinhd.models.pred.plot</code>","text":""},{"location":"reference/models/pred/plot/#chromatinhd.models.pred.plot.Copredictivity","title":"<code>Copredictivity</code>","text":"<p>         Bases: <code>chromatinhd.grid.Panel</code></p> <p>Plot co-predictivity of a gene.</p> Source code in <code>src/chromatinhd/models/pred/plot/copredictivity.py</code> <pre><code>class Copredictivity(chromatinhd.grid.Panel):\n\"\"\"\n    Plot co-predictivity of a gene.\n    \"\"\"\n\n    def __init__(self, plotdata, width):\n        super().__init__((width, width / 2))\n\n        norm = mpl.colors.CenteredNorm(0, np.abs(plotdata[\"cor\"]).max())\n        cmap = mpl.cm.RdBu_r\n\n        chromatinhd.plot.matshow45(\n            self.ax,\n            plotdata.set_index([\"window_mid1\", \"window_mid2\"])[\"cor\"],\n            cmap=cmap,\n            norm=norm,\n            radius=50,\n        )\n        self.ax.invert_yaxis()\n\n        panel_copredictivity_legend = self.add_inset(\n            chromatinhd.grid.Panel((0.05, 0.8)), pos=(0.0, 0.0), offset=(0.0, 0.2)\n        )\n        plt.colorbar(\n            mpl.cm.ScalarMappable(norm=norm, cmap=cmap),\n            cax=panel_copredictivity_legend.ax,\n            orientation=\"vertical\",\n        )\n        panel_copredictivity_legend.ax.set_ylabel(\n            \"Co-predictivity\\n(cor $\\\\Delta$cor)\",\n            rotation=0,\n            ha=\"right\",\n            va=\"center\",\n        )\n        panel_copredictivity_legend.ax.yaxis.set_ticks_position(\"left\")\n        panel_copredictivity_legend.ax.yaxis.set_label_position(\"left\")\n\n    @classmethod\n    def from_genepairwindow(cls, genepairwindow, gene, width):\n\"\"\"\n        Plot co-predictivity of a gene using a GenePairWindow object.\n        \"\"\"\n        plotdata = genepairwindow.get_plotdata(gene).reset_index()\n        return cls(plotdata, width)\n</code></pre>"},{"location":"reference/models/pred/plot/#chromatinhd.models.pred.plot.copredictivity.Copredictivity.from_genepairwindow","title":"<code>from_genepairwindow(genepairwindow, gene, width)</code>  <code>classmethod</code>","text":"<p>Plot co-predictivity of a gene using a GenePairWindow object.</p> Source code in <code>src/chromatinhd/models/pred/plot/copredictivity.py</code> <pre><code>@classmethod\ndef from_genepairwindow(cls, genepairwindow, gene, width):\n\"\"\"\n    Plot co-predictivity of a gene using a GenePairWindow object.\n    \"\"\"\n    plotdata = genepairwindow.get_plotdata(gene).reset_index()\n    return cls(plotdata, width)\n</code></pre>"},{"location":"reference/models/pred/plot/#chromatinhd.models.pred.plot.Pileup","title":"<code>Pileup</code>","text":"<p>         Bases: <code>chromatinhd.grid.Panel</code></p> Source code in <code>src/chromatinhd/models/pred/plot/predictivity.py</code> <pre><code>class Pileup(chromatinhd.grid.Panel):\n    def __init__(self, plotdata, window, width):\n        super().__init__((width, 0.5))\n\n        ax = self.ax\n        ax.set_xlim(*window)\n        ax.plot(\n            plotdata[\"position\"],\n            plotdata[\"lost\"],\n            color=\"#333\",\n            lw=1,\n        )\n        ax.fill_between(\n            plotdata[\"position\"],\n            plotdata[\"lost\"],\n            0,\n            color=\"#333\",\n            alpha=0.2,\n            lw=0,\n        )\n        ax.set_xlim(ax.get_xlim())\n        ax.set_ylabel(\n            \"# fragments\\nper 1kb\\nper 1k cells\",\n            rotation=0,\n            ha=\"right\",\n            va=\"center\",\n        )\n\n        # change vertical alignment of last y tick to bottom\n        ax.set_yticks([0, ax.get_ylim()[1]])\n        ax.get_yticklabels()[-1].set_verticalalignment(\"top\")\n        ax.get_yticklabels()[0].set_verticalalignment(\"bottom\")\n\n        # vline at tss\n        ax.axvline(0, color=\"#888888\", lw=0.5, zorder=-1, dashes=(2, 2))\n\n        ax.set_xticks([])\n        ax.set_ylim(0)\n\n    @classmethod\n    def from_genemultiwindow(cls, genemultiwindow, gene, width):\n\"\"\"\n        Plot pileup of a specific gene using a GeneMultiWindow object\n        \"\"\"\n        plotdata = genemultiwindow.get_plotdata(gene).reset_index()\n        window = np.array([plotdata[\"position\"].min(), plotdata[\"position\"].max()])\n        return cls(plotdata, window, width)\n</code></pre>"},{"location":"reference/models/pred/plot/#chromatinhd.models.pred.plot.predictivity.Pileup.from_genemultiwindow","title":"<code>from_genemultiwindow(genemultiwindow, gene, width)</code>  <code>classmethod</code>","text":"<p>Plot pileup of a specific gene using a GeneMultiWindow object</p> Source code in <code>src/chromatinhd/models/pred/plot/predictivity.py</code> <pre><code>@classmethod\ndef from_genemultiwindow(cls, genemultiwindow, gene, width):\n\"\"\"\n    Plot pileup of a specific gene using a GeneMultiWindow object\n    \"\"\"\n    plotdata = genemultiwindow.get_plotdata(gene).reset_index()\n    window = np.array([plotdata[\"position\"].min(), plotdata[\"position\"].max()])\n    return cls(plotdata, window, width)\n</code></pre>"},{"location":"reference/models/pred/plot/#chromatinhd.models.pred.plot.Predictivity","title":"<code>Predictivity</code>","text":"<p>         Bases: <code>chromatinhd.grid.Panel</code></p> <p>Plot predictivity of a gene.</p> Source code in <code>src/chromatinhd/models/pred/plot/predictivity.py</code> <pre><code>class Predictivity(chromatinhd.grid.Panel):\n\"\"\"\n    Plot predictivity of a gene.\n    \"\"\"\n\n    def __init__(self, plotdata, window, width, show_accessibility=False):\n        super().__init__((width, 0.5))\n\n        plotdata[\"effect_sign\"] = np.sign(plotdata[\"effect\"])\n        plotdata[\"segment\"] = plotdata[\"effect_sign\"].diff().ne(0).cumsum()\n\n        ax = self.ax\n        ax.set_xlim(*window)\n\n        for segment, segment_data in plotdata.groupby(\"segment\"):\n            color = \"tomato\" if segment_data[\"effect\"].iloc[0] &gt; 0 else \"#0074D9\"\n            ax.plot(\n                segment_data[\"position\"],\n                segment_data[\"deltacor\"],\n                lw=1,\n                color=color,\n            )\n            ax.fill_between(\n                segment_data[\"position\"],\n                segment_data[\"deltacor\"],\n                0,\n                alpha=0.2,\n                lw=0,\n                color=color,\n            )\n\n        # ax.plot(\n        #     plotdata[\"position\"],\n        #     plotdata[\"deltacor\"],\n        #     color=\"#333\",\n        #     lw=1,\n        # )\n        # ax.fill_between(\n        #     plotdata[\"position\"],\n        #     plotdata[\"deltacor\"],\n        #     0,\n        #     color=\"#333\",\n        #     alpha=0.2,\n        #     lw=0,\n        # )\n\n        ax.set_ylabel(\n            \"Predictivity\\n($\\\\Delta$ cor)\",\n            rotation=0,\n            ha=\"right\",\n            va=\"center\",\n        )\n\n        ax.set_xticks([])\n        ax.invert_yaxis()\n        ax.set_ylim(0, max(-0.05, ax.get_ylim()[1]))\n\n        if show_accessibility:\n            ax2 = self.add_twinx()\n            ax2.plot(\n                plotdata[\"position\"],\n                plotdata[\"lost\"],\n                color=\"tomato\",\n                # color=\"#333\",\n                lw=1,\n            )\n            ax2.fill_between(\n                plotdata[\"position\"],\n                plotdata[\"lost\"],\n                0,\n                color=\"tomato\",\n                alpha=0.2,\n                lw=0,\n            )\n            ax2.set_xlim(ax.get_xlim())\n            ax2.set_ylabel(\n                \"# fragments\\nper 1kb\\nper 1k cells\",\n                rotation=0,\n                ha=\"left\",\n                va=\"center\",\n                color=\"tomato\",\n            )\n            ax2.tick_params(axis=\"y\", colors=\"tomato\")\n            ax2.set_ylim(\n                0,\n                plotdata[\"lost\"].max() / (plotdata[\"deltacor\"].min() / ax.get_ylim()[1]),\n            )\n\n        # change vertical alignment of last y tick to bottom\n        ax.set_yticks([0, ax.get_ylim()[1]])\n        ax.get_yticklabels()[-1].set_verticalalignment(\"top\")\n        ax.get_yticklabels()[0].set_verticalalignment(\"bottom\")\n\n        # vline at tss\n        ax.axvline(0, color=\"#888888\", lw=0.5, zorder=-1, dashes=(2, 2))\n\n    @classmethod\n    def from_genemultiwindow(cls, genemultiwindow, gene, width, show_accessibility=False):\n\"\"\"\n        Plot predictivity of a specific gene using a GeneMultiWindow object\n        \"\"\"\n        plotdata = genemultiwindow.get_plotdata(gene).reset_index()\n        window = np.array([plotdata[\"position\"].min(), plotdata[\"position\"].max()])\n        return cls(plotdata, window, width, show_accessibility=show_accessibility)\n</code></pre>"},{"location":"reference/models/pred/plot/#chromatinhd.models.pred.plot.predictivity.Predictivity.from_genemultiwindow","title":"<code>from_genemultiwindow(genemultiwindow, gene, width, show_accessibility=False)</code>  <code>classmethod</code>","text":"<p>Plot predictivity of a specific gene using a GeneMultiWindow object</p> Source code in <code>src/chromatinhd/models/pred/plot/predictivity.py</code> <pre><code>@classmethod\ndef from_genemultiwindow(cls, genemultiwindow, gene, width, show_accessibility=False):\n\"\"\"\n    Plot predictivity of a specific gene using a GeneMultiWindow object\n    \"\"\"\n    plotdata = genemultiwindow.get_plotdata(gene).reset_index()\n    window = np.array([plotdata[\"position\"].min(), plotdata[\"position\"].max()])\n    return cls(plotdata, window, width, show_accessibility=show_accessibility)\n</code></pre>"}]}